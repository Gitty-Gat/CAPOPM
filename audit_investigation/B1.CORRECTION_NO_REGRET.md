# B1.CORRECTION_NO_REGRET (Stage A — Investigative Audit)

## Topology & Execution Model — B1.CORRECTION_NO_REGRET
- **Experiment definition & invocation:** Declared in `src/capopm/experiments/b1_correction_no_regret.py:71-115` with `EXPERIMENT_ID="B1.CORRECTION_NO_REGRET"`; sweep over longshot_bias, herding, timing_attack_strength, and liquidity_level; scenarios executed via `run_b1_correction_no_regret` → `run_b1_scenario` (`b1_correction_no_regret.py:118-339`).
- **Scenario construction:** Cartesian sweep built at `b1_correction_no_regret.py:82-88`; seed = `base_seed + idx` (`b1_correction_no_regret.py:91-93`); scenario naming by `scenario_id` (`b1_correction_no_regret.py:807-819`) encoding longshot/herd/attack/liquidity/seed. Audited scenarios (from `results/paper_artifacts/claim_table.md`) include seeds 0–3 with longshot=0, herding=0, attack ∈ {0,1}, liquidity ∈ {low,high}`.
- **Correction toggles:** Stage1 and Stage2 enabled for `capopm` by passing `stage1_cfg=stage1_full`, `stage2_cfg=stage2_full` into `capopm_pipeline` (`b1_correction_no_regret.py:495-518`); `capopm_stage1_only` disables Stage2 (`b1_correction_no_regret.py:520-535`); `uncorrected` uses `base_out` with Stage1/Stage2 disabled (`b1_correction_no_regret.py:178-188,536-544`). Stressors: longshot size scaling (`apply_longshot_bias`, `b1_correction_no_regret.py:341-369`), adversarial timing (`apply_strategic_timing_attack`, `b1_correction_no_regret.py:372-417`), herding toggle (`apply_herding`, `b1_correction_no_regret.py:794-804`), liquidity level adjustments (`apply_liquidity_level`, `b1_correction_no_regret.py:782-793`).
- **Baselines:**  
  - `uncorrected`: hybrid prior, no Stage1/Stage2 (`b1_correction_no_regret.py:178-188,536-544`).  
  - `raw_parimutuel`: last implied price from pool path (`b1_correction_no_regret.py:545-551`).  
  - `structural_only`, `ml_only`: Stage1/Stage2 off with prior components removed as appropriate (`b1_correction_no_regret.py:552-585`).  
  - `beta_1_1`, `beta_0_5_0_5`: fixed conjugate updates (`b1_correction_no_regret.py:586-607`).  
  - `capopm_stage1_only`: Stage1 on, Stage2 off (`b1_correction_no_regret.py:520-535`).  
  - `capopm`: Stage1 + Stage2 mixture offsets (`b1_correction_no_regret.py:495-518`; Stage1 weights in `src/capopm/corrections/stage1_behavioral.py:62-115`; Stage2 mixture in `src/capopm/corrections/stage2_structural.py:144-199`).
- **Regret definitions:** Per-run, after metrics computed, regrets are set relative to `uncorrected`: `regret_brier = brier(model) - brier(uncorrected)`; `regret_log_bad = uncorrected.log_score - model.log_score`; `regret_abs_error = abs_error_outcome(model) - abs_error_outcome(uncorrected)` (`b1_correction_no_regret.py:266-271`). Sign convention: regret > 0 is worse; audit expects non-positive (`audit_contracts.py:91-110`).
- **Aggregation & summary:** Metrics aggregated by mean in `aggregate_metrics` (`src/capopm/experiments/runner.py:374-412`). Calibration diagnostics computed with `allow_fallback=True` post-aggregation (`runner.py:263-287`). Tests on regret metrics via paired t/wilcoxon/bootstrap in `run_b1_tests` (`b1_correction_no_regret.py:613-694`). Summary sets pass if both mean regrets ≤ 0 (`b1_correction_no_regret.py:720-755`), added to `summary` and written by `write_scenario_outputs` (`runner.py:479-789`).
- **Smoke tests touching regret/corrections:**  
  - `smoke_test_b1_correction_no_regret.py:1-118` runs reduced grid (longshot=0, herding=0, timing_attack ∈ {0,1}, liquidity ∈ {low,high}, n_runs=3), validates artifact schemas, finite regret fields for capopm and capopm_stage1_only, checks existence of at least one scenario with `capopm.regret_brier <= 0`, and enforces determinism across reruns. No Stage1/Stage2 bypass; implicitly assumes at least one non-positive regret case at very small n.

## Smoke Test Audit
- **smoke_test_b1_correction_no_regret.py:1-118**  
  - **Assumption:** At least one scenario yields capopm regret_brier ≤ 0; artifacts/reliability present; determinism with identical seeds; all regret metrics finite for capopm and capopm_stage1_only.  
  - **Pipeline component legitimized:** End-to-end corrections pipeline (Stage1+Stage2) and regret computation vs uncorrected baseline.  
  - **Support vs paper:** Implicit/engineering-only; CAPOPM.pdf Theorem 14 (mixture unimodal approximation limit) does not assert finite-sample regret dominance. Small-n assumption of observing non-positive regret is not paper-backed and is in tension with audit contract thresholds when n_runs is minimal.

## Criterion-Level Investigation

### regret_brier_non_positive — Scenario B1_correction_no_regret__longshot0__herd0__attack0__liqhigh__seed1 (FAIL)
- **Audit evidence (results/B1_correction_no_regret__longshot0__herd0__attack0__liqhigh__seed1/audit.json):** metric_path `["aggregated_metrics","capopm","regret_brier"]`; comparator_path `null`; direction `<=`; threshold `0.0`; metric_value `0.0029039496968689474`; evaluated `true`; pass `false`; reason `null`. Flags: calibration `NOT_INTERPRETABLE` (n_samples=3 in summary), coverage flags absent in audit; low_n_runs implied by n_runs=3; no grid requirement; criteria_semantics_mismatch/status_mismatch `false`.
- **Backward trace:**  
  1) **audit_contracts.py:** Criterion defined at `src/capopm/experiments/audit_contracts.py:91-100` requiring CAPOPM mean Brier regret ≤ 0 (Theorem 14).  
  2) **audit.py:** `_evaluate_criteria` resolves metric via `_resolve_metric` and compares to threshold with direction `<=` (`audit.py:332-418`). No gating except metric presence.  
  3) **runner.py:** `aggregate_metrics` averages per-run regret_brier across runs (`runner.py:374-412`); calibration diagnostics added later (`runner.py:263-287`).  
  4) **b1_correction_no_regret.py:** Per-run regret_brier set as model brier − uncorrected brier (`b1_correction_no_regret.py:266-268`); capopm uses Stage1+Stage2 (`b1_correction_no_regret.py:495-518`), baseline uncorrected from `base_out` without corrections (`b1_correction_no_regret.py:178-188,536-544`). Stressors: liquidity high (`apply_liquidity_level`, `b1_correction_no_regret.py:782-793`), no longshot/herding/attack (sweep params in scenario name). Upstream: simulate_market (`market_simulator.py:73-148`), counts (`likelihood.py:33-43`), capopm_pipeline with corrections (`posterior.py:28-135`; Stage1 `stage1_behavioral.py:62-115`; Stage2 mixture `stage2_structural.py:144-199`).  
  5) **Upstream modules:** No additional gates; regret uses same p_true/outcomes as baseline.  
- **Paper alignment:** Theorem 14 (CAPOPM.pdf, text index ~92214) concerns impossibility of approximating multimodal mixture by a single Beta; it does not state that corrections reduce Brier regret vs uncorrected. The audit criterion over-claims relative to the theorem.  
- **Finite-sample testability:** Testable? NO. With n_runs=3 and stressors off (attack0, longshot0, herd0) the sign of mean regret is dominated by sampling noise; Theorem 14 offers no finite-sample dominance guarantee. Limitation stems from claim nature (overreach) and synthetic design (tiny n).  
- **Judgement calls & edge cases:** Asymmetric comparison (Stage1+Stage2 vs uncorrected); calibration `NOT_INTERPRETABLE` (n_samples=3, degenerate binning); regret averaged without CI in audit; potential sensitivity to Stage2 regime weights/clamping when mixtures close to single regime.  
- **Fix proposal (documentation only):** Either align criterion to a paper-backed property (e.g., mixture vs unimodal divergence) or require uncertainty-aware dominance with adequate n_runs; otherwise mark criterion as exploratory. Post-fix audit should not treat small positive regret (0.0029) as theorem failure without CI and paper support.

### regret_log_bad_non_positive — Scenario B1_correction_no_regret__longshot0__herd0__attack0__liqhigh__seed1 (FAIL)
- **Audit evidence:** metric_path `["aggregated_metrics","capopm","regret_log_bad"]`; direction `<=`; threshold `0.0`; metric_value `0.0272023516384929`; evaluated `true`; pass `false`; reason `null`. Same flags as above: low_n_runs, calibration not interpretable, no grid gate.  
- **Backward trace:**  
  1) **audit_contracts.py:** Defined at `src/capopm/experiments/audit_contracts.py:101-110` expecting CAPOPM log-score regret (uncorrected log − model log) ≤ 0 (Theorem 14).  
  2) **audit.py:** Comparison via `_evaluate_criteria` (`audit.py:332-418`).  
  3) **runner.py:** Aggregation by mean (`runner.py:374-412`); calibration post-processing (`runner.py:263-287`).  
  4) **b1_correction_no_regret.py:** Per-run regret_log_bad = uncorrected.log_score − model.log_score (`b1_correction_no_regret.py:268-270`); capopm uses Stage1+Stage2; baseline uncorrected without corrections. Stress settings: high liquidity, no longshot/herd/attack.  
  5) **Upstream:** capopm_pipeline with Stage1/Stage2 corrections (`posterior.py:28-135`; Stage1 weights `stage1_behavioral.py:62-115`; Stage2 mixture `stage2_structural.py:144-199`); simulation/likelihood as above.  
- **Paper alignment:** Theorem 14 provides no log-score dominance claim; audit overstates theorem content.  
- **Finite-sample testability:** NO. With n_runs=3 and no stressor, log-score differences are noisy; claim not implied by paper. Limitation from claim overreach and small synthetic sample.  
- **Judgement calls & edge cases:** Asymmetric corrections vs baseline; no CI or variance estimate; calibration degenerate; Stage2 mixture may introduce variability; log regret defined as “bad if positive” but small sample may flip sign.  
- **Fix proposal:** Re-map criterion to paper-backed measure or use CI-based evaluation with sufficient runs; otherwise mark as exploratory. Audit should not treat small positive regret (0.027) as theorem failure without paper support.

### regret_brier_non_positive — Scenario B1_correction_no_regret__longshot0__herd0__attack100__liqlow__seed2 (FAIL)
- **Audit evidence:** metric_path `["aggregated_metrics","capopm","regret_brier"]`; direction `<=`; threshold `0.0`; metric_value `0.002453663228344762`; evaluated `true`; pass `false`; reason `null`. Calibration in audit.json not interpretable (n_samples=3); low_n_runs implied; no grid gate.  
- **Backward trace:**  
  1) **audit_contracts.py:** Same dominance criterion (`audit_contracts.py:91-100`).  
  2) **audit.py:** Metric resolution/comparison in `_evaluate_criteria` (`audit.py:332-418`).  
  3) **runner.py:** Aggregated regret_brier mean (`runner.py:374-412`), calibration post-processing (`runner.py:263-287`).  
  4) **b1_correction_no_regret.py:** Per-run regret_brier defined at `b1_correction_no_regret.py:266-268`; scenario stressors: timing_attack_strength=1.0 (applied via `apply_strategic_timing_attack`, `b1_correction_no_regret.py:372-417`), low liquidity settings (`apply_liquidity_level`, `b1_correction_no_regret.py:782-793`), no longshot/herd. capopm uses Stage1+Stage2; baseline uncorrected lacks corrections.  
  5) **Upstream:** capopm_pipeline with corrections; simulate_market and likelihood as above.  
- **Paper alignment:** Theorem 14 (mixture approximation) does not assert regret dominance under timing attacks or liquidity stress. Audit over-asserts theorem.  
- **Finite-sample testability:** NO. n_runs=3 and high-attack stress yield noisy regret; paper lacks finite-sample dominance claim. Limitation from claim nature and small synthetic design.  
- **Judgement calls & edge cases:** Asymmetric corrections vs uncorrected baseline; timing attack and low liquidity may shift p_true distribution; calibration degenerate; no CI used; positive regret small (0.00245).  
- **Fix proposal:** Align criterion to paper-backed statement or evaluate with CI and adequate runs across stress grid; otherwise classify as exploratory.

### regret_log_bad_non_positive — Scenario B1_correction_no_regret__longshot0__herd0__attack100__liqhigh__seed3 (FAIL)
- **Audit evidence:** metric_path `["aggregated_metrics","capopm","regret_log_bad"]`; direction `<=`; threshold `0.0`; metric_value `0.007127078319609459`; evaluated `true`; pass `false`; reason `null`. Calibration not interpretable (n_samples=3); low_n_runs; no grid gate.  
- **Backward trace:**  
  1) **audit_contracts.py:** Log-score regret dominance (`audit_contracts.py:101-110`).  
  2) **audit.py:** Comparison logic in `_evaluate_criteria` (`audit.py:332-418`).  
  3) **runner.py:** Aggregated mean; calibration diagnostics (`runner.py:263-287,374-412`).  
  4) **b1_correction_no_regret.py:** Per-run regret_log_bad at `b1_correction_no_regret.py:268-270`; stressors: timing_attack_strength=1.0 (attack applied), high liquidity (`apply_liquidity_level`), no longshot/herd. capopm uses Stage1+Stage2; baseline uncorrected is correction-free.  
  5) **Upstream:** capopm_pipeline with Stage1/Stage2; simulate_market; likelihood counts.  
- **Paper alignment:** Theorem 14 does not guarantee log-score improvement; criterion is not implied.  
- **Finite-sample testability:** NO. n_runs=3; stress regime; paper lacks dominance claim. Limitation from claim overreach and small synthetic design.  
- **Judgement calls & edge cases:** Asymmetric corrections vs uncorrected; calibration degenerate; log regret sensitive to extreme p_hat; no CI.  
- **Fix proposal:** Remap to paper-backed measure or introduce uncertainty-aware evaluation with sufficient runs; otherwise treat as exploratory.

## AUDIT SUMMARY
- Files inspected: results/paper_artifacts/claim_table.md; results/paper_artifacts/borderline_atlas.md; results/B1_correction_no_regret__*/audit.json and summary.json; src/capopm/experiments/audit_contracts.py; src/capopm/experiments/audit.py; src/capopm/experiments/runner.py; src/capopm/experiments/b1_correction_no_regret.py; src/capopm/likelihood.py; src/capopm/posterior.py; src/capopm/market_simulator.py; src/capopm/corrections/stage1_behavioral.py; src/capopm/corrections/stage2_structural.py; src/capopm/metrics/scoring.py; smoke_test_b1_correction_no_regret.py; docs/CAPOPM_paper.pdf.
- Smoke tests audited: smoke_test_b1_correction_no_regret.py (artifact/schema/determinism; assumes at least one non-positive capopm regret_brier at n_runs=3).
- Criteria traced: `regret_brier_non_positive` (fails in attack0-liqhigh-seed1; attack100-liqlow-seed2), `regret_log_bad_non_positive` (fails in attack0-liqhigh-seed1; attack100-liqhigh-seed3).
- Finite-sample untestable claims: All regret dominance assertions under Theorem 14; paper gives no dominance guarantee and runs use n_runs=3 per scenario.
- Paper–code–audit contradictions: Theorem 14 addresses multimodal mixture approximation, not regret dominance; audit maps it to regret ≤ 0. Calibration/coverage uninterpretable yet used in regret context; asymmetric comparisons (corrected vs uncorrected) not normalized.
- Unresolved ambiguities: Intended theoretical basis for regret dominance; whether Stage2 mixture behaviors under stress should be evaluated differently; absence of grid/CI in audit despite stress sweeps.
- What I need next: Direction on paper-backed criteria for B1 (if any) and whether to treat regret dominance as exploratory before any Stage B planning. 
