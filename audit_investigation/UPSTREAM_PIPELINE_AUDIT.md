# UPSTREAM PIPELINE AUDIT (Stage A — CAPOPM Core Modules)

## A) Simulation / Trader Dynamics
- **Responsibilities (inputs/outputs):** `simulate_market` consumes RNG, `MarketConfig`, and list of `Trader` objects; returns `trade_tape` (list of `Trade` with pools/prices before/after) and `pool_path` snapshots (`src/capopm/market_simulator.py:73-148`). Traders decide via `Trader.decide` using p_true, historical order flow `p_hist`, signal model, and realized_state (`src/capopm/trader_model.py:44-105`). Inputs: config fields `n_steps`, `arrivals_per_step`, pools, fee, signal_model, herding flag; Outputs: executed trades with implied probabilities.
- **Paper alignment:** Implements Phase 3 asymmetric information trading (Trader docstring references Assumptions A1–A6, `src/capopm/trader_model.py:1-10`). Herding is Phase 7 engineering extension (`trader_model.py:8-12`). The simulator enforces parimutuel odds updates; no direct theorem; uses Phase 3 assumptions.
- **Invariants & assumptions:** Pools must be positive (`market_simulator.py:86-92`); `Trader.decide` requires outcome∈{0,1} and signal_model in {bernoulli_p_true, conditional_on_state} (`trader_model.py:45-98`); herding_intensity ∈ [0,1] (`trader_model.py:107-113`); trade sizes >0 (`market_simulator.py:125-127`). If realized_state missing for conditional signals, raises (`trader_model.py:86-90`).
- **Failure modes / fallbacks:** Warning printed if using simplified signal model (`market_simulator.py:106-110`); no clipping of pools beyond positivity; herding applies linear blend—could mask true signal; empirical history resets only via windowing in Stage1 (not here). Violation: if trader list empty, raises.
- **Audit touchpoints:** All experiments depend on trade_tape/pool_path; metrics for prices/coverage use p_hat from posterior that depends on these trades. Any bias here propagates to Brier/log metrics used in audit_contracts (e.g., A1 capopm vs baselines, B1 regrets).
- **High-risk divergences:** Phase 3 assumptions simplified; herding optional; signal model “bernoulli_p_true” deviates from conditional-on-state; potential mismatch with paper’s equilibrium derivations. Realized_state random draw may differ from paper’s information structure; no adversary “whale” logic beyond trader types—needs confirmation against CAPOPM.pdf definitions.

## B) Priors
- **Structural prior:** `compute_q_str` maps structural params to surrogate q_str via logistic of moneyness and adjustments (`src/capopm/structural_prior.py:18-70`). Inputs params must include T,K>0, S0,V0>0 (`structural_prior.py:22-41`); optional jitter. Outputs q_str ∈ (0,1).
- **ML prior:** `simulate_p_ml` produces p_ML with bias/noise/calibration, clipped to (1e-12,1-1e-12) (`src/capopm/ml_prior.py:15-35`); `compute_r_ml` ∈[0,1] (`ml_prior.py:37-43`); `compute_n_ml` nonnegative (`ml_prior.py:46-55`).
- **Hybrid fusion:** `build_structural_beta`/`build_ml_beta` convert q_str/p_ML and pseudo-counts to Beta params (nonnegative checks) (`src/capopm/hybrid_prior.py:13-35`); `fuse_priors` adds pseudo-counts (`hybrid_prior.py:37-45`); `hybrid_weights` returns weights or zeros if total=0 (`hybrid_prior.py:47-55`).
- **Paper alignment:** Surrogate structural prior explicitly NOT the full Phase 1 Heston model (comment `structural_prior.py:1-7`) → “NO DIRECT PAPER SPEC” beyond interface. ML prior is synthetic; hybrid fusion reflects Phase 2 conjugate Beta blending per paper definitions (Proposition 6 uses posterior mean; not explicit about surrogate).
- **Invariants & assumptions:** Positivity of Beta params; probabilities in (0,1); calibration factor >0; pseudo-counts nonnegative; surrogate assumes smooth logit function—engineering choice.
- **Failure modes / fallbacks:** Clipping to [1e-12, 1-1e-12] may hide boundary issues; surrogate may misrepresent tail regimes; r_ml and n_ml scaling arbitrary; zero total pseudo-count leads to (0,0) weights → potential NaN in downstream if not handled.
- **Audit touchpoints:** Posterior mean/variance depend on these priors; affects metrics in A1 (Brier dominance), A3/B1 regrets, B2 rates, B4 mixture weights. Semantic divergence risk if surrogate prior deviates from paper’s structural density (Lemma 3/Theorem references).
- **High-risk divergences:** Structural surrogate vs paper Heston prior; ML prior synthetic with calibration; pseudo-count magnitudes may not match paper’s effective sample size assumptions; potential prior domination at low n.

## C) Likelihood → Posterior
- **Likelihood & update:** `beta_binomial_update` enforces 0<=y<=n, alpha0,beta0>0; returns alpha_post,beta_post (`src/capopm/likelihood.py:12-20`). `posterior_moments` provides mean/var (`likelihood.py:23-30`). `counts_from_trade_tape` sums YES and totals (`likelihood.py:33-43`).
- **Posterior pipeline:** `capopm_pipeline` (Phase 1–5) computes structural prior q_str, ML prior p_ML, fuses priors, applies Stage1/Stage2 corrections as configured, then Beta-Binomial update and prices/credible intervals (`src/capopm/posterior.py:28-135`). Stage1 applies behavioral weights (`stage1_behavioral.py:62-115`); Stage2 applies offsets or mixture (`stage2_structural.py:144-199`). Outputs include pi_yes/pi_no, alpha_post/beta_post, mixture weights if Stage2 mixture enabled.
- **Paper alignment:** Beta-Binomial matches Phase 4; posterior mean pricing matches Proposition 6 (posterior predictive). Stage1/Stage2 correspond to Phase 6 corrections; mixture relates to Theorem 15 (posterior concentration). Surrogate priors limit exact paper fidelity.
- **Invariants & assumptions:** Beta params positive; counts nonnegative; Stage1 weights clipped to [w_min,w_max] (`stage1_behavioral.py:62-114`); Stage2 mixture requires positive regime priors and counts (`stage2_structural.py:144-175`). Assumes Bernoulli outcomes; independence across trades implicit.
- **Failure modes / fallbacks:** Stage1 herding/longshot clipping may distort counts; Stage2 clamping of regime params (diagnostics stored but not surfaced, `stage2_structural.py:155-183`) can mask invalid regimes; mixture weights default to prior if likelihood equal; counts from trade tape ignore price impact beyond trades; surrogate priors may bias posteriors.
- **Audit touchpoints:** All experiment metrics derive from posterior outputs; A/B experiments audit Brier/log, coverage, regime entropy, regrets. Misalignment here propagates to all audit criteria.
- **High-risk divergences:** Surrogate prior + simplistic likelihood may not reflect paper’s continuous-time model; Stage1/Stage2 heuristics may break formal Bayesian semantics; mixture weights symmetry causes entropy plateaus (B4); counts ignore fee effects beyond implied_probs.

## D) Corrections
- **Stage1 behavioral:** Applies longshot/herding weights per trade, clipping weights to [w_min,w_max]; counts down/up weighted accordingly; summaries captured (`src/capopm/corrections/stage1_behavioral.py:62-115`). Assumes positive window length; herding_lambda ≥0.
- **Stage2 structural:** Computes regime offsets or mixture parameters; `mixture_posterior_params` builds regime-specific alpha/beta, log-likelihoods, and weights via softmax; clamps regime params if nonpositive (`src/capopm/corrections/stage2_structural.py:144-199`). Returns mixture_mean/var and diagnostics.
- **Paper alignment:** Stage1/Stage2 correspond to Phase 6 behavioral/structural corrections; Theorem 12/14/15 cover robustness/mixture properties. Implementation uses engineered weights/offsets, not fully derived from paper’s kernels.
- **Invariants & assumptions:** Stage1 weights positive; Stage2 regimes require priors_pi ≥0, alpha/beta >0; mixture weights sum to 1. Assumes independent regime components; no label switching handling.
- **Failure modes / fallbacks:** Stage1 can downweight/clip extremes silently; Stage2 clamping if regime params invalid; mixture weights revert to priors if likelihood flat, causing high entropy; diagnostics not surfaced to audits → potential false confidence.
- **Audit touchpoints:** B1/B3 regrets vs uncorrected depend on Stage1/Stage2 presence; B4 regime_entropy relies on mixture weights; A3 Stage1+Stage2 vs baselines; any clipping/clamping can alter audit metrics.
- **High-risk divergences:** Behavioral weights ad hoc vs paper; regime mixture symmetry causing no concentration; lack of diagnostics in audit may hide clamping or degeneracy; Stage2 offsets/modes may diverge from theoretical kernel regularization.

## E) Pricing / Projection
- **Pricing:** `posterior_prices` returns Beta mean YES/NO (`src/capopm/pricing.py:14-20`); credible intervals via `credible_intervals`/`beta_ppf` (`pricing.py:23-66`). Arbitrage-free projection normalizes nonnegative prices to sum 1 (`pricing.py:34-44`).
- **Projection utilities (experiments):** `project_probs` simplex projection (euclidean/kl), `projection_distance` (L1/L2/KL), `detect_violation` (`src/capopm/experiments/projection_utils.py:6-46`). Used in B5 to project deliberately violated probabilities (`b5_arbitrage_projection_impact.py:98-160`).
- **Paper alignment:** Posterior mean pricing aligns with Phase 5/Proposition 6; projection relates to Phase 7.8 and Theorem 13 (no-arbitrage preservation). Euclidean projection is engineering choice; Theorem 13 may assume kernel-based regularization rather than simplex projection.
- **Invariants & assumptions:** Arbitrage-free projection requires nonnegative probabilities; `arbitrage_free_projection` clamps and renormalizes (`pricing.py:34-44`). Projection_utils clamps probabilities to eps before projection; KL projection divides by sum.
- **Failure modes / fallbacks:** Clipping to eps may mask severe violations; projection method choice not validated against paper; distances depend on injected violation scaling (B5 uses VIOLATION_PUSH=0.35); no check that projection preserves other moments.
- **Audit touchpoints:** B5 audit criteria use `proj_l1`, `delta_brier`, `delta_log_score` from summary; A1/A3/B1 etc. rely on posterior_prices for p_hat; calibration/coverage uses credible_intervals. Divergence risk if projection differs from paper’s prescribed kernel regularization.
- **High-risk divergences:** Projection heuristic vs theoretical arbitrage correction; beta_ppf numerical tolerance could affect coverage; projection distances sensitive to eps; no guarantee scores improve post-projection in finite samples.

## F) Metrics & Scoring
- **Scoring:** Brier/log_score/mae defined with minimal guards (log_score clamps p_hat to [eps,1-eps], `src/capopm/metrics/scoring.py:10-26`). Inputs: p_true/outcome; outputs floats.
- **Calibration/coverage:** reliability_bins/ECE with fallback to equal_mass if bins sparse; diagnostics track degenerate binning (`src/capopm/metrics/calibration.py:14-145`). Interval coverage for outcome and p_true (`calibration.py:149-173`). reliability_table uses binning mode actually used (`calibration.py:187-214`).
- **Paper alignment:** ECE/coverage are Phase 7 interpretability checks, not explicit theorems; coverage relates to credible intervals in Phase 5 but thresholds/tolerances are engineering. Scoring rules are standard proper scores (paper uses for claims like Proposition 6 dominance, but implementations are straightforward).
- **Invariants & assumptions:** p_hat in [0,1]; outcomes 0/1; n_bins>0; min_nonempty_bins enforced; ECE fallback if nonempty<min_nonempty_bins and length sufficient (`calibration.py:124-134`). Degeneracy flagged if nonempty<=2 or unique<=2 (`calibration.py:136-144`).
- **Failure modes / fallbacks:** allow_fallback=True changes binning silently; degenerate bins produce ECE but marked; NaNs propagate if lists empty (guards). Coverage uses Beta quantiles; numerical tolerance not exposed. Clamping in log_score prevents -inf but biases scores when p_hat near 0/1.
- **Audit touchpoints:** Brier/log feed all audit criteria; ECE/coverage flags show in borderline_atlas; B1/B3 regrets computed from scores; B2 slopes use bias/variance metrics derived from posterior_mean/variance; B4 entropy independent of calibration but uses aggregated metrics structure.
- **High-risk divergences:** ECE fallback changes semantics vs fixed equal-width bins; scoring clamps alter extremes; coverage tolerance in audit (COVERAGE_TOLERANCE) may not align with paper nominal guarantees.

## G) Aggregation Boundary (runner.py / audit.py)
- **Responsibilities:** `run_experiment` orchestrates per-run simulation → posterior → metrics, aggregates via mean, computes calibration, tests, and writes outputs (`src/capopm/experiments/runner.py:49-329`). `aggregate_metrics` averages non-None metrics, NaN-safe, and sets coverage aliases (`runner.py:374-412`). `write_scenario_outputs` persists metrics/tests/reliability and calls `run_audit_for_results` (`runner.py:479-789`). `audit.py` evaluates criteria using metric paths and handles gating (grid_missing, conditional_on_violation) (`src/capopm/experiments/audit.py:332-418`).
- **Paper alignment:** Runner is engineering layer; audit contracts map to paper theorems (Proposition 6, Theorem 7, 12, 13, 14, 15) as declared in `audit_contracts.py`. Aggregation by mean is not a paper mandate; audit logic enforces thresholds/flags per paper_config (grid/runs) but limited per-scenario aggregation.
- **Invariants & assumptions:** Requires ≥2 predictions for ECE (raises otherwise, `runner.py:265-270`); assumes binary outcomes; uses allow_fallback for calibration; tests assume paired lists; audit assumes grid_points_observed=1 per scenario (no cross-scenario aggregation).
- **Failure modes / fallbacks:** mean aggregation hides variance; NaN handling drops None; calibration fallback and degenerate bins; audit grid gate per scenario → grid_missing_for_claim even when multiple scenarios exist; conditional_on_violation auto-passes when violation_strength<=0. Audit overrides recorded status if semantics mismatch.
- **Audit touchpoints:** All audit criteria resolved via `_resolve_metric` on summary/aggregated; path errors yield metric_missing. Divergence risk if recorded status differs; per-scenario grid counting misrepresents sweeps; mean aggregation may misalign with paper asymptotic rates.
- **High-risk divergences:** Per-scenario grid isolation; mean aggregation vs quantile-based claims; allow_fallback calibration altering metrics; conditional_on_violation marking passes without evaluation.

---

## AUDIT SUMMARY
- Modules inspected: market_simulator.py; trader_model.py; structural_prior.py; ml_prior.py; hybrid_prior.py; likelihood.py; posterior.py; corrections/stage1_behavioral.py; corrections/stage2_structural.py; pricing.py; experiments/projection_utils.py; metrics/scoring.py; metrics/calibration.py; experiments/runner.py; experiments/audit.py.
- Key invariants: probabilities in [0,1]; Beta params >0; trade sizes >0; herding_intensity∈[0,1]; ECE binning requires ≥2 predictions and min_nonempty_bins; projection normalizes to simplex; mixture weights sum to 1.
- Highest-risk fallbacks: calibration fallback to equal_mass; Stage1 weight clipping; Stage2 clamping/regime symmetry; projection eps/clipping; surrogate structural prior; audit grid counting per scenario.
- Major paper–code gaps: Structural prior surrogate vs Phase 1 model; audit dominance/entropy claims not directly in theorems; projection heuristics vs Theorem 13 kernel regularization; per-scenario grid gating vs multi-scenario sweeps.
- What I need next: Confirmation on intended structural prior vs paper model; clarity on evidence/grid aggregation in audit; validation of projection method against Theorem 13 assumptions; any missing coverage module (no coverage.py found) — coverage relies on calibration.py interval functions. 
