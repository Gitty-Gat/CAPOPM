# B3.MISSPECIFICATION_REGRET_GRID (Stage A — Investigative Audit)

Top-ranked by failure count (11 non-passing/indeterminate criteria across 4 scenarios). All runs are smoke-level (n_runs=3), so metrics are noisy and calibration/coverage gates are mostly uninterpretable.

## Scenario: B3_misspec_regret__struct0__ml0__seed0 (structural_shift=0.0, ml_bias=0.0, n_runs=3)

### Criterion: regret_brier_non_positive — **FAIL**
- **Claim header:** experiment `B3.MISSPECIFICATION_REGRET_GRID`; scenario `B3_misspec_regret__struct0__ml0__seed0`; theorem `Proposition 8`; criterion `regret_brier_non_positive`.
- **Audit evidence (audit.json):** id `regret_brier_non_positive`; description “Mean Brier regret vs uncorrected should be ≤ 0 across grid points.”; metric_path `["aggregated_metrics","capopm","regret_brier"]`; comparator_path `null`; direction `<=`; threshold `0.0`; metric_value `0.0002719293842306469`; evaluated `true`; pass `false`; reason `null`. `criteria_semantics_mismatch`: `false`; `grid_missing_for_claim`: not triggered for this criterion. Calibration: all models `ece_status=NOT_INTERPRETABLE` (n_samples=3, n_unique_predictions=3, n_nonempty_bins=2). Coverage flags present: `overall_90_off_nominal`, `overall_95_off_nominal`. Borderline: `degenerate_calibration_models` for all models; `low_n_runs=true`.
- **Trace:** contract defines metric in `src/capopm/experiments/audit_contracts.py:152-158` (capopm regret_brier ≤ 0). Audit evaluation resolves metric via `_evaluate_criteria` and `_resolve_metric` in `src/capopm/experiments/audit.py:332-418` (metric pulled from `aggregated_metrics` in summary; compared with `threshold=0.0`, direction `<=`). `regret_brier` is the mean across runs produced in `run_b3_scenario` (`src/capopm/experiments/b3_misspecification_regret_grid.py:190-194`) after per-run regrets are computed against the uncorrected baseline (`base_out` built with stage1/stage2 disabled at `108-116`). Per-run metrics come from model outputs (`121-169`) which feed aggregated means via `aggregate_metrics` (`src/capopm/experiments/runner.py:374-412`). Summary status uses the same aggregated regrets (`b3_misspecification_regret_grid.py:455-473`) and is written to `summary.json` then audited by `run_audit_for_results` (`src/capopm/experiments/runner.py:479-789`). Upstream data: trade_tape from `simulate_market` (`src/capopm/market_simulator.py:73-148`), counts `y,n` via `counts_from_trade_tape` (`src/capopm/likelihood.py:33-43`), posterior via `capopm_pipeline` with stage1 behavioral weights enabled for capopm (`src/capopm/posterior.py:28-135` calling `apply_behavioral_weights` `src/capopm/corrections/stage1_behavioral.py:62-115`), structural/ML misspec parameters set in `apply_structural_mis` and `apply_ml_mis` (`src/capopm/experiments/b3_misspecification_regret_grid.py:343-355`).
- **Paper alignment:** Proposition 8 in `docs/CAPOPM_paper.pdf` states continuity of the posterior price in the Beta parameters (p. around text index ~87674), not a regret dominance claim. The audit criterion therefore over-claims relative to the paper. Even if regret were intended, n_runs=3 is far below paper-ready `MIN_RUNS_PER_CELL=30` (`src/capopm/experiments/paper_config.py:10-24`), so the metric is a noisy point estimate rather than a validated grid mean.
- **Judgement calls / edge cases:** Tiny positive regret (2.7e-4) arises from averaging only 3 runs (run 1 had capopm Brier worse than uncorrected). Calibration/coverage gates are non-interpretable; aggregated means ignore NaN but keep small-sample variance. Stage1 weights apply only to capopm, so regret compares a weighted pipeline vs unweighted baseline—sensitive to streak weighting with almost no data.
- **Fix proposal (do not implement):** (1) Align contract to paper by replacing this regret gate with a continuity/robustness check actually implied by Proposition 8, or remap to a theorem that supports regret dominance if available. (2) Re-run the full misspecification grid with ≥30 runs per grid point using `paper_config.PAPER_GRIDS["B3"]` to reduce small-sample sign flips; optionally pool regrets across the grid before per-scenario auditing to reflect the “across grid points” wording. Risks: changing the claim mapping may reduce comparability with prior audits; pooling grid points must avoid masking genuine misspecification failures.

### Criterion: regret_log_bad_non_positive — **FAIL**
- **Claim header:** experiment `B3.MISSPECIFICATION_REGRET_GRID`; scenario `B3_misspec_regret__struct0__ml0__seed0`; theorem `Proposition 8`; criterion `regret_log_bad_non_positive`.
- **Audit evidence:** metric_path `["aggregated_metrics","capopm","regret_log_bad"]`; direction `<=`; threshold `0.0`; metric_value `0.004103238400468705`; evaluated `true`; pass `false`. Same calibration/coverage/context flags as above; `overall_pass=false`; `criteria_semantics_mismatch=false`.
- **Trace:** same contract/evaluation path as above; metric sourced from aggregated per-run log-score regret computed in `b3_misspecification_regret_grid.py:190-194` (uncorrected log_score minus capopm log_score) averaged by `aggregate_metrics` (`runner.py:374-412`). Summary embeds these values (`b3_misspecification_regret_grid.py:455-473`) and audit consumes them via `_resolve_metric` (`audit.py:364-372`).
- **Paper alignment:** Proposition 8 (continuity) does not assert log-score regret dominance. No theorem in CAPOPM.pdf identified that directly bounds log regret under misspecification for finite-sample grids; criterion is unsupported without an alternative citation.
- **Judgement calls / edge cases:** With only 3 runs, log-score regret is dominated by one run where capopm underperformed; bootstrap/variance not checked. Stage1 weighting vs unweighted baseline amplifies sensitivity. Calibration uninterpretable.
- **Fix proposal:** Same as Brier regret—either remap the theorem or soften the criterion to a directional effect size with uncertainty (e.g., CI crossing zero) once paper-ready runs are available. Running the full grid with paper-ready sample sizes should clarify whether the positive regret is sampling noise.

### Criterion: grid_requirement — **INDETERMINATE (grid_missing_for_claim)**
- **Claim header:** experiment `B3.MISSPECIFICATION_REGRET_GRID`; scenario `B3_misspec_regret__struct0__ml0__seed0`; theorem `Proposition 8`; criterion `grid_requirement`.
- **Audit evidence:** metric_path `null`; evaluated `false`; pass `null`; reason `grid_missing_for_claim`; comparator_path/direction/threshold `null`. `grid_points_observed` hardcoded as 1 in audit; `grid_axes` detected from sweep_params = `["ml_bias","structural_shift"]`.
- **Trace:** `_grid_requirement` marks `requires_grid=True` (`audit_contracts.py:168-171`). `_evaluate_criteria` sets `grid_points_observed=1` and requires at least `thresholds.paper_ready_min_grid=2` (`audit.py:342-359` with threshold set from `paper_config.PAPER_READY_MIN_GRID=2`). Because each scenario is audited independently, the presence of the other grid points is ignored. The same per-run/per-metric pipeline as above supplies the summary, but the criterion short-circuits before metric resolution.
- **Paper alignment:** Proposition 8 continuity is inherently about behavior across varying inputs, so requiring multiple grid points is reasonable, but the audit layer does not aggregate across the four grid scenarios already present; the failure is an audit bookkeeping gap rather than absence of grid data.
- **Judgement calls / edge cases:** Single-scenario audit cannot satisfy grid requirement even when the suite has the full 2x2 grid; `grid_points_observed` is constant 1. Low n_runs also prevents paper-readiness.
- **Fix proposal:** Audit should aggregate grid coverage across scenarios before applying `grid_requirement` (e.g., using registry snapshot or paper suite manifest) instead of per-scenario gating. Alternatively, mark the criterion as satisfied when all grid points in `PAPER_GRIDS["B3"]` exist in the registry. Risk: must avoid counting incomplete grids; ensure robustness to missing runs.

## Scenario: B3_misspec_regret__struct0__ml5__seed1 (structural_shift=0.0, ml_bias=0.05, n_runs=3)

### Criterion: regret_brier_non_positive — **FAIL**
- **Audit evidence:** metric_value `0.000343778557755281`; direction `<=`; threshold `0.0`; evaluated `true`; pass `false`; reason `null`. Calibration: all models `NOT_INTERPRETABLE` (n_samples=3). Coverage flags: `overall_90_off_nominal`, `overall_95_off_nominal`. Borderline: low_n_runs, degenerate calibration across all models.
- **Trace:** contract at `audit_contracts.py:152-158`; evaluation in `audit.py:332-418`; metric originates from per-run regrets (`b3_misspecification_regret_grid.py:190-194`) aggregated by `aggregate_metrics` (`runner.py:374-412`) and recorded in `summary["status"]["metrics"]["mean_regret_brier"]` (`b3_misspecification_regret_grid.py:470-472`). Upstream pipeline identical to previous scenario with ml bias applied in `apply_ml_mis` (`b3_misspecification_regret_grid.py:350-355`) shifting ML bias to +0.03.
- **Paper alignment:** Same mismatch: Proposition 8 (price continuity) does not guarantee regret ≤ 0. Sample size (3 runs) and lack of grid aggregation make the positive regret (3.4e-4) non-probative.
- **Judgement calls / edge cases:** One run with worse Brier drives sign; stage1 vs uncorrected baseline asymmetry; calibration/coverage uninterpretable.
- **Fix proposal:** Align theorem mapping or add uncertainty-aware regret check after paper-ready reruns with the full grid.

### Criterion: regret_log_bad_non_positive — **FAIL**
- **Audit evidence:** metric_value `0.004146022144651688`; direction `<=`; threshold `0.0`; evaluated `true`; pass `false`. Calibration and coverage flags identical to the Brier criterion here.
- **Trace:** same path as above; log regret computed per run (`b3_misspecification_regret_grid.py:190-194`) and averaged (`runner.py:374-412`); evaluated in `audit.py:332-418`.
- **Paper alignment:** No direct Proposition 8 support for log regret dominance; small-sample noise likely.
- **Judgement calls / edge cases:** Positive regret entirely from small run count; no CI/variance considered.
- **Fix proposal:** Same as above—rerun with sufficient n and revisit audit mapping to a paper-backed claim.

### Criterion: grid_requirement — **INDETERMINATE**
- **Audit evidence:** evaluated `false`; reason `grid_missing_for_claim`; requires_grid `true` from `_grid_requirement`. Same calibration/coverage context as above.
- **Trace:** `_evaluate_criteria` short-circuits with `grid_points_observed=1` (`audit.py:342-359`) despite sweep_params containing two axes; grid threshold set by `paper_config.PAPER_READY_MIN_GRID=2`.
- **Paper alignment:** Multi-point grid is appropriate, but audit ignores other grid scenarios; bookkeeping issue.
- **Judgement calls / edge cases:** Low n_runs and per-scenario grid counting.
- **Fix proposal:** Aggregate grid coverage across scenarios before applying the grid gate; ensure completeness of the 2x2 grid.

## Scenario: B3_misspec_regret__struct10__ml0__seed100 (structural_shift=0.1, ml_bias=0.0, n_runs=3)

### Criterion: regret_brier_non_positive — **FAIL**
- **Audit evidence:** metric_value `2.7232727000817412e-05`; direction `<=`; threshold `0.0`; evaluated `true`; pass `false`; reason `null`. Calibration: `NOT_INTERPRETABLE` (n_samples=3). Borderline: low_n_runs=true; no coverage flags recorded (n=3 overall, extreme_p empty).
- **Trace:** Same contract and evaluation path (`audit_contracts.py:152-158`, `audit.py:332-418`). Metric arises from per-run regret means (`b3_misspecification_regret_grid.py:190-194`) after structural S0 is scaled by 1.1 in `apply_structural_mis` (`343-348`). Aggregation via `aggregate_metrics` (`runner.py:374-412`) feeds audit metric resolution.
- **Paper alignment:** Proposition 8 continuity does not entail regret dominance; sample size 3 prevents reliable sign determination.
- **Judgement calls / edge cases:** Regret is barely positive (2.7e-5), effectively numerical noise; calibration non-interpretable; grid still uncounted.
- **Fix proposal:** Treat as inconclusive until paper-ready grid runs; consider replacing hard ≤0 gate with CI-based check or aligning to the paper’s continuity statement.

### Criterion: grid_requirement — **INDETERMINATE**
- **Audit evidence:** evaluated `false`; reason `grid_missing_for_claim`; requires_grid `true`. Calibration: `NOT_INTERPRETABLE`; low_n_runs=true.
- **Trace:** Grid short-circuit in `_evaluate_criteria` (`audit.py:342-359`); paper grid threshold from `paper_config.PAPER_READY_MIN_GRID=2`; audit counts only current scenario.
- **Paper alignment:** Appropriate to require grid, but audit not aggregating across existing grid points.
- **Judgement calls / edge cases:** Per-scenario audit prevents satisfying grid; n_runs insufficient.
- **Fix proposal:** Aggregate grid coverage across scenarios; rerun with sufficient runs.

## Scenario: B3_misspec_regret__struct10__ml5__seed101 (structural_shift=0.1, ml_bias=0.05, n_runs=3)

### Criterion: regret_brier_non_positive — **FAIL**
- **Audit evidence:** metric_value `0.00017825657428397046`; direction `<=`; threshold `0.0`; evaluated `true`; pass `false`. Calibration: `NOT_INTERPRETABLE` (n_samples=3). Borderline: low_n_runs=true; no coverage flags recorded.
- **Trace:** Same contract/evaluation route (`audit_contracts.py:152-158`, `audit.py:332-418`). Metric computed per run with combined structural and ML misspecification (S0 scaled ×1.1, ml bias +0.05) in `b3_misspecification_regret_grid.py:343-355`, regrets in `190-194`, aggregated in `runner.py:374-412`.
- **Paper alignment:** As above, Proposition 8 continuity does not guarantee regret dominance; small sample and mixed misspecification leave sign uncertain.
- **Judgement calls / edge cases:** Positive regret dominated by single run; calibration/coverage uninterpretable.
- **Fix proposal:** Re-run with paper-ready grid and reconsider criterion mapping to the paper statement.

### Criterion: regret_log_bad_non_positive — **FAIL**
- **Audit evidence:** metric_value `0.0028212555526665093`; direction `<=`; threshold `0.0`; evaluated `true`; pass `false`. Calibration: `NOT_INTERPRETABLE`; borderline low_n_runs=true.
- **Trace:** Same path as other log-regret failures—per-run computation (`b3_misspecification_regret_grid.py:190-194`), aggregation (`runner.py:374-412`), audit evaluation (`audit.py:332-418`).
- **Paper alignment:** No Proposition 8 backing for log regret dominance; finite-sample noise likely.
- **Judgement calls / edge cases:** Minimal runs; asymmetry between stage1-corrected capopm and unweighted baseline.
- **Fix proposal:** Same as above—paper-ready rerun and criterion/theorem remapping.

### Criterion: grid_requirement — **INDETERMINATE**
- **Audit evidence:** evaluated `false`; reason `grid_missing_for_claim`; requires_grid `true`. Calibration: `NOT_INTERPRETABLE`; low_n_runs=true.
- **Trace:** Grid gate short-circuited at `audit.py:342-359`; grid threshold from `paper_config.PAPER_READY_MIN_GRID=2`; audit ignores other grid scenarios.
- **Paper alignment:** Grid requirement conceptually valid, but implementation fails to consider full grid coverage.
- **Judgement calls / edge cases:** Per-scenario audit plus small n.
- **Fix proposal:** Aggregate across grid points before applying the gate; rerun with paper-ready n.

---

AUDIT SUMMARY
- Files inspected: results/paper_artifacts/claim_table.md; results/paper_artifacts/borderline_atlas.md; src/capopm/experiments/audit_contracts.py; src/capopm/experiments/audit.py; src/capopm/experiments/runner.py; src/capopm/experiments/b3_misspecification_regret_grid.py; src/capopm/likelihood.py; src/capopm/posterior.py; src/capopm/market_simulator.py; src/capopm/corrections/stage1_behavioral.py; docs/CAPOPM_paper.pdf; results/B3_misspec_regret__struct0__ml0__seed0/audit.json (+summary.json); results/B3_misspec_regret__struct0__ml5__seed1/audit.json (+summary.json); results/B3_misspec_regret__struct10__ml0__seed100/audit.json (+summary.json); results/B3_misspec_regret__struct10__ml5__seed101/audit.json (+summary.json).
- Criteria traced: `regret_brier_non_positive`, `regret_log_bad_non_positive`, `grid_requirement` for all four B3 scenarios.
- Key judgement calls: contract mapped Proposition 8 to regret dominance despite paper Proposition 8 covering price continuity; grid gate failing because audit counts only current scenario (grid_points_observed=1); small-sample (n_runs=3) runs treated as evidence though below paper_ready thresholds.
- Most concerning edge cases: calibration/coverage not interpretable; regret compares stage1-weighted capopm to unweighted baseline; tiny positive regrets likely sampling noise; grid requirement structurally impossible per-scenario.
- Contradictions: audit contract claims (Proposition 8 → regret ≤ 0) conflict with CAPOPM.pdf Proposition 8 statement (continuity, not regret); grid requirement reports missing grid even though four grid points exist across scenarios.
- What is needed next: approval to proceed to Stage B with a plan to (1) remap/adjust Proposition 8 audit criteria, (2) redesign grid aggregation across scenarios, and (3) rerun B3 with paper-ready grid and run counts.
