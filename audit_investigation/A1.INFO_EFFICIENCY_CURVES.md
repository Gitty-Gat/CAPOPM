# A1.INFO_EFFICIENCY_CURVES (Stage A — Investigative Audit)

## Topology & Execution Model — A1.INFO_EFFICIENCY_CURVES
- **Experiment definition:** `src/capopm/experiments/a1_info_efficiency_curves.py:31-87` defines `EXPERIMENT_ID="A1.INFO_EFFICIENCY_CURVES"`, sweep over signal_quality × informed_share × adversarial_share via `run_a1_info_efficiency_curves` and scenario naming in `scenario_id` (`a1_info_efficiency_curves.py:89-96`).
- **Signal quality parameterization:** Default grid `DEFAULT_SIGNAL_QUALITY=[0.60,0.75,0.90]` (`a1_info_efficiency_curves.py:20-24`); each scenario sets trader params `signal_quality` for informed and adversarial traders (`a1_info_efficiency_curves.py:69-75`). In the audited scenario, metadata captures `signal_quality=0.65` (`results/a1_info_eff_q65_inf30_adv10_seed99101/summary.json`).
- **Scenario enumeration & naming:** Cartesian product of signal_quality/informed_share/adversarial_share; noise_share = 1 − informed − adversarial, skip if <=0 (`a1_info_efficiency_curves.py:48-53`). Seed = base_seed + index; scenario_name = `a1_info_eff_q{rho_pct}_inf{inf_pct}_adv{adv_pct}_seed{seed}` (`a1_info_efficiency_curves.py:54-56,89-96`). For this audit: scenario `a1_info_eff_q65_inf30_adv10_seed99101` (metadata in audit/summary).
- **Configuration truth:** Base config with Stage1+Stage2 enabled and model list in `build_base_config` (`a1_info_efficiency_curves.py:98-193`). Paper-ready grids live in `paper_config.PAPER_GRIDS["A1"]` (`src/capopm/experiments/paper_config.py:31-36`, min grid points=2 at `paper_config.py:13,24`). Actual run uses snapshot settings in `summary.json` (n_runs=4; p_true_dist beta(2,2); trader proportions overridden by sweep params).
- **Metric flow:**  
  simulator `simulate_market` (`src/capopm/market_simulator.py:73-148`) → likelihood counts `counts_from_trade_tape` (`src/capopm/likelihood.py:33-43`) → posterior hybrid priors + Stage1/Stage2 via `capopm_pipeline` (`src/capopm/posterior.py:28-135`), Stage1 weights if enabled (`src/capopm/corrections/stage1_behavioral.py:62-115`), Stage2 mixture offsets if enabled (`src/capopm/corrections/stage2_structural.py:144-199`) → pricing/posterior probabilities `posterior_prices` inside pipeline → per-run metrics (brier/log_score/mae etc.) computed in `run_experiment` (`src/capopm/experiments/runner.py:183-258`) → aggregation by mean in `aggregate_metrics` (`runner.py:374-412`) with calibration ECE computed post-aggregation (`runner.py:263-287`) → summary/results written via `write_scenario_outputs` (`runner.py:479-789`) → audit evaluation `_evaluate_criteria` (`src/capopm/experiments/audit.py:332-418`) driven by `audit_contracts.py` (`a1` contract at `src/capopm/experiments/audit_contracts.py:28-47`).
- **Smoke tests touching pipeline:** `smoke_test_a1_info_efficiency.py:1-120` invokes `run_a1_info_efficiency_curves` on a single grid point (q=0.65, inf=0.30, adv=0.10, n_runs=4), validates schemas for metrics/tests/summary/reliability, checks no NaNs in core metrics, and enforces determinism by re-running with identical seeds; it does not assert Brier dominance or grid monotonicity.

## Smoke Test Audit (A1)
- **smoke_test_a1_info_efficiency.py:1-120**  
  - **Assumption encoded:** Artifacts exist; schema validity; CAPOPM core metrics are finite; outputs are deterministic across reruns for the same seed/grid. No explicit dominance/monotonicity checks; no Stage1/Stage2 bypass.  
  - **Pipeline coverage:** Full runner path (simulate_market → posterior → metrics → aggregation → outputs) via `run_a1_info_efficiency_curves`, including Stage1/Stage2 defaults and reliability table generation.  
  - **Relation to CAPOPM.pdf:** Determinism and schema validity are implicit engineering requirements, not stated theorems (implicit). No direct tie to Proposition 6 or Lemma 3; no probabilistic guarantee is validated.  
  - **Tension:** None on math claims; test stops short of validating Brier dominance or info-efficiency monotonicity.

## Criterion-Level Investigation

### capopm_dominates_raw_parimutuel_brier — Scenario a1_info_eff_q65_inf30_adv10_seed99101 (FAIL)
- **Audit evidence (results/a1_info_eff_q65_inf30_adv10_seed99101/audit.json):** metric_path `["aggregated_metrics","capopm","brier"]`; comparator_path `["aggregated_metrics","raw_parimutuel","brier"]`; direction `<=`; threshold `0.06664770023922015`; metric_value `0.07024771984812316`; evaluated `true`; pass `false`; reason `null`. Flags: `low_n_runs=true` (seed_grid_coverage n_runs=4), coverage flags numerous (`overall_90_off_nominal`, `overall_95_off_nominal`, `extreme_p_90_off_nominal`, `extreme_p_95_off_nominal`), calibration `NOT_INTERPRETABLE` (n_samples=4, degenerate binning). No semantics mismatch/status mismatch.
- **Backward trace:**  
  1) **audit_contracts.py**: Criterion defined at `src/capopm/experiments/audit_contracts.py:35-42` requiring CAPOPM Brier ≤ raw parimutuel (Prop 6).  
  2) **audit.py**: `_evaluate_criteria` loads metric/comparator via `_resolve_metric` and compares with direction `<=` (`src/capopm/experiments/audit.py:332-418`); uses metric value from aggregated metrics; no grid gate here.  
  3) **runner.py**: Aggregated metrics are mean over per-run `brier` (`src/capopm/experiments/runner.py:374-412`); calibration post-processing at `runner.py:263-287`; tests computed afterward.  
  4) **a1_info_efficiency_curves.py**: Scenario built with Stage1+Stage2 enabled default config (`a1_info_efficiency_curves.py:98-193`), parameters overridden per sweep (`a1_info_efficiency_curves.py:62-76`). Metrics are produced inside `run_experiment` (`runner.py:183-258`) where per-run Brier = `brier(p_true, p_hat)` for each model. CAPOPM p_hat uses Stage1+Stage2 pipeline (`runner.py:118-129`), while raw_parimutuel uses pool path (`runner.py:131-134`), no Stage1/Stage2.  
  5) **Upstream**: capopm_pipeline (hybrid prior + Stage1/Stage2 corrections) (`posterior.py:28-135`), trade simulation `simulate_market` (`market_simulator.py:73-148`), counts `counts_from_trade_tape` (`likelihood.py:33-43`).  
- **Paper alignment:** Proposition 6 in CAPOPM.pdf (posterior predictive distribution of Bernoulli payoff, p. text index ~86779) asserts π_pred = α_post/(α_post+β_post); it does **not** claim CAPOPM Brier ≤ raw parimutuel. The audit criterion over-claims relative to the cited proposition.  
- **Finite-sample testability:** Not justified under current design: only n_runs=4 with highly variable p_true (see summary per-run). Proposition 6 is pointwise identity, not a dominance claim; with finite noisy runs, Brier ordering is uncontrolled. Claim is therefore not testable as written; failure is not theoretically meaningful against the paper.  
- **Judgement calls & edge cases:** Asymmetric comparison (Stage1+Stage2 vs raw parimutuel baseline with no corrections); calibration `NOT_INTERPRETABLE` (n_samples=4, degenerate bins) so Brier may be unstable; coverage flags indicate off-nominal intervals; threshold equals raw_parimutuel brier (0.0666) so comparator and threshold coincide—any noise can flip sign. No CI used; per-run p_true includes extreme value (~0.92) yielding large Brier variance.  
- **Fix proposal (documentation only):** Align criterion to a paper-backed property (e.g., evaluate whether CAPOPM price equals posterior mean per Proposition 6, or use effect-size/CI dominance with adequate runs). Require paper-ready n_runs and a grid over signal_quality per paper_config before asserting dominance; otherwise mark criterion as not implied by Proposition 6. Post-fix audit should either drop the dominance claim or treat it as exploratory with uncertainty bounds.

### grid_requirement — Scenario a1_info_eff_q65_inf30_adv10_seed99101 (INDETERMINATE)
- **Audit evidence:** metric_path `null`; comparator_path `null`; direction/threshold `null`; metric_value `null`; evaluated `false`; pass `null`; reason `grid_missing_for_claim`. Flags: `low_n_runs=true`; `criteria_semantics_mismatch=false`; coverage/calibration flags as above (degenerate calibration models; coverage off-nominal).  
- **Backward trace:**  
  1) **audit_contracts.py**: `_grid_requirement` used for A1 at `src/capopm/experiments/audit_contracts.py:43-46`, requires grid for monotonic info-efficiency curve (Prop 6).  
  2) **audit.py**: `_evaluate_criteria` sets `grid_points_observed=1` and if `requires_grid` and `grid_points_observed < paper_ready_min_grid(=2)` then evaluated=False, reason `grid_missing_for_claim` (`audit.py:342-359`; thresholds from `paper_config.py:23-24`).  
  3) **runner.py**: Seed/grid coverage classification sets grid_axes from `sweep_params` and grid_points_observed=1 (`audit.py:582-604`), not aggregating across scenarios.  
  4) **a1_info_efficiency_curves.py**: Sweep supports multiple signal qualities/informed/adversarial shares (`a1_info_efficiency_curves.py:31-87`), but the smoke run only instantiates one grid point. No metric is computed for monotonicity.  
  5) **Upstream**: Not reached; metric short-circuited.  
- **Paper alignment:** Proposition 6 describes posterior predictive mean, not monotonic info-efficiency curves across signal quality; the audit’s grid requirement references monotonicity that is not explicit in the proposition. Even if monotonicity were intended, a single scenario cannot evaluate it; per-scenario gating makes the claim untestable.  
- **Finite-sample testability:** No; with only one grid point and n_runs=4, monotonic trends over signal quality cannot be assessed. This is an audit design limitation (per-scenario grid counting) and a data limitation (single grid point).  
- **Judgement calls & edge cases:** Per-scenario grid isolation guarantees indeterminate result; low n_runs; calibration/coverage uninterpretable; no CI or sweep-aware metric; paper reference does not encode monotonicity.  
- **Fix proposal (documentation only):** Aggregate grid completeness across scenarios and evaluate a monotonicity metric only when ≥2 signal_quality points exist, using paper-ready run counts. Alternatively, map this criterion to a paper-backed claim or mark it as not implied by Proposition 6. Post-fix audit should not short-circuit when the configured sweep grid is present, and should compute an appropriate cross-grid statistic.

---

AUDIT SUMMARY
- Files inspected: results/paper_artifacts/claim_table.md; results/paper_artifacts/borderline_atlas.md; results/a1_info_eff_q65_inf30_adv10_seed99101/audit.json; results/a1_info_eff_q65_inf30_adv10_seed99101/summary.json; src/capopm/experiments/audit_contracts.py; src/capopm/experiments/audit.py; src/capopm/experiments/runner.py; src/capopm/experiments/a1_info_efficiency_curves.py; src/capopm/likelihood.py; src/capopm/posterior.py; src/capopm/market_simulator.py; src/capopm/corrections/stage1_behavioral.py; src/capopm/corrections/stage2_structural.py; smoke_test_a1_info_efficiency.py; docs/CAPOPM_paper.pdf.
- Smoke tests audited: smoke_test_a1_info_efficiency.py (schema/determinism; no dominance/monotonicity check).
- Criteria traced: `capopm_dominates_raw_parimutuel_brier` (fail), `grid_requirement` (indeterminate) for scenario a1_info_eff_q65_inf30_adv10_seed99101.
- Finite-sample untestable claims: Brier dominance framed as Proposition 6 (paper states posterior mean, not dominance); monotonic info-efficiency grid requirement with only one grid point and per-scenario grid counting.
- Paper–code–audit contradictions: Proposition 6 does not assert CAPOPM Brier ≤ raw parimutuel; grid requirement invokes monotonicity not present in Proposition 6; audit thresholds applied despite n_runs=4.
- Unresolved ambiguities: No explicit theorem for monotonic info-efficiency curves in CAPOPM.pdf; unclear intended cross-grid metric; asymmetry between Stage1+Stage2 capopm and uncorrected/raw baselines.
- What I need next: Guidance on paper-backed criteria for A1 (dominance vs posterior-mean identity; proper cross-grid metric) before any Stage B planning; confirmation whether to treat monotonicity as out-of-scope given Proposition 6’s text. 
