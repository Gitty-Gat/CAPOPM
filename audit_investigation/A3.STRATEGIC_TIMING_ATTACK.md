# A3.STRATEGIC_TIMING_ATTACK (Stage A — Investigative Audit)

## Topology & Execution Model
- **Experiment definition & scenarios:** Declared in `src/capopm/experiments/a3_strategic_timing_attack.py:48-99` with `EXPERIMENT_ID="A3.STRATEGIC_TIMING_ATTACK"`. Scenarios sweep `attack_strength`, `attack_window`, and `adversarial_size_scale`, seeding as `base_seed + idx` and naming via `scenario_id` (`a3_strategic_timing_attack.py:721-726`). Paper grid defaults live in `paper_config.PAPER_GRIDS["A3"]` (attack_strength_grid=[0.0,1.0], window_grid=[20], scale_grid=[1,3] at `src/capopm/experiments/paper_config.py:44-48`). Run configs default to `n_runs=80` but the audited artifacts use `n_runs=5` (from `summary.json` for each scenario).
- **Configuration truth:** Base settings (trader mix, market, priors, Stage1/Stage2) are hard-coded in `build_base_config` (`a3_strategic_timing_attack.py:729-823`). Scenario-specific sweep params are embedded in `config["sweep_params"]` and persisted via `write_scenario_outputs` (`runner.py:479-789`, config_snapshot written alongside summary). Audit thresholds (min grid=2, min runs=30) come from `paper_config.py:10-25`.
- **Execution flow:** `run_a3_scenario` (`a3_strategic_timing_attack.py:100-312`) draws p_true (`draw_p_true`, `runner.py:332-349`), simulates trades (`simulate_market`, `market_simulator.py:73-148`), applies timing attack (`apply_strategic_timing_attack`, `a3_strategic_timing_attack.py:315-368`, rebuilds tape at `371-427`), computes counts (`counts_from_trade_tape`, `likelihood.py:33-43`), then runs capopm_pipeline with Stage1+Stage2 enabled for `capopm` and Stage1-only/none for baselines (`a3_strategic_timing_attack.py:440-559`; Stage1 weights in `corrections/stage1_behavioral.py:62-115`; Stage2 mixture offsets in `corrections/stage2_structural.py:144-199`). Per-run metrics include `log_score` and `regret_log` vs uncorrected baseline (`a3_strategic_timing_attack.py:175-247`). Aggregation by mean is in `aggregate_metrics` (`runner.py:374-412`). Calibration ECE with fallback runs per model (`a3_strategic_timing_attack.py:251-274`, allow_fallback=True). Summary and CSVs are written, then audited via `run_audit_for_results` (`audit.py:57-148` invoking `_evaluate_criteria` at `audit.py:332-418`).
- **Smoke tests touching this pipeline:** `smoke_test_a3_strategic_timing_attack.py:1-123` calls `run_a3_strategic_timing_attack` on a reduced grid (attack_strength=[0,1], window=[0.2], scale=[1,3], n_runs=5), validates schema for `summary.json`, `metrics_aggregated.csv`, `tests.csv`, reliability tables, asserts finite regime metrics, checks at least one improvement in MAE when attack_strength=1, and enforces determinism across reruns. It does not verify log-regret sign or Proposition 9 grid semantics.

## Smoke Tests (interpretation)
- `smoke_test_a3_strategic_timing_attack.py:29-108` validates artifacts and basic determinism, and requires at least one MAE improvement when attack_strength=1. This assumption (MAE improvement under full attack) is **implicit** rather than explicitly tied to CAPOPM.pdf; Theorem 12 focuses on preserving no-arbitrage properties under kernel regularization, not MAE dominance. The test does not probe log-score regret or grid behavior, so it does not guard the audited claims.

## Criterion Analyses

### Criterion: regret_log_non_negative — Scenario A3_strategic_timing__attack0__seed0__window20__scale1 (FAIL)
- **Claim header:** Experiment `A3.STRATEGIC_TIMING_ATTACK`; scenario `A3_strategic_timing__attack0__seed0__window20__scale1`; theorem `Theorem 12`; criterion `regret_log_non_negative`.
- **Audit evidence:** From `results/A3_strategic_timing__attack0__seed0__window20__scale1/audit.json`: metric_path `["aggregated_metrics","capopm","regret_log"]`; direction `>=`; threshold `0.0`; metric_value `-0.01737691629202185`; evaluated `true`; pass `false`; reason `null`. Grid criterion separate; overall_pass `false`; calibration ECE `NOT_INTERPRETABLE` (n_samples=5, n_unique_predictions=5, n_nonempty_bins=5 for capopm/uncorrected).
- **Trace:** 
  1) Contract defines log-regret non-negativity at `src/capopm/experiments/audit_contracts.py:70-84`.
  2) Audit resolves metric via `_evaluate_criteria` and `_resolve_metric` (`audit.py:332-418`), comparing aggregated capopm regret_log to threshold 0.
  3) Aggregation uses mean over per-run metrics in `aggregate_metrics` (`runner.py:374-412`).
  4) Per-run log_score and regret_log are set in `run_a3_scenario` (`a3_strategic_timing_attack.py:175-247`), with regret_log = log_score(model) − log_score(uncorrected) (`a3_strategic_timing_attack.py:242-246`).
  5) Model outputs for capopm include Stage1+Stage2 mixture (`a3_strategic_timing_attack.py:440-469`, Stage1 weights `stage1_behavioral.py:62-115`, Stage2 mixture `stage2_structural.py:144-199`). Baseline `uncorrected` uses no Stage1/Stage2 (`a3_strategic_timing_attack.py:486-494`). Upstream data derive from trade simulation (`market_simulator.py:73-148`) and counts (`likelihood.py:33-43`).
- **Paper alignment:** Theorem 12 (“Kernel Regularization and No–Arbitrage Preservation”) in CAPOPM.pdf asserts kernel-based corrections preserve no-arbitrage; it does not state that log-score under attacks strictly improves relative to an uncorrected baseline in finite samples. Using mean log-regret over 5 runs is therefore an **unjustified proxy** for Theorem 12.
- **Finite-sample testability:** Current design (n_runs=5, attack_strength=0) yields high variance; with no attack, Theorem 12’s mitigation premise is absent, so the criterion is effectively untestable for this scenario. Failing value (-0.017) is within plausible sampling noise and not a paper-backed finite-sample guarantee.
- **Judgement calls & edge cases:** Asymmetric comparison (Stage1+Stage2 vs no-stage baseline); calibration flagged as NOT_INTERPRETABLE; allow_fallback=True may change binning; attack_strength=0 means grid axis not exercised; regret is averaged without CI.
- **Fix proposal (do not implement):** Recast the criterion to a paper-backed property (e.g., no-arbitrage preservation or attack-induced improvement when attack_strength>0) and evaluate with uncertainty-aware statistics at paper-ready run counts; otherwise mark this scenario as out-of-scope for Theorem 12. Expected post-fix audit: this scenario would be excluded or marked not applicable, removing the false fail.

### Criterion: regret_log_non_negative — Scenario A3_strategic_timing__attack0__seed1__window20__scale3 (FAIL)
- **Claim header:** Experiment `A3.STRATEGIC_TIMING_ATTACK`; scenario `A3_strategic_timing__attack0__seed1__window20__scale3`; theorem `Theorem 12`; criterion `regret_log_non_negative`.
- **Audit evidence:** `metric_value -0.026166778976910908`; metric_path `["aggregated_metrics","capopm","regret_log"]`; direction `>=`; threshold `0.0`; evaluated `true`; pass `false`; reason `null`. Calibration `NOT_INTERPRETABLE` (n_samples=5); overall_pass `false`.
- **Trace:** Same contract and audit resolution as above (`audit_contracts.py:70-84`; `audit.py:332-418`). Aggregated metrics from mean of per-run regrets (`runner.py:374-412`), per-run computation in `a3_strategic_timing_attack.py:175-247` with regret subtraction at `242-246`. Model outputs use Stage1+Stage2 for capopm vs no-stage baseline (`a3_strategic_timing_attack.py:440-494`), upstream trade simulation and counts (`market_simulator.py:73-148`; `likelihood.py:33-43`).
- **Paper alignment:** Same misalignment: Theorem 12 does not guarantee positive log-regret, especially when attack_strength=0. The metric is an **unjustified surrogate** for the theorem’s no-arbitrage focus.
- **Finite-sample testability:** n_runs=5 with attack_strength=0, so the hypothesized manipulation is absent; test is not meaningful for the theorem. Negative regret likely reflects sampling noise; finite-sample validation of Theorem 12 in this regime is not feasible.
- **Judgement calls & edge cases:** Asymmetric stage corrections vs baseline; allow_fallback calibration; small sample; adversarial size scale affects trade sizes even though attack_strength=0 (adversarial volume still scaled), potentially biasing regret without representing timing manipulation.
- **Fix proposal (do not implement):** Restrict the criterion to attack_strength>0 and use paper-ready run counts with CI; or remap to a theorem-backed metric (e.g., regime mixture entropy behavior). Post-fix audit should mark this scenario as not evaluated or use a tolerant statistical test rather than a hard ≥0 gate.

### Criterion: grid_requirement — Scenario A3_strategic_timing__attack0__seed0__window20__scale1 (INDETERMINATE)
- **Claim header:** Experiment `A3.STRATEGIC_TIMING_ATTACK`; scenario `A3_strategic_timing__attack0__seed0__window20__scale1`; theorem `Proposition 9`; criterion `grid_requirement`.
- **Audit evidence:** metric_path `null`; comparator `null`; direction `null`; threshold `null`; metric_value `null`; evaluated `false`; pass `null`; reason `grid_missing_for_claim`. Grid flags arise even though sweep_params include attack_strength/window/scale. Calibration `NOT_INTERPRETABLE`; low_n_runs implied (n_runs=5).
- **Trace:** Contract marks `requires_grid=True` (`audit_contracts.py:84-87`). `_evaluate_criteria` sets `grid_points_observed=1` and requires at least `paper_ready_min_grid=2` (`audit.py:342-359`, threshold from `paper_config.py:23-24`), so criterion short-circuits before metric resolution. Scenario written via `write_scenario_outputs` (`runner.py:479-789`) with sweep_params recorded; audit ignores other scenarios.
- **Paper alignment:** Proposition 9 (mixture posterior properties) is inherently about multiple regimes/conditions; requiring a grid is reasonable, but per-scenario evaluation that never counts other grid points makes the claim untestable despite multiple scenarios existing.
- **Finite-sample testability:** Not testable per scenario because audit enforces multi-point grid but only counts the current point. Design limitation in audit aggregation, not paper or simulator.
- **Judgement calls & edge cases:** Single-scenario grid counting; small n_runs; mixture regime weights may be undefined if Stage2 disabled (not here) but unused due to short-circuit.
- **Fix proposal (do not implement):** Aggregate grid coverage across scenarios before applying the gate (using manifest or registry snapshot) and require completion of the configured grid; then evaluate any cross-grid metric implied by Proposition 9. Post-fix audit should mark this criterion evaluated once grid completeness is confirmed.

### Criterion: grid_requirement — Scenario A3_strategic_timing__attack0__seed1__window20__scale3 (INDETERMINATE)
- **Audit evidence & trace:** Same fields and short-circuit reason as above (evaluated `false`, reason `grid_missing_for_claim`; contract at `audit_contracts.py:84-87`; gating in `audit.py:342-359`).
- **Paper alignment & testability:** Same: per-scenario audit cannot satisfy Proposition 9 grid requirement; untestable until grid aggregation is considered.
- **Judgement calls & fix proposal:** Same as prior grid case—requires cross-scenario aggregation and paper-backed grid completeness check before evaluation.

### Criterion: grid_requirement — Scenario A3_strategic_timing__attack100__seed2__window20__scale1 (INDETERMINATE)
- **Audit evidence:** evaluated `false`; reason `grid_missing_for_claim`; otherwise identical null metric/comparator fields. Calibration `NOT_INTERPRETABLE`; n_runs=5.
- **Trace:** Contract and audit gating identical (`audit_contracts.py:84-87`; `audit.py:342-359`).
- **Paper alignment & testability:** Proposition 9 still requires cross-grid behavior; per-scenario audit blocks evaluation even though multiple attack strengths are present across scenarios. Untestable due to audit design.
- **Judgement calls & fix proposal:** Same need for cross-scenario grid aggregation; once available, this high-attack scenario would be critical for evaluating timing mitigation.

### Criterion: grid_requirement — Scenario A3_strategic_timing__attack100__seed3__window20__scale3 (INDETERMINATE)
- **Audit evidence:** evaluated `false`; reason `grid_missing_for_claim`; null metric/comparator. Calibration `NOT_INTERPRETABLE`; n_runs=5.
- **Trace:** Same as other grid cases (`audit_contracts.py:84-87`; `audit.py:342-359`).
- **Paper alignment & testability:** Proposition 9 grid claim remains untestable per-scenario; requires cross-scenario aggregation.
- **Judgement calls & fix proposal:** Same as above—aggregate grid completeness before gating, then evaluate a Proposition 9-aligned cross-grid metric.

---

AUDIT SUMMARY
- Files inspected: results/paper_artifacts/claim_table.md; results/paper_artifacts/borderline_atlas.md; results/A3_strategic_timing__*/audit.json and summary.json; src/capopm/experiments/audit_contracts.py; src/capopm/experiments/audit.py; src/capopm/experiments/runner.py; src/capopm/experiments/a3_strategic_timing_attack.py; src/capopm/likelihood.py; src/capopm/market_simulator.py; src/capopm/posterior.py; src/capopm/corrections/stage1_behavioral.py; src/capopm/corrections/stage2_structural.py; smoke_test_a3_strategic_timing_attack.py; docs/CAPOPM_paper.pdf.
- Smoke tests audited: `smoke_test_a3_strategic_timing_attack.py` (artifact/schema/determinism checks; implicit MAE improvement under attack_strength=1).
- Criteria traced: `regret_log_non_negative` (attack0 seed0 scale1; attack0 seed1 scale3); `grid_requirement` for all four A3 scenarios.
- Key judgement calls: Theorem 12 does not imply positive log-regret vs uncorrected; per-scenario grid counting forces indeterminate outcomes despite multi-scenario sweep; asymmetric Stage1+Stage2 vs uncorrected baseline; calibration/coverage not interpretable at n_runs=5.
- Finite-sample untestable claims: Theorem 12 log-regret dominance in no-attack scenarios (attack_strength=0); Proposition 9 grid requirement per scenario (audit design blocks evaluation even with multiple scenarios).
- Paper–code–audit contradictions: Audit asserts log-regret non-negativity as Theorem 12, but CAPOPM.pdf Theorem 12 concerns no-arbitrage kernel regularization; grid requirement flags missing grid although scenarios span multiple attack strengths in the run set.
- What I need next: Guidance on remapping Theorem 12 criterion to a paper-backed property and on redesigning grid aggregation for Proposition 9 before proceeding to Stage B planning (no code changes made). 
