# B5.ARBITRAGE_PROJECTION_IMPACT (Stage A — Investigative Audit)

## Topology & No-Arbitrage Projection Intent — B5.ARBITRAGE_PROJECTION_IMPACT
- **Arbitrage definition (operational):** Arbitrage/coherence violation is any probability vector off the simplex (negative, >1, or sum ≠ 1). Detection uses `detect_violation` (`src/capopm/experiments/projection_utils.py:22-39`) on the two-outcome vector `[p_yes, p_no]` generated from the unprojected CAPOPM posterior (`capopm_pipeline` without Stage1/Stage2 corrections in this experiment).
- **Projection implementation:** Projection onto the simplex via either KL normalization or Euclidean simplex projection (`projection_utils.project_probs`, `src/capopm/experiments/projection_utils.py:6-21`; Euclidean method `_euclidean_simplex_projection` at `projection_utils.py:48-78`). Distances computed as L1/L2/KL in `projection_distance` (`projection_utils.py:24-46`). Before-vector violation injected by `inject_violation` (in `b5_arbitrage_projection_impact.py:110-118`), adding `VIOLATION_PUSH` scaled by `violation_strength` to skew probabilities.
- **Projection application point:** In `run_b5_scenario`, after computing raw probabilities `p_raw_yes/p_raw_no` from `capopm_pipeline` (Stage1/Stage2 disabled), a violation is injected and then projected (`b5_arbitrage_projection_impact.py:98-160`). Per-run metrics for `before_projection` and `after_projection` are computed with the projected/unprojected probabilities; `after_projection` includes projection distances and score deltas (`b5_arbitrage_projection_impact.py:158-180`).
- **Baselines compared:** Models: `capopm` (raw posterior without projection), `before_projection` (violated probabilities), `after_projection` (projected probabilities). Impact metrics are deltas (`delta_brier`, `delta_log_score`, `delta_abs_error_outcome`) defined as after - before (`b5_arbitrage_projection_impact.py:158-160`); distance metrics `proj_l1/l2/kl` stored on after_projection.
- **Impact metric:** Audit criteria use `proj_l1` (projection distance) and score deltas (`delta_brier`, `delta_log_score`) from `summary.status.metrics` (`b5_arbitrage_projection_impact.py:221-234` builds summary including these deltas in status). Audit paths reference these summary fields.
- **Smoke tests touching projection:** `smoke_test_b5_arbitrage_projection_impact.py:1-106` runs violation_strength_grid=[0.0,0.5,1.0], validates schemas and reliability, asserts near-zero projection distance and score deltas in coherent regime, and improvement (delta_brier<0, delta_log_score>0) plus increased projection distance in violated regime (lines 55-85). Enforces determinism across reruns (lines 87-103). This assumes dominance in violated regimes and neutrality when coherent; not directly theorem-backed for finite runs.

## Smoke Test Audit
- **smoke_test_b5_arbitrage_projection_impact.py**  
  - Assumes: (a) coherent case has ~0 projection distance and score deltas; (b) increasing violation increases projection distance and improves Brier/log-score after projection.  
  - Pipeline legitimized: projection detection, simplex projection (Euclidean), impact metrics (`delta_brier`, `delta_log_score`) from `after_projection`.  
  - Alignment: Theorem 13 covers no-arbitrage preservation under projection but does not guarantee finite-sample score improvement; the test encodes dominance assumptions heuristically.

## Criterion-Level Investigation
All criteria in audit.json for B5 scenarios are passing; no failed or indeterminate criteria to trace. (Scenarios: `B5_arbitrage_proj__methodeuclidean__viol0__seed0`, `viol50__seed1`, `viol100__seed2`; all `overall_pass=True`.)

---

AUDIT SUMMARY
- Files inspected: results/paper_artifacts/claim_table.md; results/paper_artifacts/borderline_atlas.md; results/B5_arbitrage_proj__methodeuclidean__viol*_*/audit.json and summary.json; src/capopm/experiments/audit_contracts.py; src/capopm/experiments/audit.py; src/capopm/experiments/runner.py; src/capopm/experiments/b5_arbitrage_projection_impact.py; src/capopm/experiments/projection_utils.py; src/capopm/posterior.py; src/capopm/likelihood.py; src/capopm/market_simulator.py; src/capopm/corrections/stage1_behavioral.py; src/capopm/corrections/stage2_structural.py; src/capopm/metrics/scoring.py; smoke_test_b5_arbitrage_projection_impact.py; docs/CAPOPM_paper.pdf.
- Smoke tests audited: smoke_test_b5_arbitrage_projection_impact.py (assumes projection distance grows and scores improve under violation; neutrality when coherent).
- Criteria traced: None failed/indeterminate; all B5 criteria passed (projection distance non-negative; score deltas conditioned on violation).
- Paper–audit mismatches: Theorem 13 does not explicitly guarantee finite-sample score improvement after projection; smoke test asserts dominance; audit conditions permit pass by “no_violation_triggered” in coherent case.
- Unresolved ambiguities: Extent to which projection is justified as likelihood-preserving vs heuristic; sensitivity to `VIOLATION_PUSH` scaling; impact of calibration fallback (allow_fallback=True) on after/before metrics not audited.
- What I need next: If deeper verification is required, inspect finite-sample robustness of score improvements and projection method choices versus Theorem 13 assumptions; no Stage B action taken. 
