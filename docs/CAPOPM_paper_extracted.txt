CAPOPM: A Bayesian Hybrid Framework for
Derivative Pricing in Behavioral Parimutuel
Markets
Sean Slattery
November 2025
Abstract
This paper develops CAPOPM, a crowd-adjusted parimutuel option
pricing mechanism that integrates structural financial models, machine
learning forecasts, and trader behavior forecasting, and trader behavior
within a unified Bayesian framework. Rather than treating derivative
prices as primitives or relying solely on structural models, CAPOPM views
trader actions in a parimutuel setting as informational signals that update
a hybrid prior. The prior combines a tempered fractional Heston model,
which captures long-memory volatility and is compatible with risk-neutral
pricing, with neural-network forecasts trained on historical market data.
These two components are fused through a pseudo-sample interpretation,
producing a prior distribution that balances theoretical features of struc-
tural modeling with data-driven predictive content.
Trader actions are incorporated through a two-stage adjustment pro-
cess. The first stage accounts for behavioral patterns such as long-shot
bias, herding, and asymmetries in participation. The second stage intro-
duces structural corrections for liquidity imbalances or whale dominance.
Both adjustments are designed to preserve Beta–Binomial conjugacy, en-
abling closed-form posteriors while allowing the model to represent several
sources of distortion present in real markets.
The resulting posterior distribution over event probabilities can be
viewed as an interpretable belief-extraction mechanism. Phase 7 examines
the empirical properties of this mechanism through simulation, including
stress tests for herding and correlated behavior. Phase 8 provides theo-
retical analysis, showing that under reasonable assumptions and bounded
distortions, the posterior is robust, consistent, and asymptotically normal
as the effective sample size grows.
The goal of this work is not to claim empirical dominance or to present
a fully calibrated pricing engine, but to establish a structured approach
for combining structural finance, machine learning, and crowd behavior
into a coherent Bayesian updating procedure. CAPOPM is proposed as
a flexible foundation that can be extended, calibrated, and empirically
validated in a variety of market environments.
1

1 Introduction
1.1 Background and Motivation
Modern derivative markets bring together structural financial models, algorith-
mic trading systems, and heterogeneous crowds of participants, each possessing
partial or noisy information. As quantitative methods have advanced, two broad
approaches to pricing derivatives have emerged. The first centers on struc-
tural asset dynamics, using stochastic volatility models and their extensions to
obtain risk-neutral prices. The second leverages machine learning algorithms,
which are trained directly on historical data to approximate option prices or
event probabilities. Both approaches are powerful, but each is limited in isola-
tion: structural models may be rigid or miscalibrated in volatile markets, while
machine learning methods lack interpretability and can behave unpredictably
outside their training regimes.
In parallel to these developments, parimutuel prediction markets and crowd-
based financial platforms have shown that aggregate order flow can encode
meaningful information about uncertain outcomes. When participants act on
private signals or interpretations of public news, their trades generate a rich
stream of behavioral and informational content. Yet this information is rarely
integrated into option pricing models in a principled way. The goal of this paper
is to explore a mechanism through which such crowd signals, structural models,
and machine learning outputs can be combined through a Bayesian framework.
1.2 Challenges in Existing Approaches
Approaches to belief aggregation and pricing often assume clean separation be-
tween trader behavior, structural assumptions, and statistical estimation. Struc-
tural models depend heavily on parameter calibration and may struggle under
regime changes. Machine learning methods can fit historical data well but typ-
ically provide point estimates without uncertainty quantification. Prediction
markets aggregate information but do not easily interface with risk-neutral pric-
ing or structural modeling.
Behavioral biases further complicate the picture. Long-shot bias, correlated
trading, herding cascades, and liquidity asymmetry can distort raw order flow.
Without a systematic way to adjust for these effects, trader actions cannot be
directly interpreted as signals about future prices or event probabilities. Ad-
ditionally, existing Bayesian updating frameworks rely on conditional indepen-
dence, an assumption that breaks down in settings where trader behavior is
reactive or networked.
These challenges motivate a framework that (i) integrates structural and
machine learning priors, (ii) preserves interpretability, (iii) incorporates behav-
ioral and structural corrections, and (iv) provides theoretical guarantees under
realistic assumptions.
2

1.3 Literature Review
A number of strands of research motivate the CAPOPM framework. First,
the tempered fractional Heston model of Shi [21] introduces a volatility process
with fractional memory through a Riccati–Volterra system, offering a generaliza-
tion of classical stochastic volatility while retaining exponential-affine structure
useful for option pricing. This model forms the structural component of the
CAPOPM prior.
Second, work by Koessler, Noussair, and Ziegelmeyer [17] examines informa-
tion aggregation in parimutuel betting markets under asymmetric information.
Their analysis demonstrates that trader participation and signal structure influ-
ence market-clearing probabilities in systematic ways. This literature informs
the representation of trader actions as information signals within CAPOPM.
Third, Axelrod, Kulick, Plott, and Roust [1] develop mechanisms to improve
parimutuel-type aggregation, addressing inefficiencies such as long-shot bias and
disequilibrium phenomena. Their results motivate the need for behavioral ad-
justments before interpreting order flow as informational.
A separate line of work, represented by D’Uggento, Biancardi, and Ciriello [7],
investigates machine learning approaches for pricing derivatives and predicting
option prices.
Their findings show that neural network models can capture nonlinearities
that structural models miss, though such models require careful interpretation
due to uncertainty and generalization limits.
Beyond these core sources, the broader literature on prediction markets, be-
havioral biases, and market microstructure provides context. Prediction mar-
kets demonstrate that crowds can aggregate dispersed private signals, especially
when incentives are aligned. Behavioral finance research documents systematic
distortions in participant behavior, including overreaction, herding, and asym-
metry in risk perception. Finally, microstructure models highlight how liquidity
imbalances, strategic trading, and asymmetric information influence observable
order flow.
CAPOPM draws from each of these areas, but its primary novelty lies in
integrating them into a single Bayesian mechanism for belief extraction.
1.4 Overview of the CAPOPM Framework
CAPOPM is organized into an eight-phase structure. The framework begins
with a structural prior based on a tempered fractional Heston model and an
empirical prior derived from neural network forecasts. These priors are fused
using a pseudo-sample interpretation to form a hybrid prior distribution for the
event probabilityp. Trader actions are processed through a two-stage adjust-
ment layer that accounts for behavioral effects (e.g., herding, long-shot bias)
and structural distortions (e.g., liquidity imbalances), while preserving Beta–
Binomial conjugacy.
The adjusted likelihood is combined with the hybrid prior to produce a
posterior distribution. Simulation regimes are used to study how the posterior
3

behaves under dependence, correlated trading, and other deviations from model
assumptions. Phase 8 provides theoretical results showing that the posterior is
robust to bounded distortions and concentrates near the true probability under
fairly mild conditions.
1.5 Contributions and Novelty
While CAPOPM draws on ideas from structural option pricing, machine learn-
ing forecasting, and parimutuel information aggregation, the contribution of this
work is the way these elements are organized into a coherent belief-updating
framework. The approach developed here differs from existing models in three
main respects. First, structural information from a tempered fractional Heston
model and predictive signals from neural-network methods are combined within
a unified Bayesian prior using a pseudo-sample interpretation; this provides a
transparent way to balance theoretical modeling with data-driven estimates.
Second, parimutuel order flow is incorporated through a two-stage adjustment
that accounts for behavioral patterns (such as long-shot bias and herding) and
structural distortions (such as imbalanced liquidity), while preserving conjugacy
and permitting closed-form posteriors. Third, the model is developed as an
eight-phase architecture that separates structural, behavioral, inferential, and
theoretical components, allowing each layer to be examined and stress-tested
independently.
The goal of this paper is not to claim empirical superiority but to establish a
foundation for a mathematically interpretable belief-extraction mechanism that
could be applied to derivative markets. By treating trader actions as informa-
tional signals updating a hybrid prior, CAPOPM offers a way to study how
structural models, machine learning outputs, and crowd behavior interact in a
setting where probabilities, not prices, are the central object, with prices recov-
ered subsequently via the structural prior and pricing kernel. The framework is
intended as a starting point for further empirical and theoretical development
rather than a finalized model.
1.6 Limitations and Scope
While CAPOPM combines several modeling elements into a unified framework,
it is not intended as a fully calibrated pricing engine. The structural prior inher-
its sensitivity to the parameterization of the fractional Heston model. Machine
learning predictions require careful tuning and may behave unpredictably out-
side their training domain. Behavioral corrections mitigate but do not eliminate
distortions from extreme herding or low-liquidity conditions. Finally, empirical
validation is left for future work.
1.7 Roadmap
The remainder of the paper is organized as follows. Phase 1 develops the struc-
tural prior using the tempered fractional Heston model. Phase 2 constructs the
4

hybrid prior combining structural and machine learning components. Phase 3
sets out the trader information structure and event definition. Phase 4 presents
the Bayesian updating mechanism. Phase 5 introduces the parimutuel likeli-
hood. Phase 6 develops behavioral and structural correction layers. Phase 7
conducts simulation analysis under a variety of trader behaviors. Phase 8 pro-
vides theoretical guarantees on robustness, consistency, and asymptotic normal-
ity. The paper concludes with discussion and avenues for further research.
Phase 1 establishes the structural prior for the CAPOPM framework, ground-
ing the binary event probability in a well–posed stochastic volatility model.
This prior provides the Bayesian anchor for the later fusion of machine learning
predictions and behavioral distortions.
Phase 1. Structural Prior and Event Foundations
0. Notation, Probability Space, and Market Environment
We work on a filtered probability space
(Ω,F,(F t)t≥0, Q),
whereQis the risk–neutral measure. LetS t be the underlying asset price,V t
the variance process, and (W t, Bt) a pair of Brownian motions with correlation
ρ∈[−1,1]. All expectationsE[·] are taken underQ.
Tempered fractional kernel.Forα∈(1/2,1) and tempering parameter
λ≥0, define
Kα,λ(t−s) :=e −λ(t−s)(t−s) α−1,
as introduced in the tempered–fractional Heston framework of Shi [21].
The restrictionα >1/2 guarantees square–integrability of the kernel, ensuring
that the stochastic convolution in (2) is well defined [16, 9].
Well–posedness references.The tempered fractional kernelK α,λ enters the
Volterra SDE forV t. Existence and uniqueness of the associated stochastic inte-
grals follow from the general theory of Volterra–type SDEs in [25, 16], while the
connection to rough–Heston kernels and their regularity properties is detailed
in [9]. Shi’s construction [21] provides the specialized tempered–fractional case
used here.
Event of interest.Following the event structure of Koessler, Noussair, and
Ziegelmeyer [17], define the CAPOPM binary state:
A:={S T > K}, A c :={S T ≤K}.
A parimutuel YES contract pays 1 ifAoccurs and 0 otherwise. Following the
event framework of Koessler, Noussair, and Ziegelmeyer [17], define the binary
CAPOPM terminal event under the risk–neutral measureQinduced by the
structural prior:
5

Interpretation.The binary payoff structure mirrors the experimental parimutuel
design of Koessler, Noussair, and Ziegelmeyer [17], where traders submit YES/NO
orders on terminal events. This creates a direct mapping between the struc-
tural prior probability and the parimutuel prior odds used in later phases of
CAPOPM. This matches the parimutuel aggregation mechanism studied by Ax-
elrod, Plott, and coauthors [1], where prices reflect aggregated beliefs on binary
terminal events.
Trader population and private information.There areN={1, . . . , n}
traders. Traderireceives a private signalξ i ∈ {H, L}with likelihoods
P(ξi =H|A) =p, P(ξ i =H|A c) = 1−p,
withp∈(1/2,1). This symmetric specification matches the binary-signal design
used in experimental parimutuel markets [17].
Traderiselects
si ∈ {YES,NO},
and the full action profile iss= (s 1, . . . , sn).
Remark.The conditional i.i.d. assumption is used only in Phase 1 and is
relaxed in later phases to accommodate dependence (herding), multimodality,
and nonlinear distortions; see [6, 4].
Cross–phase notation.To prepare for later phases, define:
πprior :=Q(S T > K), L(s|A), L(s|A c)
as the parimutuel likelihoods;
πpost :=Q(A|s)
as the CAPOPM posterior; and
πpred
as the posterior–predictive (crowd–adjusted) derivative price.
1. Structural Prior: Shi’s Tempered–Fractional Heston
Model
Having established notation and the CAPOPM event structure, we now specify
the structural prior dynamics for (S t, Vt). The tempered fractional form pre-
serves affine structure while incorporating the empirically observed roughness of
volatility increments [9, 2]. The goal is to provide a stable, well–posed Bayesian
anchor that later phases (mixture, HMM, nonlinear distortion) update rather
than replace.
The structural prior for CAPOPM is the tempered fractional Heston model
introduced by Shi [21], chosen due to its empirical performance in capturing
6

volatility memory, roughness, and asymmetry—properties essential for a re-
alistic Bayesian anchor. This specification captures long-memory behavior in
volatility while preserving affine transform structure, enabling tractable poste-
rior and pricing calculations [9, 21].
1.1 Model specification
Under the risk–neutral measureQ, the asset price satisfies
dSt =S t
p
Vt dWt.(1)
where the drift has been eliminated by a standard Girsanov change of mea-
sure underQ[16, 18].
This form assumes the standard Girsanov drift adjustment underQ[16, 18],
ensuring that the discounted price is aQ–martingale.
The variance process is given by the tempered–fractional Volterra SDE:
Vt =V 0 + γ
Γ(α)
Z t
0
Kα,λ(t−s)(θ−V s)ds+ σ
Γ(α)
Z t
0
Kα,λ(t−s)
p
Vs dBs.(2)
The Volterra SDE (2) follows the framework of fractional and rough volatility
models [9, 15], specialized to Shi’s tempered kernel [21]. Such kernels fall within
the class of completely monotone or weakly singular Volterra kernels analyzed
in [15]. Existence and uniqueness of this Volterra SDE follow from the Lipschitz
and linear-growth bounds satisfied by the square-root diffusion and the weak
singularity ofK α,λ [25, 15].
This model incorporates:
•long–memory (fractional exponentα),
•exponential tempering (parameterλ),
•Heston–style mean reversion (γ, θ),
•stochastic volatility of volatility (σ).
We assumeα >1/2, ensuring thatK α,λ ∈L 2([0, T]) so that the stochastic
convolution is well defined [16].
[15, 9]
Assumption 1 (Admissible parameter set).The parameter vector Θ =
(γ, θ, σ, α, λ, ρ, V0) satisfies:
γ >0, θ >0, σ >0, α∈(1/2,1), λ≥0, V 0 >0.
7

1.2 Well–posedness of the structural prior
Lemma 1(Existence and uniqueness of the variance process).Under Assump-
tion 1, the Volterra SDE(2)admits a unique strong solution with continuous
sample paths.
Proof.Shi [21] verifies the kernel regularity and monotonicity conditions re-
quired for Volterra–Heston equations. General existence and uniqueness follow
from Volterra SDE theory in [25, 16]. Such kernels fall within the class of
completely monotone or weakly singular Volterra kernels analyzed in [15].
Lemma 2(Positivity of variance).The solutionV t of(2)satisfiesV t >0
almost surely for allt≤T.
Proof.Positivity follows from the square–root diffusion structure and the fact
that Volterra kernels preserve positivity in fractional affine systems [9].
Proposition 1(Existence of a continuous density forS T ).The log–return
XT := lnS T has a continuous densityf Θ(·;T), obtained by Fourier inversion of
a well–defined characteristic function.
Proof.Shi [21] proves analyticity of the characteristic function via a fractional
Riccati–Volterra system. Existence and continuity of the density follow from
L´ evy’s inversion theorem [19] and the affine rough volatility framework [9]. Con-
tinuity of the resulting density follows from the regularity of the affine Riccati–
Volterra solution [15].
1.3 Sketch of the Riccati–Volterra Derivation (Following
Shi)
The Riccati–Volterra decomposition parallels the affine transform method for
rough volatility models [9], with Shi’s tempered kernel modifying the memory
structure while preserving the exponential–affine form.
For completeness, we outline the structural steps that lead from the tempered
fractional Heston dynamics to the Riccati–Volterra system used in evaluating
the characteristic function. The purpose of this sketch is not to rederive the
full result of Shi [21], but to show how the fractional kernel structure enters the
characteristic exponent.
Starting from the log-price processX t = logS t and applying Itˆ o’s formula
to the price dynamicsdS t =S t
√Vt dWt, we obtain
dXt =− 1
2 Vt dt+
p
Vt dWt.
8

To compute the characteristic function Φ(u;T) =E

euXT

, we consider exponential–
affine forms of the type
Φ(u;T) = exp
 
uX0 +γθ
Z T
0
h(s)ds+V 0
Z T
0
g(s)ds
!
,
and substitute this form into the Kolmogorov backward equation associated
with the pair (X t, Vt) under the tempered fractional Heston dynamics.
The variance process satisfies
Vt =V 0 + γ
Γ(α)
Z t
0
Kα,λ(t−s)(θ−V s)ds+ σ
Γ(α)
Z t
0
Kα,λ(t−s)
p
Vs dBs,
whereK α,λ(t) =e −λttα−1 is the tempered fractional kernel. Using the Laplace
transform representation of fractional integrals, the expected value of exp(uXT )
can be written in terms of a Volterra convolution involvingK α,λ. This repre-
sentation is standard in fractional calculus and rough volatility analysis [5, 15].
Matching terms of like order inV t yields the system
9

h(t) = 1
Γ(α)
Z t
0
Kα,λ(t−s)g(s)ds,
g(t) = 1
2 (u2 −u) + (uρσ−γ)h(t) + σ2
2 h(t)2.
Here, the convolution (Kα,λ ∗g)(t) denotes
Rt
0 Kα,λ(t−s)g(s)ds.
Numerical approximation of (h, g) uses graded–mesh fractional Adams meth-
ods, whose stability and convergence properties are analyzed in [5] and imple-
mented for fractional Riccati systems in [9].
These equations express the fractional memory of the volatility process
through the convolution withK α,λ, leading directly to the tempered fractional
Riccati–Volterra system of Shi [21]. Numerical solution methods such as graded-
mesh fractional Adams schemes then provide stable and convergent approxima-
tions to (h, g) and therefore to Φ(u;T).
1.4 Effect of the Fractional Parameter on Structural Tails
In the tempered fractional Heston model of Shi, the volatility dynamics are
driven by a Volterra kernel
Kα,λ(t−s) =e −λ(t−s)(t−s) α−1, α∈(1/2,1), λ≥0,
which enters the Riccati–Volterra system for (h, g) and the characteristic func-
tion Φ(u;T) of lnS T . Asαdecreases toward 1/2, the kernel becomes more
singular at the origin and the variance process exhibits “rougher” behavior; as
αincreases toward 1, the kernel approaches a more classical, smoother mean-
reversion structure.
This roughness has consequences for the tails of the risk-neutral distribution
ofS T , and hence for the structural digital event probability
qShi(K, T; Θ, α) =QΘ,α(ST > K) =
Z ∞
K
fΘ,α(s;T)ds,
wheref Θ,α(·;T) denotes the risk-neutral density corresponding to parameters
Θ and fractional indexα.
We now state a qualitative sensitivity result that connects the fractional
parameter to the tail probabilities ofS T .
Theorem 1(Fractional Parameter Sensitivity of Structural Tail Probabilities).
Fix maturityT >0, a parameter vectorΘ, and a strike levelK >0. Assume:
(i) For eachα∈(1/2,1), the risk-neutral densityf Θ,α(s;T)exists, is contin-
uous ins, and the mapα7→f Θ,α(s;T)is continuously differentiable for
eachs >0.
10

(ii) There exists a strike thresholdK tail >0such that for allK≥K tail and
alls≥K,
∂
∂α fΘ,α(s;T)≤0,
i.e. decreasingα(rougher volatility) weakly increases the right tail den-
sity. Assumption (ii) reflects the monotonicity properties proved in rough-
Heston models [9, 2].
(iii) The derivative∂f Θ,α(s;T)/∂αis dominated by an integrable function on
[K,∞), uniformly forαin compact subsets of(1/2,1), so that differen-
tiation under the integral sign is justified. Assumption (iii) follows from
polynomial growth bounds on the Riccati–Volterra solution established in
[9, 15].
Then, for allK≥K tail,
∂
∂α qShi(K, T; Θ, α) =∂
∂α
Z ∞
K
fΘ,α(s;T)ds≤0.
In particular, for anyK≥K tail andα 1 < α2,
qShi(K, T; Θ, α1)≥q Shi(K, T; Θ, α2),
so that rougher volatility (smallerα) yields larger structural probabilities for far
out-of-the-money events{S T > K}. (iii) holds whenever the fractional Riccati
solution admits polynomial growth bounds, as established in [9, 15].
Proof (Sketch).Under assumptions (i) and (iii), we may differentiate under the
integral sign. Dominated convergence applies by assumption (iii), allowing pas-
sage of the derivative through the integral:
∂
∂α qShi(K, T; Θ, α) =
Z ∞
K
∂
∂α fΘ,α(s;T)ds.
By (ii), the integrand is nonpositive on [K,∞) wheneverK≥K tail, so the
integral is nonpositive. This yields
∂
∂α qShi(K, T; Θ, α)≤0.
Integrating the derivative inαfromα 1 toα 2 withα 1 < α2 gives
qShi(K, T; Θ, α2)−q Shi(K, T; Θ, α1) =
Z α2
α1
∂
∂a qShi(K, T; Θ, a)da≤0,
which impliesq Shi(K, T; Θ, α1)≥q Shi(K, T; Θ, α2) for allK≥K tail.
Remark 1(Connection to Rough Volatility and CAPOPM Prior).Assumption
(ii) encodes, in a simplified form, the empirical and theoretical observation from
rough volatility models that rougher variance dynamics tend to generate heavier
11

implied tails forS T at a fixed horizonT. In the tempered fractional Heston
setting, this is reflected in the dependence of the Riccati–Volterra solution(h, g)
and the characteristic functionΦ(u;T)onα: smallerαincreases the effective
roughness of volatility increments and, under suitable parameter configurations,
leads to fatter risk-neutral tails.
From the CAPOPM perspective, the theorem shows that the structural prior
mean
qShi(K, T; Θ, α) =QΘ,α(ST > K)
for far out-of-the-money events is monotone inαwhenever the assumptions
hold. Decreasingαmoves structural mass toward the long-shot region, increas-
ing the prior probability of rare upside events. This interacts nontrivially with
the machine learning prior: if the ML model is calibrated under a smoother or
effectively different volatility regime, a mismatch inαcan generate systematic
tension between the structural and ML components in the hybrid prior, particu-
larly at extreme strikes. In Phase 2 and Phase 8, this sensitivity can be exploited
in robustness analysis by varyingαand examining how the hybrid prior and re-
sulting posterior react at high and low strikes. Unlike the classical Heston model,
where tails are primarily controlled by the volatility-of-volatility parameter, the
tempered fractional specification amplifies tail sensitivity through the fractional
indexα[2].
1.5 Interpretation of Shi’s Model as a Bayesian Prior
Shi’s model is selected as the prior because:
•it is astructural modelwith proven pricing accuracy across volatility
regimes;
•its tempered fractional kernel captures empirical roughness;
•it integrates naturally with Bayesian updating: the structural density pro-
vides the prior mean for the digital probability;
•it behaves well under scaling and conditioning, supporting the recursive
belief–updating used in CAPOPM.
Thus, the prior digital probability
πprior :=Q Θ(ST > K)
serves as an information–theoretic anchor that traders distort through their
heterogeneous private signals and parimutuel order flow.
Remark.This interpretation aligns with Bayesian predictive stacking [24],
where structural and machine-learning components form a coherent prior en-
semble updated through posterior inference.
12

2. Prior Event Probability and Parimutuel Prior Odds
SinceX T = lnS T has densityf Θ(·;T), define the structural probability of the
CAPOPM event:
πprior =Q Θ(ST > K) =
Z ∞
K
fΘ(s;T)ds.(3)
Lemma 3(Integrability of the structural density).The densityf Θ(s;T)is
integrable and Z
R
fΘ(s;T)ds= 1.
Proof.Follows from existence of the characteristic function and the boundedness
of the fractional Riccati solution (Shi).
Parimutuel interpretation of the prior.Let the ex ante market odds for
YES beO prior
YES . The structural prior implies:
Oprior
YES = 1−π prior
πprior
.
This serves as the “prior odds” that later phases update using
•parimutuel order flow,
•private signals,
•crowd belief extraction.
Proposition 2(Structural prior as parimutuel prior odds).The prior odds
derived from(3)uniquely determine the initial pricing kernel for YES/NO con-
tracts in CAPOPM.
Proof.Since the event is binary and traders are risk–neutral, prior odds are the
likelihood ratio corresponding to the structural prior probability. Uniqueness
follows because the mappingπ7→ 1−π
π is injective on (0,1).
Phase 2. Multi–Tier Structural–ML Hybrid Prior
In Phase 1, we constructed a structural prior for the event
A:={S T > K}
based on Shi’s tempered–fractional Heston model. In this phase, we construct
an extended multi–tier prior by combining:
1. astructural risk–neutral priorfrom the fractional Heston model,
2. amachine–learning predictive priorderived from ANN/RNN models
following D’Uggento et al. [7],
13

into a single coherenthierarchical Bayesian priorfor the unknown prob-
ability
p:=Q(A).
This hybrid prior serves as the foundation for Phase 4 (Binomial parimutuel
likelihood) and Phase 5 (posterior–predictive derivative pricing).
2.1 Bayesian Hierarchical Model Structure
We introduce the following generative hierarchy:
p∼prior from structural model (level 1)
p∼prior from ML model (level 2)
s|p∼Binomial likelihood from parimutuel actions (level 3)
ST |p∼posterior–predictive distribution (level 4).
Remark.This hierarchical structure follows standard Bayesian modeling prac-
tice [13], where each layer contributes pseudo-data to the posterior. The use of
Beta–Binomial conjugacy ensures analytic tractability.
Levels 1 and 2 yield two Beta priors:
p∼Beta(α str, βstr), p∼Beta(α ML, βML),
which are fused into a hybrid prior in Sections 2.5–2.6, following the predictive
stacking framework of [24], which justifies convex fusion of heterogeneous priors.
2.2 Structural Prior from Shi’s Fractional Heston Model
Let
qstr :=Q Θ(ST > K)
be the structural digital probability derived in Phase 1.
We encode this belief as a Beta prior:
p∼Beta(α str, βstr), α str =η str qstr, β str =η str (1−q str),
whereη str >0 represents structural confidence. This encoding interprets the
structural digital probability as an imaginary-sample proportion, consistent with
Bayesian digital-option inference [12, 18].
Lemma 4(Properness of the structural prior).Ifη str >0andq str ∈(0,1), then
Beta(αstr, βstr)is proper. See [22] for general conditions on proper exponential-
family priors.
Proof.Sinceα str, βstr >0, the Beta density integrates to 1.
14

2.3 Machine–Learning Prior Based on D’Uggento et al. [7]
Motivation.The structural model captures arbitrage-free dynamics but may
miss nonlinear, data-driven relationships. Following [7], the ML component
provides a flexible predictive layer that complements the parametric Heston–
Volterra structure.
D’Uggento et al. [7] compare ANNs and RNNs against Black–Scholes for
option pricing using a large dataset of 73,154 U.S. options. These networks
extract nonlinear, multi–factor relationships between option characteristics, firm
fundamentals, and market variables.
Here we adapt their modeling framework to produce a second prior belief
aboutp=Q(A).
2.3.1 Activation Functions (Sigmoid and Tanh)
Following [7], the primary activation functions are:
Sigmoid:σ(x) = 1
1 +e −x ,
Tanh: tanh(x) = ex −e −x
ex +e −x .
The output layer of the neural network uses a sigmoid activation, ensuring
that the predicted probability
pML :=g NN(x)∈(0,1).
This mapping ensures compatibility with the Beta prior at Level 2 of the hier-
archy, sinceσ(·) naturally outputs Bernoulli success probabilities.
2.3.2 Feedforward ANN Architecture (ANN1/ANN2/ANN3)
D’Uggento et al. include three feedforward architectures:
•ANN1: Black–Scholes inputs (S, K, τ, σ),
•ANN2: ANN1 + dividend information,
•ANN3: full feature set (114 financial, risk, and market variables).
All ANN variants share the abstract structure:
z=σ(W 2 tanh(W1x+b 1) +b 2),
withσused on the output layer. The capacity of such two–layer networks to ap-
proximate nonlinear pricing surfaces follows from standard universal-approximation
results [14].
No WX+B layer–by–layer derivations appear, consistent with directive (12C).
15

2.3.3 Recurrent Neural Network (RNN) Architecture
Temporal dependencies.RNNs allow the model to incorporate time-series
structure present in high-frequency features such as volatility paths or order-flow
signals, providing a more dynamic predictive prior.
The RNN in [7] processes sequences (x1, . . . , xT ). The hidden state satisfies:
ht =f(W hxxt +W hhht−1 +b h), y t =σ(W yht +b y).
Stability of recurrent architectures and their ability to approximate dynamical
systems is established in [10].
We include a TikZ illustration matching Fig. 5 of D’Uggento et al, referencing
a graphical representation of a RNN.
x
h
o
h(...) h(t−1) h(t) h(t+1) h(...)
o(t−1) o(t) o(t+1)
x(t−1) x(t) x(t+1)
W
U
V
W W W W
U U U
V V V
Unfold
16

2.3.4 ML Prior as a Beta Distribution
Define:
pML :=g NN(x),
whereg NN is ANN3 or RNN (best performing per [7]).
We encode this via:
p∼Beta(α ML, βML), α ML =n ML pML, β ML =n ML (1−p ML).
We define an effective sample sizeN eff via the usual variance–ratio method
used in bootstrap variance estimation [8].
nML =r ML(xm)N eff.
Heren ML is a performance–based “virtual sample size” defined next. This
corresponds to an empirical-Bayes moment-matching interpretation [20].
2.4 Formal Reliability Measure Based on Performance Met-
rics
D’Uggento et al. use MAE, MSE, RMSE, MAPE, andR2 for model comparison.
We define the ML reliability indexr ML :X ml →[0,1] by
rML(xm) =σ(W r xm +b r),
whereW r, br are trainable parameters andσis the logistic link. This index
interprets soft confidence from the neural network model. Such reliability scores
parallel Bayesian-dropout uncertainty estimates [11].
Lemma 5(Normalization).If MAPE,R 2, and RMSE are scaled to[0,1], then
r∈[0,1].
Proof.Weighted averages of values in [0,1] lie in [0,1].
Proposition 3(Monotonicity).If the ANN/RNN improves in any metric while
others are fixed, thenrincreases andn ML increases.
Proof.Immediate from partial derivatives ofrwith respect to each metric.
2.5 Fusion of Structural and ML Priors
Given two Beta priors interpreted as independent imaginary data:
(αstr, βstr) = (ηstrqstr, ηstr(1−q str)),
(αML, βML) = (nMLpML, nML(1−p ML)),
the hybrid prior is:
p∼Beta(α 0, β0), α 0 =α str +α ML, β 0 =β str +β ML.
17

Justification.This additive fusion corresponds to convex predictive stack-
ing [24], where each component contributes pseudo-counts proportional to its
predictive accuracy.
Caution.This construction assumes that the ML-based pseudo-counts are
conditionally independent of the structural counts given the true event proba-
bility. Later phases correct this assumption when dependence or distortions are
detected.
The hybrid prior mean is:
p0 = α0
α0 +β 0
=w strqstr +w MLpML,
with weights
wstr = ηstr
ηstr +n ML
, w ML = nML
ηstr +n ML
.
This product form corresponds to a log-likelihood decomposition where each
component contributes additive information under conditional independence,
consistent with composite-likelihood methodology [23].
2.6 Optimal Weighting (Bayes Risk Minimization)
Motivation.Having defined the hybrid prior, we now characterize the con-
ditions under which it minimizes predictive Bayes risk. This provides a formal
justification for the pseudo-count fusion approach used above.
LetL(p,ˆp) = (p−ˆp)2 be quadratic loss. The Bayes estimator from a Beta
prior is the posterior mean. The prior mean minimizing expected loss from
combining two priors is the convex combination above.
Theorem 2(Optimal Hybrid Weighting).Among all convex combinationsˆp=
wqstr + (1−w)p ML, the Bayes–risk minimizing weight is
w∗ = ηstr
ηstr +n ML
,
yieldingˆp=p 0.
This result follows classical Bayesian decision-theory arguments [3].
Interpretation.This theorem shows that pseudo-count fusion preserves Bayes
optimality as long as each component prior contributes unbiased information
aboutp.
Proof.Direct minimization ofE[(p−ˆp)2] overw∈[0,1] using the Beta variances.
18

2.7 Cross–Phase Notation for Likelihood Updating (Pre-
view of Phase 4)
Formal definition.Let (Ω,F) denote the underlying sample space, and let
D={ξ i}n
i=1 represent the observed trader signals, eachξ i ∈ {H, L}. The
structural–ML likelihood is the map
L(D |p) =
nY
i=1
p1{ξi=H}(1−p) 1{ξi=L},
defined with respect to the productσ–algebra, ensuring compatibility with the
Beta posterior update [22].
LetyYES votes andn−yNO votes be observed. Likelihood:
L(s|p) =p y(1−p) n−y.
Posterior:
p|s∼Beta(α post, βpost),
αpost =α 0 +y, β post =β 0 + (n−y).
Thus Phase 2 supplies (α 0, β0) to Phase 4.
2.8 Misspecification Considerations
The structural prior may fail under volatility–regime shifts. The ML prior may
overfit. The hybrid prior mitigates both risks:
•If ML fails,n ML becomes small, so structure dominates.
•If structure is misspecified, high accuracy yieldsn ML ≫η str, shifting
weight to ML.
This conceptual robustness justifies the hybrid construction.
19

2.9 Robustness of the Hybrid Prior Under Structural Mis-
specification
Recall that the structural prior contributes a Beta distribution
p∼Beta(α str, βstr), α str =η str qShi, β str =η str (1−q Shi),
with structural meanq Shi, and the machine learning prior contributes
p∼Beta(α ML, βML), α ML =n ML pML, β ML =n ML (1−p ML),
with meanp ML and pseudo-sample sizen ML. The hybrid prior is then Beta
with parameters
α0 =α str +α ML, β 0 =β str +β ML,
and mean
p0 = α0
α0 +β 0
= ηstrqShi +n MLpML
ηstr +n ML
=w strqShi +w MLpML,
where
wstr = ηstr
ηstr +n ML
, w ML = nML
ηstr +n ML
.
Letp true ∈(0,1) denote the true event probability under the data-generating
process (e.g. under the physical or risk-neutral measure of interest). Define the
structural and ML prior biases
bstr :=q Shi −p true, b ML :=p ML −p true.
Proposition 4(Hybrid Prior Bias Decomposition).The bias of the hybrid prior
meanp 0 with respect to the true probabilityp true decomposes as
p0 −p true =w str bstr +w ML bML,
and satisfies the bound
|p0 −p true| ≤wstr |bstr|+w ML |bML| ≤max{|bstr|,|b ML|}.
Proof.The decomposition follows from the definition ofp 0:
p0−ptrue =w strqShi+wMLpML−ptrue =w str(qShi−ptrue)+wML(pML−ptrue) =w strbstr+wMLbML.
Taking absolute values and using the triangle inequality yields the first bound.
The second inequality usesw str, wML ∈[0,1] andw str +w ML = 1.
The preceding result is purely algebraic but makes explicit that hybrid prior
bias is a convex combination of the individual biases. It follows that if either
component prior is grossly misspecified, the hybrid prior can inherit substantial
bias unless its weight is controlled.
We now formalize a simple robustness property under structural misspecifi-
cation and controlled ML weight.
20

Theorem 3(Hybrid Prior Robustness Under Structural Misspecification).Sup-
pose the following hold:
(i) (Bounded Structural Bias) There existsB str <∞such that|b str| ≤
Bstr for the structural prior meanq Shi.
(ii) (Asymptotically Calibrated ML Prior) Along a sequence of markets
or time windows indexed bym, the ML prior mean satisfies
b(m)
ML :=p (m)
ML −p (m)
true →0asm→ ∞,
in probability or almost surely.
(iii) (Controlled ML Weight) The ML pseudo-sample size is chosen such
that
0≤n (m)
ML ≤C 0 +C 1 n∗,(m),
for constantsC 0, C1 ≥0, wheren ∗,(m) is the effective sample size of the
crowd data in marketm, andη str is either fixed or grows at most linearly
withn ∗,(m).
Then the hybrid prior bias satisfies
p(m)
0 −p (m)
true =w (m)
str b(m)
str +w (m)
ML b(m)
ML ,
with
lim sup
m→∞
|p(m)
0 −p (m)
true| ≤lim sup
m→∞
w(m)
str |b(m)
str |.
In particular, if either
(a) the structural bias is bounded andw (m)
str →0, or
(b) the structural bias itself satisfiesb (m)
str →0,
then the hybrid prior biasp (m)
0 −p (m)
true converges to zero.
Proof sketch.Conditional independence of the two prior components implies
that their joint predictive density is proportional to the product of Beta likeli-
hood factors. Minimizing expected quadratic loss reduces to matching the first
posterior moment, which under the additive fusion is
E[p|α 0, β0] = αstruct +α ML
αstruct +α ML +β struct +β ML
.
Convexity of quadratic risk then yields optimality; see [3].
Remark 2(Empirical Tuning ofn ML).The conditions above suggest that ro-
bustness under structural misspecification depends on both (i) the asymptotic
calibration of the ML prior and (ii) controls on the relative weightw ML. In
practice, one can choosen ML via empirical Bayes or cross-validation, e.g. by
selectingn ML to minimize an out-of-sample scoring rule (Brier score, log score)
for the hybrid prior on historical markets. Imposing an upper bound onn ML
relative to the crowd effective sizen ∗ prevents the ML prior from dominating in
regimes where it has not demonstrated sufficient predictive accuracy.
21

2.10 Machine-Learning Uncertainty and Beta Hybridiza-
tion
In Phase 2, the ML component enters through a point estimatep ML =g NN(x).
To incorporate uncertainty from ensembles or approximate Bayesian neural net-
works without breaking the Beta structure, we view the ML layer as generating
a posterior distribution Π ML onp, and then project Π ML onto the Beta family.
LetB={Beta(a, b) :a, b >0}and let Π ML denote any probability distribu-
tion on (0,1) that captures ML uncertainty (e.g. from an ensemble of approxi-
mate Bayesian networks).
Theorem 4(Hybridization of ML Uncertainty via Beta Projection).LetΠ ML
be a distribution onp∈(0,1)with finite meanm ML and variancev ML >0.
Consider the KL divergence fromΠ ML to a Beta(a, b)distribution,
DKL(ΠML∥Beta(a, b)) =
Z 1
0
log dΠML
dBeta(a, b)(p)dΠ ML(p),
wheneverΠ ML is absolutely continuous with respect toBeta(a, b). Then:
(a) (KL Projection of ML Posterior) There exists a unique pair(a †, b†)
such that
(a†, b†)∈arg min
a,b>0
DKL(ΠML∥Beta(a, b)),
and the minimizing Beta distribution matches the mean and variance of
ΠML,
a†
a† +b † =m ML, a†b†
(a† +b †)2(a† +b † + 1) =v ML.
(b) (Hierarchical Bayes Special Case) Suppose an approximate Bayesian
neural network yields drawsp (l)
ML from a posterior onp, and we place a
Beta(a0, b0)hyperprior onp. The Bayes estimator under a quadratic loss
and Beta restriction coincides with the KL-projection in (a) when(a †, b†)
are chosen to match the empirical mean and variance of the ensemble
{p(l)
ML}, up to hyperprior regularization.
(c) (Ensemble-as-Sample Interpretation) If{p (l)
ML}L
l=1 are treated as noisy
draws of Bernoulli success probabilities andLis large, then the empiri-
cal mean¯p= 1
L
P
l p(l)
ML and empirical varianceˆvdefine a Beta(a †, b†)
distribution via the moment-matching equations in (a). This provides an
empirical-Bayes construction of the ML prior
p∼Beta(a †, b†),
which can then be fused with the structural prior via the hybrid mechanism
in Phase 2.
22

Remark 3.This theorem clarifies that CAPOPM does not requirep ML to be
a single point estimate. Any ML method that yields a distribution over proba-
bilities (e.g. approximate Bayesian neural networks, deep ensembles, bootstrap
aggregations) can be incorporated by projecting its posterior onto the Beta fam-
ily. The resulting(a †, b†)enter the hybrid prior exactly as in Phase 2, preserving
the Beta form while acknowledging ML uncertainty.
2.11 Structural–ML Prior Mismatch and Robustness
The structural priorqShi(K, T; Θ, α) and the ML priorpML(K, T) may be trained
or calibrated under different regimes. For example, the structural model may
use a rough-volatility parameterα <1 while the ML model has been fit on data
that implicitly reflects a smoother regime. This can lead to systematic tension
between the two priors.
To quantify this, consider the strike-dependent priors at a fixed maturityT:
q(K) =q Shi(K, T; Θ, α), p ML(K) =p ML(K, T),
and define the CAPOPM hybrid prior mean
p0(K) =w strq(K) +w MLpML(K),
with weights as in Phase 2.
Theorem 5(Divergence Bounds and Robustness under Prior Mismatch).For
a finite grid of strikes{K j}J
j=1, define discrete distributions
Πstr(j)∝q(K j),Π ML(j)∝p ML(Kj),Π hyb(j)∝p 0(Kj),
normalized to sum to1. Then:
(a) (Total Variation and Hellinger Bounds) For any0≤w str, wML ≤1
withw str +w ML = 1,
∥Πhyb −Π str∥TV ≤w ML∥ΠML −Π str∥TV,
and similarly for Hellinger distanceH,
H(Πhyb,Π str)≤w MLH(ΠML,Π str).
(b) (KL Divergence Control) IfΠ ML is absolutely continuous with respect
toΠ str on the grid, then the discrete KL divergence satisfies
DKL(Πhyb∥Πstr)≤w MLDKL(ΠML∥Πstr).
(c) (Posterior Stability) If the CAPOPM likelihood (based on adjusted order
counts) is Lipschitz with respect to perturbations in the prior on this grid,
then the posterior distribution induced byΠ hyb differs from that induced by
Πstr by at most a constant multiple of the distances above. In particular, as
wML →0, the hybrid posterior converges to the purely structural posterior,
and asw str →0, it converges to the purely ML posterior.
23

Remark 4.This result shows that the hybrid prior acts as a convex bridge
between the structural and ML priors: any mismatch in their implied strike-wise
probabilities is damped by the weightw ML in the distances and divergences. In
practice, this suggests usingw ML (or equivalentlyn ML) as a tuning parameter:
when structural and ML priors strongly disagree, a smaller ML weight yields a
hybrid prior and posterior that remain closer to the structural benchmark, while
still incorporating ML information.
2.12 Summary
Phase 2 constructs a rigorous, multi–tier hybrid prior using:
•tempered–fractional Heston structural probability,
•ANN/RNN predictive model probability (D’Uggento et al.[7]),
•reliability–weighted Bayesian fusion,
•hierarchical prior structure,
•optimal weighting under Bayes risk,
•cross–phase forward compatibility,
•robustness under misspecification.
The output (α0, β0) serves as the input to Phase 4.
3. Asymmetric Information Model (Trader Be-
liefs and Signals)
The purpose of this section is to establish a microeconomic foundation for
why aggregate YES/NO bet volumes—denoted (n yes, nno)—contain informa-
tion about the true event probabilityp:=Q(S T > K).We specialize the asym-
metric information framework of Koessler, Noussair, and Ziegelmeyer [17] to
the CAPOPM setting and show that under mild conditions, the resulting order
flow exhibits a monotone likelihood ratio property in the state of nature. This
in turn justifies the Binomial likelihood used in the Beta–Binomial updating of
Phase 4 and Phase 5.
Throughout, we take as given the hybrid prior from Phase 2,
p∼Beta
 
ηp0, η(1−p 0)

, p 0 ∈(0,1), η >0,(4)
withp 0 as in (??).
3.1 Primitives and Assumptions
We first formalize the economic environment.
24

State of nature and event.Letθ∈ {0,1}denote the state of nature:
θ= 1⇐⇒S T > K, θ= 0⇐⇒S T ≤K,
with prior probabilityp:=Q(θ= 1).The CAPOPM event of interest isA:=
{θ= 1}={S T > K}.
Traders.There is a finite set of tradersi∈N:={1, . . . , n},each of whom is
risk-neutral and participates in a single parimutuel market on eventA. Trader
ichooses an action
si ∈ {YES,NO},
where choosing YES corresponds to buying one YES ticket, and similarly for
NO.
Parimutuel odds.For any action profiles= (s 1, . . . , sn), let
H(s) :={i∈N:s i =H}, h(s) :=|H(s)|, H∈ {YES,NO}.
We denote the number of YES and NO tickets byn yes :=h YES(s) andn no :=
hNO(s).Total participation isn tot :=n yes +n no ≤n.
Following Koessler et al. [17], the parimutuel odds against YES are
OYES(s) := ntot −n yes
nyes
ifn yes >0,(5)
and similarly for NO. For notational simplicity we assume interior participation,
nyes, nno >0; boundary cases can be handled by continuity.
Payoffs.A YES ticket paysO YES(s) + 1 ifθ= 1 and 0 otherwise. Similarly,
a NO ticket paysO NO(s) + 1 ifθ= 0 and 0 otherwise. Traderiis risk-neutral,
so her von Neumann–Morgenstern utility equals her monetary payoff.
We now state the key modeling assumptions.
Assumption A1 (Risk neutrality).Traders have linear utility in payoffs.
Assumption A2 (Price taking).Each trader treats the oddsO YES(s) and
ONO(s) as fixed with respect to her own action, i.e. as determined by the aggre-
gate behavior of other traders. This is standard in parimutuel market models
with a large number of participants [17].
Assumption A3 (Common prior).Traders share a common prior overθ
with meanp 0 as in (4) before observing private signals.
3.2 Information Structure and Posterior Beliefs
Each trader receives a private signal about the state.
25

Assumption A4 (Private signals).Traderiobservesq i ∈ {q1, q0}with
Pr(qi =q 1 |θ= 1) =π,Pr(q i =q 1 |θ= 0) = 1−π, π > 1
2 .(6)
Conditional onθ, signals (q i)i∈N are i.i.d.
Assumption A5 (Common knowledge).The priorp 0, the signal structure
(6), and the parimutuel mechanism are common knowledge among traders.
Given prior meanp 0 and signalq i, traderiforms a posterior belief
µ(qi) := Pr(θ= 1|q i).
Lemma 3.1 (Strict belief ordering).Under Assumptions A3–A5 withπ >
1
2 , the posteriors satisfy
µ1 := Pr(θ= 1|q i =q 1)> µ0 := Pr(θ= 1|q i =q 0).
Proof.By Bayes’ rule,
µ1 = Pr(qi =q 1 |θ= 1) Pr(θ= 1)
Pr(qi =q 1 |θ= 1) Pr(θ= 1) + Pr(q i =q 1 |θ= 0) Pr(θ= 0) = πp0
πp0 + (1−π)(1−p 0),
µ0 = Pr(qi =q 0 |θ= 1) Pr(θ= 1)
Pr(qi =q 0 |θ= 1) Pr(θ= 1) + Pr(q i =q 0 |θ= 0) Pr(θ= 0) = (1−π)p 0
(1−π)p 0 +π(1−p 0).
Sinceπ > 1
2 andp 0 ∈(0,1), straightforward algebra shows thatµ 1 −µ 0 >0.□
Lemma 3.1 ensures that signals areinformative: observingq 1 leads to a
strictly higher posterior belief inθ= 1 than observingq 0.
3.3 Strategies and Bayesian Nash Equilibrium
We now formalize strategies, best responses, and equilibrium.
Definition 3.1 (Strategies).A (pure) strategy for traderiis a mapping
σi :{q 0, q1} → {YES,NO}.
A strategy profile isσ= (σ i)i∈N . A symmetric strategy profile hasσ i =σfor
alli.
Definition 3.2 (Bayesian Nash equilibrium).A symmetric Bayesian Nash
equilibrium (BNE) is a measurable functionσ ∗ :{q 0, q1} → {YES,NO}such
that, for alliand for each signalq∈ {q 0, q1},
σ∗(q)∈arg max
a∈{YES,NO}
E

ui(a, s−i, θ)|qi =q, σ ∗
−i

,
where expectations are taken with respect to the induced distribution over
(θ, q−i, s−i) given the strategy profile.
26

Expected payoffs.Under Assumption A2 (price-taking), traderitakes the
odds (OYES, ONO) as given. For a trader with beliefµ∈[0,1] aboutθ= 1, the
expected payoff from choosing YES is
Ui(YES|µ) =µ
 
OYES + 1

.(7)
Similarly, the expected payoff from choosing NO is
Ui(NO|µ) = (1−µ)
 
ONO + 1

.(8)
Lemma 3.2 (Threshold best response).Fix odds (O YES, ONO). Under
Assumptions A1–A2, there exists a thresholdµ ∗ ∈(0,1) such that:
YES is optimal⇐⇒µ≥µ ∗,NO is optimal⇐⇒µ≤µ ∗.
Proof.Consider the difference:
∆U(µ) :=U i(YES|µ)−U i(NO|µ) =µ(O YES + 1)−(1−µ)(O NO + 1).
This is affine inµ:
∆U(µ) =µ

(OYES + 1) + (ONO + 1)

−(O NO + 1).
Solve ∆U(µ∗) = 0 forµ ∗:
µ∗ = ONO + 1
(OYES + 1) + (ONO + 1) ∈(0,1).
Then ∆U(µ)≥0 iffµ≥µ ∗, establishing the threshold property.□
Combined with Lemma 3.1, Lemma 3.2 implies that traders with signalq 1
are strictly more likely to choose YES than those withq 0.
Assumption A6 (Separating equilibrium as in Koessler et al.).Fol-
lowing Koessler et al. [17], we focus on a symmetric BNE in which informed
traders adopt aseparating threshold strategy:
σ∗(q1) = YES, σ ∗(q0) = NO.(9)
Uninformed or purely noisy traders, if present, randomize independently of
the state in a way that does not overturn the strict monotonicity implied by
Lemma 3.1 and Lemma 3.2. 1
Under Assumptions A1–A6, informed traders withq 1 bet YES, and those
withq 0 bet NO in equilibrium. This yields a monotone mapping from signals
to actions.
1Koessler et al. [17] provide existence and characterization results for such equilibria in
parimutuel information aggregation mechanisms. We adopt their equilibrium selection as a
modeling assumption and specialize it to the CAPOPM event.
27

3.4 Distribution of Order Flow Conditional on the State
We next derive the distribution of YES/NO orders conditional on the stateθ
under the separating equilibrium (9).
Lemma 3.3 (Signal distribution).Under Assumption A4, conditional on
θ, the number of traders receiving signalq 1 satisfies
N1 |θ= 1∼Binomial(n, π), N 1 |θ= 0∼Binomial(n,1−π),
andN 0 :=n−N 1.
Proof.Immediate from the i.i.d. Bernoulli structure in Assumption A4.□
Under the separating equilibrium (9), informed traders withq 1 bet YES and
those withq 0 bet NO. If a fraction of traders are purely noisy or always abstain,
thenncan be reinterpreted as the number of informed traders; noisy traders add
independent Bernoulli noise that does not destroy the strict ordering established
below, as long as the informed fraction is positive.
Lemma 3.4 (Order flow distribution).Under Assumptions A1–A6 and
Lemma 3.3, there existϕ 1, ϕ0 ∈(0,1) withϕ 1 > ϕ0 such that
nyes |θ= 1∼Binomial(n, ϕ 1), n yes |θ= 0∼Binomial(n, ϕ 0).
Proof.In the benchmark case with only informed traders and separating
strategies, we have
Pr(si = YES|θ= 1) = Pr(q i =q 1 |θ= 1) =π,
Pr(si = YES|θ= 0) = Pr(q i =q 1 |θ= 0) = 1−π.
Thusϕ 1 =πandϕ 0 = 1−π, withϕ 1 > ϕ0 becauseπ > 1
2 .
If anε-fraction of traders are noisy and bet YES independently with prob-
abilityδ, then
ϕ1 = (1−ε)π+εδ, ϕ 0 = (1−ε)(1−π) +εδ.
The strict inequalityϕ 1 > ϕ0 continues to hold as long asε <1 andπ > 1
2 .
Since trades are independent conditional onθ, the number of YES ordersn yes
is Binomial with parameters (n, ϕ θ), whereϕ 1 andϕ 0 denote the respective
conditional success probabilities.□
Lemma 3.4 provides amicro-foundedBinomial distribution of YES orders
conditional on the state of nature.
3.5 Monotone Likelihood Ratio and Informativeness
We now show that the order flow is informative about the stateθ, and hence
aboutp.
28

Proposition 3.1 (Monotone likelihood ratio of order flow).Under
Lemma 3.4 withϕ 1 > ϕ0, the likelihood ratio
Λ(k) := Pr(nyes =k|θ= 1)
Pr(nyes =k|θ= 0)
is strictly increasing ink= 0,1, . . . , n.
Proof.Fork∈ {0, . . . , n},
Pr(nyes =k|θ= 1) =
n
k

ϕk
1(1−ϕ 1)n−k,
Pr(nyes =k|θ= 0) =
n
k

ϕk
0(1−ϕ 0)n−k.
Hence,
Λ(k) = ϕk
1(1−ϕ 1)n−k
ϕk
0(1−ϕ 0)n−k =
ϕ1
ϕ0
k1−ϕ 1
1−ϕ 0
n−k
.
Then
Λ(k+ 1)
Λ(k) = ϕ1
ϕ0
· 1−ϕ 0
1−ϕ 1
.
Becauseϕ 1 > ϕ0, we haveϕ 1/ϕ0 >1 and (1−ϕ 0)/(1−ϕ 1)>1, so the product
exceeds unity:
Λ(k+ 1)
Λ(k) >1.
Therefore Λ(k) is strictly increasing ink.□
Proposition 3.1 establishes a monotone likelihood ratio (MLR) property for
the aggregate YES order flow.
Theorem 3.1 (Informative parimutuel order flow).Under Assumptions
A1–A6 and Lemmas 3.1–3.4, the aggregate YES countn yes is informative about
the stateθ, and thus about the event probabilityp:
1. The posterior probability Pr(θ= 1|n yes =k) is strictly increasing ink.
2. Observing (n yes, nno) yields a non-degenerate likelihood forp.
Proof.(1) The MLR property in Proposition 3.1 implies that the family
{Pr(· |θ)}is ordered in the monotone likelihood ratio sense. By standard
Bayesian decision theory (e.g. Karlin and Rubin’s theorem), this implies that
the posterior Pr(θ= 1|n yes =k) is strictly increasing ink.
(2) Sinceϕ 1 > ϕ0, the two conditional Binomial distributions are distinct,
implying that the mapping frompto the mixture distribution ofn yes is non-
degenerate. Hence the induced likelihood overpis non-constant and thus infor-
mative.□
Theorem 3.1 provides the desired microeconomic foundation: equilibrium
YES/NO bet volumes in the CAPOPM parimutuel mechanism contain infor-
mation about the true event probabilityp.
29

3.6 Connection to the CAPOPM Beta–Binomial Likelihood
We now connect the micro-founded order-flow distribution to the CAPOPM
likelihood used in Phases 4 and 5.
Under Theorem 3.1, we may interpret the observed pair (n yes, nno) as a
Binomial sample from the unknown event probabilityp:
Pr
 
nyes =k|p

=
n
k

pk(1−p) n−k, k= 0,1, . . . , n,
up to reparameterization ofnand the effective success probabilitiesϕ θ when
conditioning onθ. Therefore, suppressing constants inp, the likelihood ofp
given the data is
L(p|n yes, nno)∝p nyes(1−p) nno .(10)
Corollary 3.1 (CAPOPM posterior under the hybrid prior).Com-
bining the hybrid prior (4) with the Binomial likelihood (10), the posterior
distribution ofpis
p|n yes, nno ∼Beta
 
ηp0 +n yes, η(1−p 0) +n no

.
Proof.Immediate from Beta–Binomial conjugacy: multiply the prior density
pηp0−1(1−p) η(1−p0)−1 by the likelihoodp nyes(1−p) nno and recognize the kernel
of a Beta distribution with parametersηp 0 +n yes andη(1−p 0) +n no.□
Corollary 3.1 closes the loop between the microeconomic model of asym-
metric information in a parimutuel environment and the Bayesian updating
machinery of CAPOPM. The Beta posterior derived here is the starting point
for the posterior predictive pricing of digital and vanilla options in Phase 4 and
Phase 5.
3.7 Strategic Behavior, Reflexivity, and Bayesian–Nash Equi-
librium
Assumption A6 treats trader actions as if they were conditionally independent
signals that do not strategically anticipate the CAPOPM correction mechanism.
While this assumption is suitable for ex-post belief extraction or markets with
numerous small traders, it does not fully address reflexive behavior in which
traders may try to influence the posterior or exploit the weighting scheme.
To formalize this tension, we introduce a stylized strategic model in which
each trader is small but rational, observes the current parimutuel oddsπ t, and
decides whether to submit a YES or NO order. The key distinction from As-
sumption A6 is that traders now optimize expected utility conditional on (i)
the current odds and (ii) their private signal. Importantly, traders donotob-
serve the detailed CAPOPM correction parameters (w beh
i , δ±), which prevents
manipulation directed at the correction mechanism itself and keeps the model
tractable.
30

Letθ∈ {0,1}be the true event state and lets i ∈ {+1,−1}denote the
private signal observed by traderi, where +1 corresponds to a signal favoring
YES and−1 to a signal favoring NO. Conditional onθ, the signals satisfy
P(si =θ) =σ >1/2,P(s i =−θ) = 1−σ,
and traders know bothσand the oddsπ t but do not know the realizedθ.
Upon submitting a YES order, traderireceives payoff 1/π t ifθ= 1 and zero
otherwise, consistent with parimutuel mechanics for unit bets; conversely, a NO
order yields 1/(1−π t) ifθ= 0.
Each trader chooses YES or NO to maximize expected payoff given (s i, πt).
Letu i(YES|s i, πt) andu i(NO|s i, πt) denote these expected payoffs. A
Bayesian–Nash equilibrium (BNE) consists of a strategy profile in which each
trader best responds to the odds, and the odds reflect the empirical frequencies
of submitted orders.
Theorem 6(Bayesian–Nash Equilibrium with Signal-Driven Strategies).As-
sume all traders are small, observe the same oddsπ t, have identical signal accu-
racyσ >1/2, and submit a single indivisible order. Suppose traders do not ob-
serve the CAPOPM correction parameters(w beh
i , δ±)and thus cannot condition
strategies on them. Then there exists a symmetric Bayesian–Nash equilibrium
in threshold form. Specifically, there is a cutoffπ ∗(σ)∈(0,1)such that:
•Ifπ t < π∗(σ)(YES is “cheap”), all traders withs i = +1submit YES and
all traders withs i =−1submit NO.
•Ifπ t > π∗(σ)(YES is “expensive”), all traders withs i =−1submit NO
and traders withs i = +1may randomize to keep the odds at equilibrium.
•Atπ t =π ∗(σ), traders are indifferent and a symmetric mixed-strategy
equilibrium exists.
Moreover, the equilibrium cutoff solves
P(θ= 1|s i = +1)
π∗(σ) = P(θ= 0|s i = +1)
1−π ∗(σ) ,
and similarly fors i =−1. The equilibrium is unique under these conditions.
Proof (Sketch).Given parimutuel payoffs, a YES order yields expected payoff
ui(YES|s i, πt) = P(θ= 1|s i)
πt
,
and a NO order yields
ui(NO|s i, πt) = P(θ= 0|s i)
1−π t
.
31

A trader chooses YES iff
P(θ= 1|s i)
πt
> P(θ= 0|s i)
1−π t
.
The posterior probabilities on the right-hand side can be computed via Bayes’
rule. The above inequality defines a threshold inπ t separating the regions where
YES or NO is optimal. Because all traders are small, they takeπ t as given; the
equilibrium odds must be consistent with aggregate order flow induced by these
best responses. Standard fixed-point arguments for parimutuel odds imply the
existence of a unique solution to the resulting consistency condition, yielding
the stated threshold equilibrium.
Remark 5(Interpretation and Relation to Assumption A6).The equilibrium
above separates (i) the informational content of private signals and (ii) the
strategic incentive created by parimutuel odds. Because traders do not observe
the CAPOPM correction parameters, they cannot directly manipulate the behav-
ioral biases or structural offsets, and the BNE captures only the odds-reactive
part of strategic behavior.
Assumption A6 corresponds to the “implicit-signal” limit in whichπ t ≈p true,
traders are small, and the odds reflect aggregated signals rather than strategic
anticipation. The model developed here formalizes the gap between A6 and a
fully reflexive environment, while remaining compatible with CAPOPM as an
ex-post belief extraction mechanism.
3.8 Strategic Feedback, Pooling Equilibria, and Asymptotic
Unraveling
Assumption A2 treats traders as price takers, even though parimutuel odds de-
pend on aggregate order flow. Here we provide a complementary game-theoretic
view that (i) constructs pooling equilibria under heterogeneous signal precision,
(ii) gives conditions under which pooling cannot persist, and (iii) shows how
informational content can unravel as the number of traders grows.
Theorem 7(Pooling, Non-Pooling, and Asymptotic Unraveling).Consider
a parimutuel market withNtraders. Traderireceives a binary signals i ∈
{0,1}aboutθ∈ {0,1}with precisionσ i =P(s i =θ)>1/2. Let oddsπ N be
a continuous function of the empirical YES fraction, and suppose payoffs are
standard parimutuel (unit stake, pool-splitting). Then:
(a) (Existence of Pooling Equilibria) If signal precisions{σ i}are het-
erogeneous and sufficiently dispersed, there exists a Bayesian–Nash equi-
librium in which some subset of traders ignores their private signal and
mimics the behavior of a reference group, leading to pooling of actions
across different signal realizations.
(b) (Impossibility Under Monotonicity and Full Support) If, in addi-
tion, odds are strictly monotone in the empirical YES fraction and each
32

trader’s expected payoff from following their signal dominates any constant
strategy (given others follow their signals), then no fully pooling equilib-
rium exists: in any BNE, at least a positive-measure subset of traders uses
their signal in a non-trivial way.
(c) (Asymptotic Unraveling) Under the conditions of (b), if the number of
tradersN→ ∞while the distribution of precisions{σ i}remains bounded
away from1/2, then any sequence of equilibria must be asymptotically
non-pooling in the sense that the empirical distribution of actions reveals
a non-degenerate amount of information aboutθ; the odds move toward
the true probability in the limit.
Remark 6.Part (a) shows that pooling equilibria are possible in finite markets,
so A2 does not literally hold as a behavioral assumption. Parts (b) and (c) clarify
that under monotone odds and sufficiently informative signals, large markets
tend to unravel pooling, making the empirical order flow informative. CAPOPM
is designed to operate in this asymptotic regime, treating observed order flow as
a noisy but informative aggregate signal aboutθ.
33

3.9 Interpretation of Assumption A2 in a Parimutuel Set-
ting
Assumption A2 posits that, conditional on the latent event probabilityp, trader
orders can be modeled as conditionally independent Bernoulli signals, with no
explicit feedback from individual actions onto the odds. In a parimutuel market,
however, odds are determined by the aggregate order flow: each trader’s action
affects the pool, and hence the implied payoffs. This creates an apparent conflict
between A2 and the mechanics of parimutuel pricing.
In this subsection, we show that A2 can be understood as alarge-market
approximationin which each trader is small and the dependence induced by the
parimutuel odds becomes negligible for finite sets of traders. The result does not
remove all strategic considerations (these are treated separately in the strategic
extension) but clarifies how an approximately independent signal model emerges
in the limit of many small traders.
Consider a sequence of parimutuel markets indexed byN, each withN
traders. In marketN, traderi∈ {1, . . . , N}chooses an actionY(N)
i ∈ {0,1},
with 1 indicating a YES order and 0 a NO order. Let
ΠN = 1
N
NX
i=1
Y (N)
i
denote the empirical YES fraction, and suppose parimutuel odds are determined
by a deterministic, continuous function
πN = Φ(ΠN ),
where Φ : [0,1]→(0,1) is Lipschitz and strictly increasing. Thus each trader
faces oddsπ N , which depend on aggregate behavior.
We assume traders are small and symmetric: conditional on the true event
probabilityp true ∈(0,1) and on the limiting oddsπ ∗, each trader adopts a mixed
strategy with YES probabilityβ(p true, π∗)∈(0,1). The equilibrium oddsπ ∗
solve the fixed-point condition
π∗ = Φ
 
β(ptrue, π∗)

.
For each finiteN, the actual oddsπ N fluctuate aroundπ ∗ as Π N fluctuates
aroundβ(p true, π∗).
Theorem 8(Approximate Conditional Independence in Large Parimutuel Mar-
kets).Suppose:
(i) The mappingΦis Lipschitz continuous and strictly increasing on[0,1].
(ii) The mixed strategyβ(p true, π)is continuous inπand strictly between0
and1for the relevant range of odds.
(iii) The fixed-point equationπ ∗ = Φ(β(ptrue, π∗))has a unique solutionπ ∗ ∈
(0,1).
34

Then the following hold asN→ ∞:
(a) The empirical YES fraction converges in probability,
ΠN
P
− →β(ptrue, π∗),
and therefore the realized odds converge in probability,
πN = Φ(ΠN )
P
− →π∗.
(b) For any fixedk≥1, the joint law of anykdistinct trader actions(Y (N)
i1 , . . . , Y(N)
ik )
converges in total variation to a product of independent Bernoulli variables
with parameterβ(p true, π∗), i.e.
lim
N→∞
L
 
(Y (N)
i1 , . . . , Y(N)
ik )

−Bernoulli
 
β(ptrue, π∗)
⊗k
TV
= 0.
In particular, for largeN, any fixed finite set of trader orders is approximately
conditionally independent givenp true and the limiting oddsπ ∗, so that Assump-
tion A2 can be interpreted as a large-market approximation valid for ex-post
belief extraction.
Proof (Sketch).Under (i)–(iii), the law of large numbers applied to the sym-
metric mixed strategies implies that the empirical fraction Π N converges in
probability to the unique fixed point of the mapping Π7→β(p true,Φ(Π)). By
continuity and uniqueness, this fixed point corresponds to Π∗ =β(p true, π∗), and
thus ΠN →Π ∗ in probability. Lipschitz continuity of Φ then yieldsπ N →π ∗ in
probability, establishing (a).
For (b), conditional onπN , the trader actions are exchangeable and indepen-
dent given the common mixed strategy parameterβ(p true, πN ). Asπ N →π ∗,
continuity ofβgives
β(ptrue, πN )→β(p true, π∗)
in probability. Standard arguments for triangular arrays of conditionally inde-
pendent Bernoulli variables imply that the finite-dimensional distributions of
(Y (N)
i1 , . . . , Y(N)
ik ) converge in total variation to those of i.i.d. Bernoulli variables
with the limiting parameter. This yields the stated approximate independence
for any fixedk.
Remark 7(Scope and Limitations).The theorem above shows that, in a large
parimutuel market with symmetric small traders and a unique fixed point for the
odds, the dependence introduced by the parimutuel mechanism becomes negligible
for any fixed finite set of traders. In this sense, Assumption A2 is consistent
with parimutuel pricing as a large-market approximation.
However, several limitations remain. First, the result does not address pool-
ing equilibria or multiple fixed points, which can arise when traders are heteroge-
neous or when the odds functionΦhas non-monotone features. Second, strategic
considerations beyond mixed strategies that depend only onπ N (e.g. anticipa-
tory behavior targeted at CAPOPM corrections) are not modeled here and can
reintroduce nontrivial dependence. These issues are examined separately in the
strategic extension and are left as directions for further work.
35

Phase 4. Parimutuel Likelihood and Bayesian
Posterior Update
In this phase, we derive the likelihood generated by parimutuel YES/NO trader
actions and combine it with the hybrid prior from Phase 2 to construct the
posterior distribution of the event probability
p:=Q(A) =Q(S T > K).
All notation, including the hybrid prior parameters (α 0, β0), follows Phase 2.
4.1 Trader Actions and the Likelihood Model
Letndenote the number of traders participating in the parimutuel book, and
letydenote the number of YES positions. A NO position is treated as a vote
for the complementA c.
Traders may act strategically: some may exaggerate their signals, herd be-
hind early order flow, or attempt to manipulate the book. We acknowledge
the possibility of such distortions but defer correction to Phase 6. For the pur-
poses of likelihood construction, we treat the realized counts (y, n−y) as the
observable actions that the mechanism must interpret.
Although traders’ private signals may be correlated, and their actions may
be strategically dependent, we assume conditional independence given the latent
probabilitypfor the sole purpose of deriving the Beta–Binomial conjugacy. This
follows standard practice in Bayesian market microfoundations.
4.2 Binomial Likelihood of the Parimutuel Order Flow
Conditionally on the latent event probabilityp, we model the YES countyas
y|p∼Binomial(n, p),
with likelihood
L(y|p) =
n
y

py(1−p) n−y.(11)
Although later phases introduce liquidity-adjusted countsy ∗ andn ∗, the
present phase uses the raw counts (y, n−y) for the purpose of deriving the
conjugate update.
4.3 Conjugate Updating with the Hybrid Prior
The hybrid prior from Phase 2 is
p∼Beta(α 0, β0), α 0, β0 >0.
36

Lemma 6(Posterior Form).Combining the Beta prior with the Binomial like-
lihood(11)yields the posterior
p|y∼Beta(α post, βpost),
where
αpost =α 0 +y, β post =β 0 + (n−y).
Proof.The Beta density is proportional top α0−1(1−p) β0−1. Multiplying by
(11) gives a kernel proportional to
pα0+y−1(1−p) β0+(n−y)−1,
the kernel of a Beta(α post, βpost).
4.4 Posterior Mean and Variance
Proposition 5(Posterior Moments).For the posterior Beta distribution above,
E[p|y] = αpost
αpost +β post
,
and
Var(p|y) = αpostβpost
(αpost +β post)2(αpost +β post + 1).
Proof.These are standard properties of the Beta distribution.
4.5 Posterior Predictive Distribution (Beta–Binomial Form)
The posterior predictive distribution of observingyYES votes under the prior
Beta(α0, β0) is
P(y|α 0, β0) =
n
y
 B(α0 +y, β0 +n−y)
B(α0, β0) ,
whereB(·,·) is the Beta function.
Theorem 9(Posterior Predictive Distribution).Letp∼Beta(α 0, β0)andy|
p∼Binomial(n, p). Then the marginal distribution ofyis Beta–Binomial with
pmf above.
Proof.Integrate the joint distributionP(y|p)f(p) overp∈[0,1], and use the
identity Z 1
0
pa−1(1−p) b−1 dp=B(a, b).
37

4.6 Conditional Independence as a Modeling Approxima-
tion
The Beta–Binomial updating step presented above relies on the assumption
that, conditional on the latent event probabilityp, individual trader actionss i ∈
{YES,NO}are independent Bernoulli draws. Such conditional independence is
standard in Bayesian aggregation models, but it is not expected to hold exactly
in parimutuel markets where traders may observe and react to earlier order flow.
In particular, herding behavior generates temporal dependence among trades:
late traders may overweight recent order patterns even when those patterns do
not reflect new private information.
In this framework, conditional independence is therefore best interpreted as
amodeling approximationrather than a literal behavioral assumption. Empiri-
cally observed violations of independence are handled in two ways:
1.Behavioral Adjustment (Phase 6).The Stage 1 correction introduces
weightsw beh
i applied to individual orders. When herding creates clusters
of correlated trades, these weights reduce the effective contribution of late
correlated orders, mitigating departures from independence.
2.Sensitivity Analysis (Phase 7).Simulation regimes in Phase 7 intro-
duce explicit dependence structures among trades, including herding and
correlated decision rules. These regimes allow us to evaluate how viola-
tions of independence affect the CAPOPM posterior and how effectively
the two-stage bias-correction layer controls such deviations.
This modeling approximation preserves conjugacy and analytic tractability
while acknowledging that the empirical behavior of order flow contains richer
dependence patterns. Phases 6 and 7 are specifically designed to examine, in-
terpret, and correct these dependencies.
4.7 Incorporation of Strategic Distortion
Because traders may submit exaggerated or strategically distorted orders, the
raw counts (y, n−y) encode:
1. private signals,
2. beliefs about other traders’ signals,
3. strategic considerations,
4. liquidity constraints.
Phase 6 will introduce formal bias adjustments usingeffectivecounts (y∗, n∗−
y∗) and distortion offsets (δ +, δ−). For now, the posterior above represents the
unadjustedBayesian update based on the observable order flow.
38

4.8 Conditional i.i.d., Dependence, and the Role of Weights
Assumption A4 models the adjusted order contributions as conditionally i.i.d.
Bernoulli (or bounded) signals given the latent event probabilityp. This is
a deliberate simplification. In realistic markets, herding, order-splitting, and
informational cascades induce dependence across orders.
There are two conceptually distinct modeling choices:
•Fully dependent likelihood.One could specify an explicit joint law for the
order sequence (Z1, . . . , Zn), for example via an Ising model or a Markov
random field. This yields a non-factorizing likelihood and a non-conjugate
posterior that typically requires MCMC or variational methods.
•Weighted pseudo-likelihood.CAPOPM instead uses a Beta–Binomial up-
date based on adjusted counts (y∗
n, n∗
n), together with mixing-based asymp-
totics (Phase 8). This implicitly replaces the true dependent likelihood
with an exponential family surrogate whose sufficient statistics are the
weighted sums. The resulting Beta posterior is then interpreted as the KL-
projection of the intractable posterior onto the Beta family (Phase 8ZZ).
In this sense, the behavioral weighting layer is not merely a “patch” on an
i.i.d. model, but a way to summarize dependence and heterogeneity into effective
counts that remain compatible with a tractable exponential family update. The
trade-off is explicit: CAPOPM sacrifices an exact likelihood for closed-form
inference plus an information-theoretic guarantee that, within the Beta family,
the posterior is as close as possible (in Kullback–Leibler sense) to the ideal but
intractable posterior.
4.9 Why CAPOPM Uses Beta Conjugacy
There are many ways to model belief aggregation in markets. CAPOPM uses a
Beta–Binomial structure for a simple reason: it gives closed-form updates and
keeps the link between data and parameters transparent.
•Each component of the prior can be read as a pseudo-count: structural in-
formation contributesη str virtual observations, the ML model contributes
nML, and the crowd contributes adjusted effective countsn ∗
n.
•Updating is a matter of adding these counts, with no numerical integration
or sampling required.
•The resulting posterior has a clear interpretation: it is the Beta distri-
bution that best matches, in KL sense, the information contained in the
adjusted counts and the hybrid prior.
More flexible approaches, such as MCMC over a fully dependent likelihood,
can capture richer structures but at the cost of interpretability and computation.
CAPOPM is intentionally positioned as a tractable, interpretable baseline: it
39

prioritizes closed-form inference and moment-based robustness over fully non-
parametric modeling. This makes it easier to diagnose, explain, and test in
simulation before considering heavier alternatives.
4.10 Output of Phase 4
The output of this phase is the posterior hyperparameter pair
(αpost, βpost),
which becomes the foundation for Phase 5 (posterior predictive pricing) and is
later refined in Phase 6 (bias–corrected posterior).
Phase 5. Posterior Predictive Derivative Pricing
Given the posterior distribution of the event probability
p:=Q(A) =Q(S T > K)
from Phase 4, we now derive the posterior–predictive prices of YES/NO parimutuel
contracts and digital derivatives. We additionally establish no–arbitrage prop-
erties, uncertainty bounds, and risk–adjusted pricing rules.
Throughout, the posterior from Phase 4 is
p|y∼Beta(α post, βpost),
with
αpost =α 0 +y, β post =β 0 + (n−y).
5.1 From CAPOPM Posteriors to Pricing Kernels and Op-
tion Prices
In previous phases, CAPOPM produces, for a fixed maturityT, a posterior
distribution for the event probability
p(K, T) =Q(ST > K| I),
whereIdenotes the combined information from the structural model, the ma-
chine learning prior, and the adjusted parimutuel order flow. Let ˆp(K, T) denote
the posterior mean, so that the CAPOPM-implied digital price at strikeKis
DCAP(K, T) =e−rT ˆp(K, T),
withrthe risk-free rate.
We now show how the entire risk-neutral distribution and pricing kernel can
be recovered (at least formally) from the strike-dependent posterior, and how
vanilla option prices follow via standard integral transforms.
40

Theorem 10(CAPOPM-Implied Risk-Neutral CDF, Density, and Pricing Ker-
nel).Fix a maturityT >0and suppose that, for each strikeK≥0, CAPOPM
produces a posterior distribution forp(K, T)with meanˆp(K, T). Assume:
(i) (Smoothness in Strike) The mapK7→ˆp(K, T)is differentiable almost
everywhere, withˆp(K, T)non-increasing inKand
lim
K→0
ˆp(K, T) = 1,lim
K→∞
ˆp(K, T) = 0.
(ii) (No-Arbitrage Regularity) The functionˆp(K, T)is right-continuous
with left limits and defines a valid tail function for a probability distribution
on[0,∞).
Then:
(a) (Risk-Neutral CDF and Density) The CAPOPM-implied risk-neutral
CDF and density at maturityTare given by
FQ
CAP(K, T) =Q(ST ≤K| I) = 1−ˆp(K, T),
and, wherever differentiable,
fQ
CAP(K, T) = ∂
∂K FQ
CAP(K, T) =− ∂
∂K ˆp(K, T).
(b) (Call Prices and Breeden–Litzenberger) The CAPOPM-implied call
price at strikeKand maturityTis
CCAP(K, T) =e−rT
Z ∞
K
(s−K)f Q
CAP(s, T)ds.
Equivalently, if we define
CCAP(K, T) =e−rT EQ[(ST −K) + | I],
then the Breeden–Litzenberger relation holds:
∂2
∂K2 CCAP(K, T) =e−rT fQ
CAP(K, T),
whenever the derivatives exist.
(c) (State-Price Density and Pricing Kernel) The state-price density
associated with CAPOPM at maturityTis
ϕCAP(s, T) =e−rT fQ
CAP(s, T),
so that
CCAP(K, T) =
Z ∞
K
(s−K)ϕ CAP(s, T)ds.
41

IfPdenotes a physical measure under whichS T has densityf P , and if
fP is strictly positive on the support off Q
CAP, then the CAPOPM-implied
pricing kernel can be written (up to normalization) as
mCAP(s, T)∝ fQ
CAP(s, T)
fP (s) .
Proof (Sketch).Under (i) and (ii), the functionK7→ˆp(K, T) satisfies the basic
properties of a strike-tail function: it is non-increasing, right-continuous, and
converges to 1 and 0 at the boundaries. Thus it defines
FQ
CAP(K, T) = 1−ˆp(K, T)
as a valid CDF on [0,∞), and the densityf Q
CAP =∂ KFQ
CAP exists almost every-
where, yielding part (a).
For part (b), the standard risk-neutral pricing relation gives
CCAP(K, T) =e−rT
Z ∞
K
(s−K)f Q
CAP(s, T)ds.
Differentiating once with respect toKyields
∂
∂K CCAP(K, T) =−e−rT
Z ∞
K
fQ
CAP(s, T)ds=−e −rT (1−F Q
CAP(K, T)),
and differentiating a second time gives
∂2
∂K2 CCAP(K, T) =e−rT fQ
CAP(K, T),
which is the Breeden–Litzenberger formula applied to the CAPOPM-implied
density.
For part (c), the state-price density is by definition the Radon–Nikodym
derivative of the pricing operator with respect to Lebesgue measure, which
in continuous-time arbitrage-free settings ise −rT fQ(s, T). Substitutingf Q
CAP
yieldsϕ CAP. When a physical densityf P is available, absolute continuity ofQ
with respect toPon the relevant support implies
dQ
dP (s)∝ fQ
CAP(s, T)
fP (s) ,
so thatm CAP(s, T)∝dQ/dPhas the indicated form.
Remark 8(Discrete Approximation from a Strike Grid).In practice, CAPOPM
will produce posterior meansˆp(Kj, T)on a discrete grid of strikes{K j}J
j=1. The
CAPOPM-implied CDF and density can then be approximated by
FQ
CAP(Kj, T)≈1−ˆp(K j, T),
42

and
fQ
CAP(Kj, T)≈ −ˆp(Kj+1, T)−ˆp(Kj, T)
Kj+1 −K j
,
forj= 1, . . . , J−1. Similarly, the call price curve can be approximated via
CCAP(Kj, T)≈
J−1X
l=j
e−rT ˆp(Kl, T) ∆Kl,∆K l =K l+1 −K l,
which implements the integral in (b) as a Riemann sum. These discrete ap-
proximations provide a direct pathway from CAPOPM posteriors to implied call
prices and densities on a finite strike grid.
5.1.1 CAPOPM-Implied Kernels and Asset Pricing Puzzles
The CAPOPM framework produces a risk-neutral distributionf Q
CAP(·, T) and
associated state-price densityϕ CAP(s, T) =e−rT fQ
CAP(s, T). This section con-
nects these objects to empirical pricing kernels and the stylized facts of asset
pricing puzzles.
Theorem 11(CAPOPM Kernels and Kernel Shapes).Letf Q
CAP(s, T)be the
CAPOPM-implied risk-neutral density, and suppose there exists a physical den-
sityf P (s)forS T withf P (s)>0on the support off Q
CAP. Define the CAPOPM
pricing kernel
mCAP(s, T) =ϕCAP(s, T)
fP (s) =e −rT fQ
CAP(s, T)
fP (s) .
Assume:
(i) The structural distributionf Θ,α(s;T)satisfies the tail monotonicity prop-
erties in the fractional-parameter theorem of Phase 1.X.
(ii) The ML prior and parimutuel adjustments preserve stochastic ordering
in the sense that higher structural tails lead to higher posterior tails at
extreme strikes.
Then:
(a) If decreasingα(rougher volatility) increases structural right tails forS T ,
the CAPOPM kernelm CAP(s, T)becomes more downward-sloping over
highsrelative to a smoother-volatility benchmark, amplifying the weight
on bad states and steepening implied risk prices.
(b) For strike ranges where CAPOPM posterior tails exceed a benchmark
Black–Scholes density, the corresponding CAPOPM kernel assigns greater
marginal value to those states, consistent with steep risk-neutral densities
and pronounced volatility smiles.
43

(c) If empiricalf P is calibrated from historical returns whilef Q
CAP embeds
both rough structural dynamics and parimutuel belief adjustments, then
mCAP(s, T)can exhibit shapes (e.g. non-monotonicity, steep downward
slopes) that resemble empirical pricing kernels used to explain the equity
premium puzzle and related anomalies.
Remark 9(Interpretation).The theorem does not claim that CAPOPM re-
solves asset pricing puzzles. Instead, it shows that the combination of rough
structural volatility, data-driven priors, and crowd-based adjustments can nat-
urally generate risk-neutral distributions and pricing kernels with heavier tails
and steeper state-price densities than classical lognormal models. This places
CAPOPM in the same qualitative family as empirical kernel reconstructions
used in equity-premium and volatility-smile studies.
5.1.2 Kernel Regularization Under the Structural Heston Prior
The final CAPOPM outputs in Phase 5 are option prices computed under
a risk–neutral measureQinduced by a pricing kernel (state–price density)
MT = dQ
dP

FT
. If the kernel derived from a misspecified model or from a raw
mixture/particle posterior is not carefully regularized, it may violate basic con-
ditions:positivity,unit expectation, or themartingaleproperty for discounted
asset prices. This subsection introduces a kernel regularization layer, based on
the structural Heston prior of Phase 1, that enforces no–arbitrage while pre-
serving the posterior information accumulated in Phases 2–6.
Structural Heston Prior underP.Recall from Phase 1 that under the
physical measurePthe asset priceS t and variancev t follow a Heston–type
stochastic volatility model:
dSt =S t
 
µ dt+√vt dWS
t

,(12)
dvt =κ(θ−v t)dt+σ √vt dWv
t ,(13)
where (W S
t , Wv
t ) are Brownian motions underPwith correlationρ∈[−1,1],
and parameters (µ, κ, θ, σ) lie in the admissible Heston region. The CAPOPM
event–probability posterior Πϕ(· | D) from Phases 5–6 is interpreted as provid-
ing information about the terminal distribution ofS T and related events (e.g.
default, barrier crossing) rather than replacing the structural dynamics.
Exponential Martingale Pricing Kernel.We construct a pricing kernel
as an exponential martingale with a market–price of risk processλ t:
Mλ
t := exp

−
Z t
0
λs dWS
s − 1
2
Z t
0
λ2
s ds

, t∈[0, T],(14)
where (λt)t∈[0,T] is progressively measurable and adapted to (F t).
44

Assumption 1(Novikov Condition for the Kernel).The processλ t satisfies
Novikov’s condition:
EP
"
exp
 
1
2
Z T
0
λ2
s ds
!#
<∞.
Lemma 7(Positivity and Normalization of the Kernel).Under Assumption 1,
the processM λ
t defined in(14)is a positiveP–martingale withM λ
0 = 1and
EP[Mλ
T | F0] = 1.
Consequently, definingQby
dQ
dP

FT
=M λ
T
yields a probability measure equivalent toP.
Proof.By Assumption 1, the stochastic exponentialM λ
t is a true martingale
(Novikov’s condition). It is strictly positive by definition and satisfiesM λ
0 = 1.
ThusE P[Mλ
T | F0] =M λ
0 = 1, andM λ
T defines a Radon–Nikodym derivative of
a probability measureQequivalent toP.
UnderQ, Girsanov’s theorem implies that
WS,Q
t =W S
t +
Z t
0
λs ds
is a Brownian motion. Substituting into (12) yields
dSt =S t
 
(µ−λ t
√vt)dt+ √vt dWS,Q
t

.
Definition 1(Drift Adjustment and No–Arbitrage).Letr t denote the short
rate. To ensure that the discounted price ˜St :=e −
R t
0 rsdsSt is aQ–martingale,
we chooseλ t such that
µ−λ t
√vt =r t,i.e.λ t = µ−r t
√vt
.
With this choice, theS t dynamics underQbecome
dSt =S t
 
rt dt+ √vt dWS,Q
t

,
ensuring no–arbitrage in the usual sense.
Esscher-Type Calibration to CAPOPM Posterior Moments.The spec-
ification in Definition 1 yields a one–parameter family of kernels indexed by the
physical driftµand the short rater t. To incorporate the information contained
45

in the CAPOPM posterior Π ϕ(· | D), we calibrateλt (or, equivalently, an Es-
scher tilt parameter) so that selected posterior moments match between the
structural Heston model and the CAPOPM event–probability posterior.
LetX T := logS T and define an Esscher–type kernel based onX T :
Mθ
T := exp(θXT )
EP[exp(θXT )| F0].
By construction,M θ
T >0 andE P[Mθ
T | F0] = 1. For a given Esscher parameter
θ, this defines a risk–neutral measureQ θ via dQθ
dP =M θ
T . We chooseθ(or a
small vector of tilting parameters) to match posterior–implied moments from
CAPOPM, e.g.
EQθ [ST | F0] =E Πϕ[ST | D],E Qθ [1{ST > K} | F0] =E Πϕ[1{ST > K} | D],
for one or more strikesK. This calibration step projects the raw CAPOPM pos-
terior information into the structurally consistent Heston kernel family without
violating positivity or the martingale property.
Theorem 12(Kernel Regularization and No–Arbitrage Preservation).LetΠ ϕ(· |
D)be the fully corrected CAPOPM posterior from Phases 5–6. Define a regu-
larized pricing kernel either as:
(a) an exponential martingaleM λ
t as in(14)withλ t chosen according to
Definition 1, or
(b) an Esscher–type densityM θ
T based onX T = logS T withθcalibrated to
match a set of CAPOPM posterior moments.
Assume Novikov’s condition (Assumption 1) holds for the chosenλ t or thatM θ
T
has finite exponential moments underP. Then:
(i)M T is strictly positive and satisfiesE P[MT | F0] = 1, so it defines a valid
pricing kernel.
(ii) The discounted price process ˜St =e −
R t
0 rsdsSt is aQ–martingale for the
induced risk–neutral measureQ.
(iii) Option prices computed as
C(K, T) =e−
R T
0 rsdsEQ

(ST −K) + | F0

are arbitrage–free within the Heston family and consistent with the CAPOPM
posterior in the sense that their moments agree with the posterior–implied
targets used in calibration.
Proof Sketch.(i) Positivity and normalization follow from Lemma 7 in the ex-
ponential martingale case and by construction in the Esscher case. (ii) The
drift adjustment in Definition 1 ensures thatS t has driftr t underQ, so ˜St
46

is a local martingale; integrability conditions (e.g. Novikov, uniform integra-
bility) promote it to a true martingale. (iii) Option prices underQinherit
no–arbitrage from the standard risk–neutral valuation framework. The Esscher
calibration conditions guarantee that chosen moments (e.g. ofS T or digital pay-
offs) match those implied by Π ϕ, thereby aligning the structural Heston kernel
with the CAPOPM posterior without violating the martingale and positivity
constraints.
Remark 10(Integration with Simulation and Robustness Phases).In Phase 7,
Monte Carlo simulations of(S t, vt)under the regularized kernelM λ orM θ can
be used to assess the stability of option prices and digital probabilities under
parameter uncertainty and posterior perturbations. In Phase 8, the robustness
results for the posterior (e.g. in Wasserstein or Hellinger distance) combined
with the exponential kernel representation yield explicit bounds on the sensi-
tivity of risk–neutral prices to data and model perturbations. Crucially, kernel
regularization is appliedafterthe event–probability posterior has been corrected
for nonlinear, dependent, and multimodal effects, so that enforcing no–arbitrage
does not undo the informational gains of CAPOPM but instead embeds them
into a structurally consistent, arbitrage–free pricing measure.
5.2 Posterior Predictive Mean and Variance
Lemma 8(Posterior Mean and Variance).For a Beta(α post, βpost)posterior,
ˆppost :=E[p|y] = αpost
αpost +β post
,
Var(p|y) = αpostβpost
(αpost +β post)2(αpost +β post + 1).
Proof.Standard Beta distribution identities.
5.3 Posterior Predictive Distribution for Digital Outcomes
LetZdenote the payoff of a YES contract:
Z=1{A}.
Proposition 6(Posterior Predictive Distribution of Digital Payoff).The posterior–
predictive distribution ofZis Bernoulli with mean
πpred =E[Z|y] = ˆp post = αpost
αpost +β post
.
Proof.SinceZ|p∼Bernoulli(p),
E[Z|y] =E[E[Z|p, y]|y] =E[p|y] = ˆp post.
47

5.4 Posterior Predictive Prices of Parimutuel YES/NO Con-
tracts
A YES contract pays 1 ifAoccurs and 0 otherwise. Under risk–neutral valua-
tion, its fair price is the posterior predictive mean.
Theorem 13(Arbitrage–Free YES/NO Pricing).The posterior–predictive fair
prices of YES and NO contracts are
πYES = ˆppost, π NO = 1−ˆppost.
They satisfy the no–arbitrage identity:
πYES +π NO = 1.
Proof.Follows immediately fromπ YES =E[Z|y] and 1−Z=1{A c}.
5.5 Properties of Posterior–Predictive Prices
Proposition 7(Monotonicity in YES Votes).The priceπ YES =α post/(αpost +
βpost)is strictly increasing in the county.
Proof.Sinceα post =α 0 +y,
∂
∂y
α0 +y
α0 +β 0 +n >0.
Proposition 8(Continuity).The price is continuous in bothα post andβ post
and therefore inyandn.
Proof.Rational function of continuous arguments.
5.6 Mixture Posterior Extension for Multimodal Beliefs
The baseline CAPOPM framework represents the posterior distribution of the
event probabilitypby a single Beta distribution. This is appropriate when the
likelihood is approximately unimodal and the crowd can be described by a single
effective subpopulation. Once nonlinear structural distortions (Phase 6) and
heterogeneous trader behavior are admitted, the true posterior often becomes
multimodal. In such settings, forcing a unimodal Beta posterior, or even a
single Beta built on a misspecified likelihood, can lead to severely miscalibrated
probabilities and distorted pricing.
To represent multimodality explicitly, we extend the CAPOPM posterior to a
finite mixture of Betas constructed via stacking of multiple candidate submodels
or strata (e.g. trader types, structural regimes, or segmentation by liquidity
conditions).
48

Assumption 2(Candidate Submodels and Stratified Posteriors).Let{M k :
k= 1, . . . , K}be a finite collection of candidate CAPOPM submodels. Each
Mk is defined by:
(i) a data subset or stratumD k (e.g. orders from a given trader type, struc-
tural regime, or filtered particle trajectory);
(ii) a Beta posterior onpof the form
p| Dk ∼Beta(α k, βk),
obtained from the usual Beta–Binomial update underM k;
(iii) a corresponding predictive densitym k(y)for new Bernoulli dataY∈
{0,1}, given by the Beta–Binomial predictive:
mk(y) =
Z 1
0
py(1−p) 1−y Beta(αk, βk)(dp) =



βk
αk +β k
ify= 0,
αk
αk +β k
ify= 1.
We interpret eachM k as capturing one coherent “mode” of the crowd’s beliefs
or structural environment.
Definition 2(Stacked Mixture Posterior overp).LetD hold ={Y hold
i :i=
1, . . . , nhold}be a holdout dataset (e.g. a subset of orders or markets not used to
fitα k, βk). Define stacking weightsw= (w 1, . . . , wK)in the probability simplex
∆K−1 :=
(
w∈[0,1] K :
KX
k=1
wk = 1
)
by minimizing the negative log predictive likelihood of the stacked model:
w∗ := arg min
w∈∆K−1
(
−
nholdX
i=1
log
 KX
k=1
wk mk
 
Y hold
i

!)
.
Thestacked mixture posteriorforpis then defined as
Πmix(dp) :=
KX
k=1
w∗
k Beta(αk, βk)(dp).
Proposition 9(Posterior Mean and Predictive under the Mixture).Under
Assumption 2 and Definition 2, the stacked mixture posteriorΠ mix is a finite
mixture of Beta distributions. Its mean and predictive distribution are given by:
(i)Posterior mean ofp:
ˆpmix :=E Πmix [p] =
KX
k=1
w∗
k
αk
αk +β k
.
49

(ii)Predictive distribution for a new Bernoulli outcomeY:
P(Y= 1) =
KX
k=1
w∗
k
αk
αk +β k
,P(Y= 0) =
KX
k=1
w∗
k
βk
αk +β k
.
In particular, the mixture meanˆp mix can be used as the crowd–adjusted event
probability for YES/NO contracts, and the full mixture distributionΠ mix can be
propagated into posterior predictive option prices as in the baseline CAPOPM
construction.
Proof.Since Π mix is a finite convex combination of Beta distributions, it is a
well–defined probability measure on [0,1]. The expression for ˆp mix follows by
linearity of expectation:
ˆpmix =
Z 1
0
pΠ mix(dp) =
KX
k=1
w∗
k
Z 1
0
pBeta(α k, βk)(dp) =
KX
k=1
w∗
k
αk
αk +β k
.
The predictive probabilities follow similarly by integratingp y(1−p) 1−y with
respect to Πmix and using the Beta–Binomial formulas.
The mixture posterior Π mix explicitly retains multimodality when the com-
ponent posteriors Beta(α k, βk) are well separated. In particular, if some strata
correspond to optimistic trader types and others to pessimistic types (or differ-
ent structural regimes), then Π mix can exhibit multiple modes. The stacking
weightsw ∗
k tilt the mixture toward components that perform better on the
holdout setD hold, reducing sensitivity to any single misspecified submodel.
Definition 3(Moment–Matched Single–Beta Approximation).For interpretabil-
ity and analytical convenience, one may define a moment–matched single–Beta
approximationBeta(˜α,˜β)to the mixture posterior by matching the first two mo-
ments:
E[p] = ˆpmix,Var[p] =
KX
k=1
w∗
k
αkβk
(αk +β k)2(αk +β k + 1)+
KX
k=1
w∗
k
 αk
αk +β k
−ˆpmix
2
.
The approximationBeta(˜α, ˜β)is then chosen such that
˜α
˜α+˜β
= ˆpmix, ˜α˜β
(˜α+˜β)2(˜α+˜β+ 1)
=Var Πmix [p].
We emphasize that this is anapproximation layerused for convenience, not an
exact representation ofΠ mix.
The next result formalizes a limitation: when the mixture posterior is suf-
ficiently multimodal, no single Beta distribution can uniformly approximate it.
This provides a theoretical warning against collapsing Π mix to a single Beta in
regimes of strong heterogeneity.
50

Theorem 14(No Unimodal Beta Can Uniformly Approximate a Strongly Mul-
timodal Mixture).LetΠ mix be a mixture of two Betas
Πmix(dp) = 1
2 Beta(α1, β1)(dp) + 1
2 Beta(α2, β2)(dp),
with meansµ 1 ̸=µ 2 and variancesσ 2
1, σ2
2 bounded. Assume that|µ 1 −µ 2| ≥ε
for someε >0, and that each component is sharply concentrated around its
mean (e.g.α k +β k large). Then there exists a constantc(ε)>0such that for
any single Beta distributionBeta(˜α, ˜β),
Πmix −Beta(˜α,˜β)

TV
≥c(ε),
where∥ · ∥TV denotes total variation distance. In particular, no single Beta can
approximateΠ mix arbitrarily well as the component means separate.
Proof.Since each Beta component is sharply concentrated around its mean, for
anyδ >0 small enough there exist disjoint intervalsI 1, I2 ⊂[0,1] such that
PBeta(α1,β1)(I1)≥1−δ,P Beta(α2,β2)(I2)≥1−δ, I 1 ∩I 2 =∅,
andI 1 andI 2 are separated by at leastε/2. Then
Πmix(I1)≥ 1
2 (1−δ),Π mix(I2)≥ 1
2 (1−δ).
Any single Beta Beta(˜α, ˜β) has unimodal density on (0,1) and cannot assign
mass arbitrarily close to 1
2 (1−δ) to both disjoint, well–separated intervalsI 1
andI 2. Consequently there existsc(ε, δ)>0 such that
sup
A⊂[0,1]
Πmix(A)−Beta(˜α, ˜β)(A)
 ≥c(ε, δ),
for all choices of (˜α, ˜β). Takingδsmall and absorbing it intoc(ε) yields the
claim.
Remark 11(Implications for CAPOPM).Theorem 14 shows that when the
crowd beliefs are strongly multimodal (e.g. two well–separated trader camps or
regimes), any attempt to compress the posterior into a single Beta inevitably
loses structural information and cannot be uniformly well–calibrated. In such
regimes CAPOPM should operate directly with the mixture posteriorΠ mix (or
its predictive functionals), and treat any moment–matched single–Beta repre-
sentation as an approximation with explicit, non–vanishing divergence from the
true posterior. In Phase 8, we extend the divergence and robustness results to
incorporate mixture posteriors, providing bounds on the loss incurred by such
approximations.
51

5.7 Credible Intervals and Price Uncertainty Bands
From the posterior variance,
σ2
post := Var(p|y),
we obtain a symmetric credible interval
πYES ±z γ σpost,
wherez γ is the standard normal quantile. Exact Beta quantiles may also be
used: 
BetaInv
 γ
2 ;α post, βpost

,BetaInv
 
1− γ
2 ;α post, βpost

.
5.8 Risk–Adjusted Prices
Agents may wish to incorporate uncertainty into the price. Define the risk–
adjusted YES price
πrisk,+ = ˆppost +c σpost, c≥0,
and the corresponding conservative price
πrisk,− = ˆppost −c σpost.
Lemma 9(Risk Monotonicity).Risk–adjusted prices increase with uncertainty:
∂πrisk,+
∂σpost
=c >0.
Proof.Immediate from the definition.
5.9 Strategic Distortion Considerations
As noted in Phase 4, traders may strategically exaggerate their positions. Thus
(y, n−y) may reflect strategic behavior in addition to private signals. The
posterior derived above therefore represents theunadjustedprediction based
solely on the observed order flow.
Phase 6 introduces:
•liquidity–adjusted countsy ∗, n∗,
•distortion offsetsδ +, δ−,
•and the bias–corrected posterior Beta(α adj, βadj).
5.10 Output of Phase 5
The output of this phase consists of:
(αpost, βpost), π YES = ˆppost, π NO = 1−ˆppost,
together with credible intervals and risk–adjusted variants. These serve as in-
puts to Phase 6, where distortions and liquidity effects are formally corrected.
52

Phase 6. Bias Correction and Robustness Layer
Phases 4 and 5 produce a posterior distribution
p|y∼Beta(α post, βpost)
and posterior–predictive prices
πYES = αpost
αpost +β post
, π NO = 1−π YES.
However, the observed parimutuel order flow (y, n−y) may be distorted by
behavioral biases and structural market effects, including:
•Long–shot bias:overbetting low–probability outcomes,
•Herd behavior:traders imitating late order flow.
In this phase, we construct a two–stage correction layer:
1. Stage 1: behavioral bias correction(long–shot, herding),
2. Stage 2: structural/liquidity distortion correction via offset parameters
(δ+, δ−).
The goal is a bias–corrected posterior
p|s adj ∼Beta(α adj, βadj)
with associated robust prices, while preserving Beta–Binomial conjugacy and
no–arbitrage.
6.1 From Raw Counts to Behavioral and Structural Distor-
tions
Lets i ∈ {YES,NO}denote traderi’s action, and recall:
y=
nX
i=1
1{s i = YES}, n−y=
nX
i=1
1{s i = NO}.
We conceptually distinguish:
•Behavioral distortions, driven by perception and psychology (long–shot
bias, herd behavior);
•Structural distortions, driven by market mechanics (liquidity imbalances,
whale trades, microstructure noise).
Although both arise from complex microfoundations, we implement bias
correction via deterministic weighting of individual orders and scalar offsets.
Stochastic liquidity processes (e.g. Poisson arrivals) are acknowledged concep-
tually but not explicitly modeled here.
53

6.2 Stage 1: Behavioral Bias Correction (Long–Shot Bias
and Herd Behavior)
Long–shot bias.Empirical and experimental work on parimutuel markets
documents a tendency for traders to overbet low–probability events (long–shot
bias). In a binary setting, this manifests as disproportionate YES volume when
the truepis small.
Herd behavior.Traders may alsoherdon late–arriving order flow: observing
a run of recent YES bets, they may overweight YES regardless of their private
signals.
We encode behavioral distortions through weightsw beh
i ∈(0,∞) applied to
each trader action:
y(1) :=
nX
i=1
wbeh
i 1{s i = YES}, n (1) −y (1) :=
nX
i=1
wbeh
i 1{s i = NO}.(15)
Example (qualitative).
•To mitigate long–shot bias, YES trades on extreme low–probability strikes
may receivew beh
i <1.
•To mitigate herding, late trades that follow a long run of identical orders
may receivew beh
i <1, while early trades receivew beh
i ≈1.
Lemma 10(Behaviorally Adjusted Counts).If allw beh
i >0, then
y(1) >0, n (1) −y (1) >0 =⇒0< y (1) < n(1).
Proof.Positivity and finiteness follow from finiteness ofnand positivity of
weights.
6.3 Stage 2: Nonlinear Structural Distortions via Regime
Mixtures
Stage 1 produces a behaviorally corrected Beta posterior
p| D1 ∼Beta(α 1, β1),
whereD 1 denotes the effective (possibly weighted) order flow after behavioral
adjustments (herding, long–shot bias, etc.) have been accounted for. In the orig-
inal linear specification of Stage 2, structural distortions such as liquidity imbal-
ances or whale dominance were represented by constant additive offsets (δ+, δ−)
to the pseudo–counts. This amounts to replacing (α 1, β1) by (α1 +δ +, β1 +δ −)
and preserves single–Beta conjugacy. However, this specification implicitly as-
sumes that all distortions areadditivein pseudo–counts and therefore fails to
represent multiplicative or exponential distortions (e.g. nonlinear amplification
54

of long–shot bias under herding). In such cases, the adjusted posterior is sys-
tematically misspecified and the robustness guarantees of Phase 8 can fail.
To accommodate nonlinear distortions while preserving an analytically tractable
posterior, we introduce a finite collection ofstructural distortion regimesand
represent Stage 2 as a mixture over regime–specific pseudo–count corrections.
Assumption 3(Structural Distortion Regimes).LetR∈Nbe finite. For each
r∈ {1, . . . , R}, there is astructural distortion regimecharacterized by:
1. a prior weightπ r >0with PR
r=1 πr = 1;
2. measurable pseudo–count corrections
g+
r :S →R, g −
r :S →R,
whereSdenotes the Stage 1 summary statistics (e.g. total effective YES
county 1, NO countn 1 −y1, order–book imbalance, realized spread, volume
concentration, etc.);
3. a boundedness condition
sup
s∈S
max

|g+
r (s)|,|g −
r (s)|
	
≤G r <∞;
4. an admissibility condition ensuring that, for alls∈ S,
αr(s) :=α 1 +g +
r (s)>0, β r(s) :=β 1 +g −
r (s)>0.
We interpretras a latent structural state (e.g. “balanced liquidity”, “whale–dominated”,
“thin–book”), and the functionsg ±
r may be nonlinear in the Stage 1 statistics
s∈ S.
Definition 4(Stage 2 Nonlinear Structural Adjustment).LetD 2 denote the full
data entering Stage 2, including the Stage 1 summarys∈ Sand any structural
covariates (e.g. book depth, cross–venue imbalance). Under Assumption 3, the
Stage 2 adjustment proceeds as follows:
1. Draw a latent regimeR ∗ ∈ {1, . . . , R}withP(R∗ =r) =π r.
2. GivenR ∗ =randD 2, replace the Stage 1 parameters(α 1, β1)by
αr(s) =α 1 +g +
r (s), β r(s) =β 1 +g −
r (s),
and define the regime–conditional Stage 2 posterior
p|(D 2, R∗ =r)∼Beta
 
αr(s), βr(s)

.
The unconditional Stage 2 posterior is obtained by marginalizing overR ∗:
ΠCAPOPM(dp| D2) =
RX
r=1
ωr(D2) Beta
 
αr(s), βr(s)

dp,
where the regime weightsω r(D2)are the posterior probabilitiesP(R ∗ =r| D2).
55

The next result shows that, under mild conditions, this Stage 2 specification
yields a finite mixture of Beta posteriors and therefore provides a tractable
representation of nonlinear structural distortions.
Theorem 15(Mixture–of–Beta Conjugacy under Nonlinear Structural Correc-
tions).Suppose that the Stage 1 posterior satisfies
p| D1 ∼Beta(α 1, β1),
and that Assumption 3 holds. LetD 2 be anyσ–algebra generating the Stage 2
summarys∈ Sand any additional structural covariates used to evaluateg ±
r .
Then:
1. For each fixed regimer∈ {1, . . . , R}and realizations∈ Swithαr(s)>0,
βr(s)>0, the regime–conditional Stage 2 posterior
p|(D 2, R∗ =r)∼Beta
 
αr(s), βr(s)

is a well–defined Beta distribution.
2. The unconditional Stage 2 posteriorΠ CAPOPM(· | D2)is a finite mixture
of Beta distributions:
ΠCAPOPM(dp| D2) =
RX
r=1
ωr(D2) Beta
 
αr(s), βr(s)

dp,
with regime weights
ωr(D2) = πr Lr(D2)PR
k=1 πk Lk(D2)
,
whereL r(D2)is the marginal likelihood ofD 2 under regimer.
3. In particular, the Stage 2 posterior mean can be written as
E

p| D2

=
RX
r=1
ωr(D2) αr(s)
αr(s) +β r(s).
Proof.(i) For eachrands∈ S, the admissibility condition in Assumption 3(iv)
guaranteesα r(s)>0 andβ r(s)>0. Hence Beta(α r(s), βr(s)) is a proper Beta
distribution.
(ii) By construction, the latent regimeR ∗ has prior distributionP(R ∗ =
r) =π r. Conditional onR ∗ =randD 2, the posterior ofpis Beta(α r(s), βr(s)).
Applying the law of total probability yields
ΠCAPOPM(A| D2) =
RX
r=1
P(R∗ =r| D2)P
 
p∈A| D2, R∗ =r

,
56

for any Borel setA⊂[0,1]. Identifyingω r(D2) :=P(R ∗ =r| D2) andP(p∈
A| D2, R∗ =r) = Beta(α r(s), βr(s))(A) establishes the mixture representation.
The explicit expression forω r(D2) follows from Bayes’ rule:
ωr(D2) = πr Lr(D2)PR
k=1 πk Lk(D2)
,
whereL r(D2) is the marginal likelihood under regimer.
(iii) The expression for the posterior mean is obtained by integratingp
against the mixture:
E[p| D2] =
Z 1
0
pΠ CAPOPM(dp| D2) =
RX
r=1
ωr(D2)
Z 1
0
pBeta(α r(s), βr(s))(dp),
and the Beta mean formula yields
R1
0 pBeta(α r(s), βr(s))(dp) =α r(s)/(αr(s) +
βr(s)).
Remark 12(Linear Offsets as a Special Case).The original linear offset model
is recovered by takingR= 1andg +
1 (s)≡δ +,g −
1 (s)≡δ − constant ins. In that
case,ω 1(D2)≡1andΠ CAPOPM(· | D2)reduces to a singleBeta(α 1+δ+, β1+δ−)
posterior.
Remark 13(Representation of Nonlinear Distortions).Assumption 3 allows
the correctionsg ±
r (s)to be nonlinear functions of the Stage 1 summary statis-
tics. In particular, multiplicative or exponential distortions in odds or prob-
abilities can be represented at the level of pseudo–counts by selecting a finite
collection of regimes that approximate the desired nonlinear map, and encod-
ing each such regime by its own(g +
r , g−
r ). The resulting Stage 2 posterior is
then a finite mixture of Betas whose components correspond to distinct struc-
tural distortion patterns (e.g. “whale–dominated long–shot amplification” versus
“balanced liquidity”). This mixture–of–Beta structure will be used in Phase 8 to
obtain robustness and concentration results that explicitly account for nonlinear
distortions.
6.3.1 Stage 2(Special Case): Structural and Liquidity Distortion Cor-
rection
Beyond behavioral biases, structural features of the market can distort order
flow:
•Whale dominance:a small number of large traders dominate volume,
•Liquidity imbalances:thin order books amplify individual trades,
•Microstructure asymmetries:fee structures, tick sizes, etc.
57

We summarize these effects via scalar offsetsδ+, δ− ∈R, reflecting net struc-
tural pressure on YES and NO sides, respectively.
Starting from behaviorally adjusted countsy (1), n(1), we define structurally
adjusted pseudo–counts:
y∗ :=y (1) +δ +, n ∗ −y ∗ := (n(1) −y (1)) +δ −.(16)
Interpretation.
•A positiveδ + increases effective YES support, e.g. if structural frictions
suppressed YES participation.
•A negativeδ + decreases effective YES support, e.g. if whale trades are
suspected of artificially inflating YES volume.
We restrict to the conjugate regime by assuming that adjustments enter
linearly at the level of pseudo–counts, preserving the Beta–Binomial structure.
Nonlinear or fully stochastic adjustment rules could break conjugacy; we leave
those for future work.
6.4 Bias–Corrected Posterior
Recall the hybrid prior parameters (α 0, β0) from Phase 2 and the unadjusted
posterior parameters from Phase 4,α post, βpost. The bias–corrected posterior is
defined as:
αadj =α 0 +y ∗, β adj =β 0 + (n∗ −y ∗),(17)
and
p|s adj ∼Beta(α adj, βadj).(18)
Interpretation and Justification of Linear Offsets.The structural off-
setsδ + andδ − in (16) are introduced as a first-order correction for systematic
distortions in order flow, such as liquidity imbalances, whale dominance, or me-
chanical features of the parimutuel pool. The choice of linear offsets preserves
the affine form of the Beta parameters:
αadj =α 0 +y (1) +δ +, β adj =β 0 + (n(1) −y (1)) +δ −,
and therefore maintains exact Beta–Binomial conjugacy. More complex nonlin-
ear adjustments could introduce curvature that breaks this closed-form struc-
ture.
Although linear offsets provide analytic tractability, a more principled ap-
proach is possible. In practice, one could view (δ +, δ−) as hyperparameters
calibrated across panels of markets. LetD={(y m, nm, Zm)}M
m=1 denote his-
torical markets, whereZ m ∈ {0,1}is the realized outcome. An empirical Bayes
estimator of (δ+, δ−) could maximize the marginal likelihood
(ˆδ+, ˆδ−) = arg max
δ+,δ−
MY
m=1
Z 1
0
Beta(p;α 0+ym+δ+, β0+(nm−ym)+δ−)p Zm(1−p)1−Zm dp,
58

or alternatively match the empirical long-shot bias or herding bias by aligning
observed market miscalibration with the expected posterior mean under (δ+, δ−)
via a method-of-moments criterion.
This formulation highlights that the linear offsets of (16) are not merely
ad hoc additions but constitute a conjugacy-preserving approximation to more
general structural distortions whose systematic components may be estimated
directly from cross-market data.
Corollary 1(Effect of Offset Uncertainty on Posterior Variance).Letˆp n be the
CAPOPM posterior mean based on adjusted counts(y ∗
n, n∗
n)and fixed(δ +, δ−),
and let˜pn denote the posterior mean when(δ +, δ−)are themselves random with
finite variancesVar(δ +)andVar(δ −). Under the Lipschitz conditions of the
error-propagation theorem and a first-order delta-method approximation,
Var(˜pn)≈Var(ˆpn |δ +, δ−) +
∂ˆpn
∂δ+
2
Var(δ+) +
∂ˆpn
∂δ−
2
Var(δ−),
where the derivatives are evaluated at the posterior mode of(δ+, δ−)or their Em-
pirical Bayes estimates. In particular, uncertainty in the calibration of offsets
inflates the posterior variance forpby a term that is quadratic in the sensitivity
ofˆpn to(δ +, δ−)and linear in their variances.
Lemma 11(Properness of Adjusted Posterior).Ifα 0, β0 >0andy ∗ >−α 0,
n∗ −y ∗ >−β 0, thenα adj, βadj >0and the Beta posterior is proper.
Proof.Immediate from (17).
Proposition 10(Robustness of Posterior Mean, Variance, and Distribution).
Letp|s adj ∼Beta(α adj, βadj)denote the adjusted posterior of Phase 6, where
αadj =α 0 +y ∗, β adj =β 0 + (n∗ −y ∗),
andy ∗, n∗ are the behaviorally and structurally adjusted pseudo-counts.
Define perturbations
∆α= ∆y ∗,∆β= ∆(n ∗ −y ∗).
Assume thatα adj andβ adj lie in a compact subset of(0,∞)and that(∆α,∆β)
are sufficiently small. Then:
(i) (Mean Robustness) The posterior mean satisfies the Lipschitz bound

αadj
αadj +β adj
− αadj + ∆α
αadj +β adj + ∆α+ ∆β
 ≤L 1 (|∆α|+|∆β|)
for someL 1 >0.
(ii) (Variance Robustness) The posterior variance satisfies
Varadj(p)−Var ′
adj(p)
 ≤L 2 (|∆α|+|∆β|),
whereVar adj denotes the variance under(α adj, βadj)andVar ′
adj denotes
the variance under the perturbed parameters.
59

(iii) (Distributional Robustness via Hellinger Distance) LetBeta(α, β)
andBeta(α ′, β′)denote the original and perturbed posteriors. Then the
squared Hellinger distance satisfies
H2(Beta(α, β),Beta(α′, β′))≤L 3 (|∆α|+|∆β|),
for some constantL 3 >0depending only on the compact parameter set.
Proof.We prove each part separately.
(i) Mean Robustness.The posterior meanµ(α, β) =α/(α+β) is smooth
on any compact set that avoids the boundary of (0,∞) 2. Using a first-order
Taylor expansion:
µ(α+ ∆α, β+ ∆β) =µ(α, β) +∇µ(α, β)·(∆α,∆β) +O
 
∥(∆α,∆β)∥ 2
.
Because the gradient satisfies
∥∇µ(α, β)∥=

 β
(α+β) 2 ,− α
(α+β) 2
 ≤ 1
4m2
on any compact set withα, β≥m >0, the result follows withL 1 = 1/(4m2).
(ii) Variance Robustness.The Beta variance is
V(α, β) = αβ
(α+β) 2(α+β+ 1) .
This is smooth on any compact domain bounded away from the axes. By
the mean value theorem,
|V(α+ ∆α, β+ ∆β)−V(α, β)| ≤sup
(˜α,˜β)∈K
∇V(˜α,˜β)
 ·(|∆α|+|∆β|),
withKthe compact region under consideration. Let the supremum beL 2; then
the result holds.
(iii) Hellinger Distance Robustness.For two densitiesf, gon [0,1],
H2(f, g) = 1−
Z 1
0
p
f(p)g(p)dp.
The Beta density isf(p)∝p α−1(1−p) β−1. On compact subsets of (0,∞) 2,
the mapping
(α, β)7→fα,β(p)
is Lipschitz in (α, β) uniformly inp∈(0,1) because logf α,β(p) is affine in (α, β)
and bounded on compact sets.
60

Thus, 
q
fα,β(p)−
q
fα′,β′(p)
 ≤C(|∆α|+|∆β|)
for eachp. Integrating overp∈[0,1] yields
H2(Beta(α, β),Beta(α′, β′))≤L 3(|∆α|+|∆β|),
withL 3 depending only on the compact domain.
6.5 Robustness to Small Distortions
We now show that the adjusted posterior is stable under small perturbations in
the behavioral and structural corrections.
Let
∆y∗ =y ∗ −y,∆(n ∗ −y ∗) = (n∗ −y ∗)−(n−y).
Theorem 16(Lipschitz Robustness of Adjusted Price).Consider the adjusted
YES price
πadj
YES = ˆpadj = αadj
αadj +β adj
.
Assume thatα 0, β0 >0and that the total pseudo–countα adj +β adj is bounded
away from zero. Then small changes iny ∗ andn ∗ −y ∗ produce small changes
inπ adj
YES; in particular, there existsL >0such that
πadj
YES(y∗, n∗)−π adj
YES(y, n)
 ≤L(|∆y ∗|+|∆(n ∗ −y ∗)|).
Proof.π adj
YES is a smooth (rational) function of (α adj, βadj), which are affine in
y∗ andn ∗ −y ∗. On any compact set whereα adj +β adj > c >0, the gradient is
bounded, yielding the Lipschitz bound.
Interpretation.If behavioral and structural corrections are small in magni-
tude, then the CAPOPM price changes smoothly and does not exhibit explosive
sensitivity to local adjustments.
6.6 Arbitrage–Free Pricing after Bias Correction
Define the bias–corrected YES/NO prices:
πadj
YES = ˆpadj, π adj
NO = 1−ˆpadj.
Proposition 11(No–Arbitrage Identity Preserved).For any admissible(y ∗, n∗)
and offsets(δ +, δ−),
πadj
YES +π adj
NO = 1.
Proof.By definitionπ adj
NO = 1−ˆpadj.
Thus the bias–correction layer modifies the posterior but preserves the fun-
damental no–arbitrage structure of the YES/NO market.
61

6.7 Qualitative Examples of the Two–Stage Correction
Example 1: Long–shot bias.Suppose that at a deep out–of–the–money
strike with small structural priorq str, a surprisingly large number of YES bets
arrives. If experimental evidence indicates long–shot bias at such strikes, we
may choosew beh
i <1 for these YES trades, reducingy (1) relative to the raw
county.
Example 2: Herd behavior.If the last fraction of the trading window
is dominated by YES volume without corresponding structural news, we may
downweight late trades via smallerw beh
i for those timestamps, thereby mitigat-
ing herding.
Example 3: Whale dominance.If a few traders submit extremely large
YES positions, we can encode the suspicion of manipulation via a negativeδ +,
reducingy ∗ even after behavioral correction; analogously, suppressed liquidity
may justify a positiveδ +.
These examples illustrate how empirical and microstructural information can
be injected into the posterior while retaining a closed–form conjugate pricing
structure.
6.8 Sequential Updating and Time Dynamics
Up to this point, CAPOPM has been presented in a static form, with all trades
aggregated into adjusted counts (y∗, n∗ −y ∗) over the trading window. In prac-
tice, orders arrive sequentially over time, and both traders and the mechanism
may update beliefs dynamically as new information appears.
We now sketch how the adjusted posterior can be updated in real time and
how this relates to the fractional structure of the underlying Heston model.
Discrete-Time Posterior Updates.Let trades arrive at timest= 1,2, . . . , T,
and writes t ∈ {YES,NO}for thet-th action. Define the time-tadjusted counts
y∗
t =
tX
i=1
wbeh
i 1{s i = YES}+δ +,t, n ∗
t −y ∗
t =
tX
i=1
wbeh
i 1{s i = NO}+δ −,t,
whereδ +,t, δ−,t allow for time-varying structural offsets (e.g. evolving liquidity
conditions). The corresponding time-tposterior is
p| Ft ∼Beta(α t, βt), α t =α 0 +y ∗
t , β t =β 0 + (n∗
t −y ∗
t ),
whereF t is the sigma-field generated by trades up to timetand the chosen
correction rules.
When weights and offsets are updated deterministically based on past infor-
mation, the posterior at timet+ 1 may be written as
αt+1 =α t + ∆y∗
t+1, β t+1 =β t + ∆(n∗
t+1 −y ∗
t+1),
62

where ∆y∗
t+1 and ∆(n∗
t+1 −y ∗
t+1) capture the contribution of the (t+1)-st trade
after behavioral and structural adjustments. This provides a filter-like evolution
of (αt, βt) across the trading horizon.
Proposition 12(Martingale Property of the Posterior Mean (Idealized Case)).
Suppose behavioral weights and structural offsets are constant in time, i.e.w beh
i ≡
1andδ +,t =δ −,t = 0, and that trader actions(s t)are conditionally independent
Bernoulli draws givenp true. Then the posterior mean
mt :=E[p| Ft] = αt
αt +β t
satisfies
E[mt+1 | Ft] =m t,
i.e.(m t)is a martingale with respect to(F t).
Proof.Under the stated assumptions, the standard Beta–Binomial update ap-
plies:
αt+1 =α t +1{s t+1 = YES}, β t+1 =β t +1{s t+1 = NO}.
Conditional onF t, we have
P(st+1 = YES| Ft) =p true,P(s t+1 = NO| Ft) = 1−p true.
A direct computation ofE[m t+1 | Ft] using these transition probabilities shows
that it equalsm t, a standard property of conjugate Beta–Binomial updating in
the absence of additional adjustments.
In the full CAPOPM setting, behavioral weightsw beh
i and offsetsδ ±,t can
depend on time and on past order flow, breaking the exact martingale structure.
However, this idealized case illustrates that the posterior mean naturally inherits
a martingale-like behavior under pure conjugate updating, and that CAPOPM’s
corrections can be viewed as systematically tilting this baseline dynamic to
account for biases and structural distortions.
Connection to Fractional Dynamics.The tempered fractional Heston model
of Phase 1 introduces memory in the variance dynamics via a Volterra kernel
Kα,λ. A similar idea can be applied at the informational level by defining time-
decayed behavioral weights of the form
wbeh
i (t)∝K ˜α,˜λ(t−t i) = exp
 
−˜λ(t−t i)

(t−t i)˜α−1,
so that older trades have a fractional-decay influence on the current posterior,
analogous to how past variance shocks influence current volatility. This parallel
suggests a unified way to model both price dynamics and information dynamics
within a common kernel-based framework, and provides a natural direction for
future extensions of CAPOPM.
63

6.9 Estimation of Behavioral Weights and Structural Off-
sets
The behavioral weightsw beh
i and structural offsetsδ +, δ− play a central role in
the CAPOPM adjustment layer. To use them in practice, we must specify how
they are estimated from data.
We model the behavioral weights as a parametric function
wbeh
i =w(x i;ψ),
wherex i is a vector of observable features for tradei(e.g. time, order size,
trader cohort, or market conditions), andψis a parameter vector belonging
to a compact set Ψ⊂R d. The structural offsets (δ +, δ−) are collected into a
parameterδ= (δ +, δ−)∈∆, where ∆ is also assumed compact.
Given a historical panel of markets indexed bym= 1, . . . , M, we observe
for each market:
Dm =

(s(m)
i , x(m)
i ) :i= 1, . . . , nm
	
,
wheres (m)
i ∈ {YES,NO}is the action andx (m)
i the associated features. For
each market, we compute adjusted counts via
y∗,(m)(ψ, δ) =
nmX
i=1
w
 
x(m)
i ;ψ

1{s (m)
i = YES}+δ +,
n∗,(m)(ψ, δ) =
nmX
i=1
w
 
x(m)
i ;ψ

+δ + +δ −,
which feed into the Beta–Binomial updating step.
Under a given prior Beta(α 0, β0) and a true event probabilityp (m)
true, the
adjusted counts in marketmare modeled as
Y ∗,(m)(ψ, δ)|p(m) ∼Binomial
 
n∗,(m)(ψ, δ), p(m)
, p (m) ∼Beta(α 0, β0),
so that the marginal (Empirical Bayes) likelihood for (ψ, δ) factorizes as
LM (ψ, δ) =
MY
m=1
Z 1
0
n∗,(m)(ψ, δ)
y∗,(m)(ψ, δ)

[p(m)]y∗,(m)(ψ,δ)[1−p(m)]n∗,(m)(ψ,δ)−y∗,(m)(ψ,δ)π0(p(m))dp (m),
whereπ 0 is the Beta prior density. The integral has the closed form
LM (ψ, δ) =
MY
m=1
B
 
α0 +y ∗,(m)(ψ, δ), β0 +n ∗,(m)(ψ, δ)−y∗,(m)(ψ, δ)

B(α0, β0) ,
with B the Beta function.
We define Empirical Bayes estimates ( ˆψM , ˆδM ) as
( ˆψM , ˆδM )∈arg max
(ψ,δ)∈Ψ×∆
LM (ψ, δ),
64

or equivalently, as maximizers of the log-likelihood
ℓM (ψ, δ) =
MX
m=1
log B
 
α0 +y ∗,(m)(ψ, δ), β0 +n ∗,(m)(ψ, δ)−y∗,(m)(ψ, δ)

.
Under standard regularity conditions (compact parameter space, continuity
and identifiability), these Empirical Bayes estimators converge to a pseudo-true
value (ψ†, δ†) that best fits the historical markets in the Beta–Binomial sense.
In Phase 8, we quantify how deviations ( ˆψM , ˆδM )−(ψ †, δ†) propagate into the
CAPOPM posterior.
6.10 Calibration of Mixture Components for Multimodal
Beliefs
The mixture posterior introduced in Phase 5 (Definition 2) requires a consistent
procedure for calibrating the component parameters (α k, βk), specifying the
partition{D k}, and estimating stacking weightsw ∗. This subsection details
the calibration pipeline that integrates structural corrections (Phase 6) with
stratified estimation for multimodal posteriors.
1. Stratification and Data Partitioning.LetDdenote the full dataset
entering Stage 2 after behavioral corrections. We partitionDintoKstrata
D1, . . . ,DK,
where the partition may be defined by:
•trader type (e.g. informed, liquidity, noise traders),
•structural regimes determined by order–book metrics or the nonlinear
functionsg ±
r of Phase 6,
•particle filter trajectories in a sequential model forp t,
•or cross–validated submodel specifications.
The strata are permitted to overlap or be soft–assigned if particle filters or
responsibility weights are used.
2. Component Posterior Estimation.For each stratumD k, we apply the
Stage 1 and Stage 2 adjustments restricted to that stratum. This yields a Beta
posterior
p| Dk ∼Beta(α k, βk)
where the pseudo–counts (αk, βk) incorporate:
•behavioral weights from Stage 1,
•regime–specific nonlinear structural offsets viag ±
r from Assumption 3,
•additional stratum–specific transformations (e.g. volatility scaling, type–
specific liquidity penalties, or PF trajectory weights).
65

3. Holdout–Based Stacking for Mixture Weights.LetD hold be a dis-
joint holdout sample. The stacking weightsw ∗
k are chosen to minimize the
negative log predictive likelihood:
w∗ = arg min
w∈∆K−1


−
X
Y∈D hold
log
 KX
k=1
wk mk(Y)
!


wherem k is the predictive density associated with Beta(α k, βk).
4. Final Mixture Posterior for Pricing.The calibrated mixture posterior
is
Πmix(dp) =
KX
k=1
w∗
k Beta(αk, βk)(dp),
with mixture mean
ˆpmix =
KX
k=1
w∗
k
αk
αk +β k
.
This mixture posterior is propagated through the option–pricing map in Phase 5,
preserving multimodality and avoiding the distortions caused by unimodal pro-
jections. Calibration of (α k, βk) ensures that each component captures one
coherent belief mode, while stacking weights adaptively balance them using
out–of–sample evidence.
6.11 Dynamic Regime–Switching Bias Corrections for Non-
stationary Environments
The empirical Bayes calibration used in the baseline CAPOPM framework as-
sumed a stationary historical environment: bias parameters (δ +, δ−) and be-
havioral distortionsψwere treated as fixed across a block of marketsM. Under
regime shifts (e.g. volatility spikes, crashes, or structural liquidity changes), this
stationarity assumption fails, leading to invalid ˆδand non–convergent posteri-
ors. This subsection introduces a dynamic, regime–switching specification for
(δt, ψt) that adapts to nonstationary environments while remaining compatible
with the nonlinear and multimodal posterior structure of Phases 5–6.
Hidden Markov Regimes for Structural Distortions.Let (S t)t≥1 be a
hidden Markov chain with finite state spaceS={1, . . . , R}, transition matrix
P= (P rs)R
r,s=1, and stationary distributionπ. Each regimer∈ Sdescribes a
structural environment, such as:
•low vs. high volatility,
•balanced vs. whale–dominated order flow,
•thin vs. deep order books,
66

•stable vs. stressed liquidity.
We associate to each regimera regime–specific bias parameterδ r = (δ+,r, δ−,r)
and a behavioral distortion parameterψ r (e.g. encoding long–shot amplification
or participation asymmetry).
Assumption 4(Regime–Switching Dynamics for (δ t, ψt)).For each time step
t= 1,2, . . ., the bias and distortion parameters(δ t, ψt)evolve according to the
hidden Markov chain(S t):
(i)S 1 ∼π,S t+1 |S t ∼P(S t,·);
(ii)(δ t, ψt) = (δSt, ψSt);
(iii) conditioned on(S t), order flowD t has emission probabilityp(D t |S t).
Switching Beta Prior for Time–Varying Bias.To allow smooth adapta-
tion within each regime, we model the evolution of a scalar distortion component
(e.g.δ +) as a regime–specific Beta transition. For simplicity, consider a generic
distortion coordinated t (e.g.δ +,t) with
dt |(d t−1, St =r)∼Beta
 
α(d)
r +cd t−1, β(d)
r +c(1−d t−1)

,
for some concentration constantc >0 and regime–specific hyperparameters
(α(d)
r , β(d)
r ). This defines aswitching Beta priorford t, which pullsd t toward
both the previous valued t−1 and the regime–specific baseline (α (d)
r , β(d)
r ).
Definition 5(Dynamic Bias State and Emissions).LetX t = (St, dt)be the
joint hidden state at timet. GivenX t = (S t, dt), the effective order flow at
timet(e.g. a weighted YES countZ t or aggregated summary statisticsD t) has
likelihood
p(Dt |X t) =p(D t |S t, dt),
obtained by applying the Stage 1 and Stage 2 corrections with distortion level
dt and regime–specific nonlinear offsetsg ±
St from Assumption 3. The overall
dynamic model is then a hidden Markov model (HMM) for(X t)with emissions
Dt.
Online Bayesian Updating via the Forward Algorithm.Given obser-
vationsD 1:t = (D1, . . . ,Dt), the filtering distribution over regimes is updated
using the forward recursion
γt(r) :=P(S t =r| D1:t)∝
" RX
s=1
γt−1(s)Psr
#
p(Dt |S t =r),
with normalization PR
r=1 γt(r) = 1. Conditional onS t =r, the distribution
ofd t can be updated via the switching Beta transition and the local emission
likelihoodp(D t |S t =r, dt). In practice, we work with a finite set of representa-
tive distortion values or particles{d (j)
t }and reweight them using the emission
likelihood, yielding a particle approximation top(d t |S t =r,D 1:t).
67

Definition 6(Dynamic Bias Estimates for Stage 2).At timet, the CAPOPM
Stage 2 correction uses the filtered expectation
ˆdt :=E[d t | D1:t] =
RX
r=1
γt(r)E[dt |S t =r,D 1:t],
and similarly for each coordinate ofδ t and forψ t. The corresponding pseudo–
count corrections and behavioral distortion parameters enter the nonlinear regime
mixture of Phase 6 via
αdyn
r (t) =α 1 +g +
r
 
st, ˆdt

, β dyn
r (t) =β 1 +g −
r
 
st, ˆdt

,
wheres t denotes the Stage 1 summary at timetandg ±
r are the regime–specific
structural corrections from Assumption 3. The resulting time–indexed compo-
nent posteriorsBeta(α dyn
r (t), βdyn
r (t))feed into the mixture posterior of Phase 5
with time–varying parameters.
Rolling–Window HMM Calibration.To avoid assuming global stationar-
ity of the transition matrixPand emission parameters, we estimate the HMM
on rolling windows of historical markets of sizeM:
Hℓ ={D t :t∈[t ℓ, tℓ +M−1]}.
For each windowH ℓ, we fit (P (ℓ),{δ (ℓ)
r , ψ(ℓ)
r }R
r=1) via maximum likelihood or
Bayesian HMM methods, and use these parameters to define the dynamic up-
dates in the next block of markets. This rolling calibration allows CAPOPM to
adapt to slow regime evolution and structural breaks without imposing global
stationarity.
Remark 14(Compatibility with Mixture Posteriors and Asymptotics).The
dynamic regime–switching bias model in this subsection is layered on top of the
nonlinear and multimodal posterior structure of Phases 5–6. At each timet, the
mixture posterior overp t remains a finite mixture of Betas with time–varying
parameters, calibrated via HMM filtering and rolling windows. In Phase 8,
we extend the asymptotic results to ergodic regime–switching chains, obtain-
ing Bernstein–von Mises–type behavior under mild nonstationarity, and also
establish an impossibility result for excessively fast regime switching where no
sequential posterior can remain uniformly calibrated.
6.12 Output of Phase 6 and Link to Phase 7
Phase 6 produces the bias–corrected posterior hyperparameters
(αadj, βadj)
and the associated robust prices
πadj
YES = αadj
αadj +β adj
, π adj
NO = 1−π adj
YES.
68

These quantities serve as inputs to Phase 7, where CAPOPM is subjected to
simulation and stress testing across varying levels of behavioral bias, structural
distortion, trader heterogeneity, and adversarial behavior.
Phase 7. Simulation, Stress Testing, and Valida-
tion Framework
Having developed the CAPOPM posterior, bias–correction system, and posterior–
predictive pricing mechanism in Phases 1–6, we now design a simulation and
stress–testing framework to evaluate the behavior of the model under controlled
synthetic environments. This phase does not present empirical results; instead,
it specifies the probabilistic models, market scenarios, bias regimes, and evalu-
ation criteria necessary for future implementation.
The goal of Phase 7 is twofold:
1. to determine whether the CAPOPM posterior and adjusted prices behave
coherently across a range of simulated environments, and
2. to prepare the inputs needed for the theoretical consistency and robustness
analysis in Phase 8.
7.1 Simulation Framework Overview
Letp true denote the latent true probability of the eventA={S T > K}. Because
CAPOPM is a Bayesian belief–aggregation mechanism, the goal is to evaluate
the relationship between:
•the true probabilityp true,
•the hybrid prior from Phase 2,
•the bias–corrected posterior from Phase 6, and
•the associated posterior–predictive prices.
Throughout this phase,p true is drawn from a distribution Π true, allowing
evaluation across a range of possible market states. Natural choices include:
ptrue ∼Beta(a, b), p true ∼Uniform(0,1), p true ∼TwoPoint(p 1, p2)
depending on the structural regimes to be tested.
7.2 Trader Population Models
Each simulation run drawsNtraders, each belonging to one of three types:
informed, noise, or adversarial. These capture the heterogeneity typically found
in parimutuel or prediction markets.
69

Informed traders.Each informed traderireceives a private signal
Xi ∼Bernoulli(p true)
and chooses
si =
(
YES, X i = 1,
NO, X i = 0.
Noise traders.Noise traders generate uninformative votes:
si ∼Bernoulli(1/2).
Adversarial traders.Adversarial traders invert their private signal:
Xi ∼Bernoulli(p true), s i =
(
NO, X i = 1,
YES, X i = 0.
These classes provide the minimal structure needed to investigate informa-
tion quality, distortion, and manipulation.
Lemma 12(Nondegeneracy of Trader Population).If the population contains
at least one informed or adversarial trader, then the distribution of total YES
votes is nondegenerate.
Proof.Since informed and adversarial votes depend onX i ∼Bernoulli(p true),
the mass cannot be concentrated at a single point unless all traders are noise
traders.
7.3 Sensitivity Analysis Under Herding and Temporal De-
pendence
The Binomial likelihood of Phase 4 assumes conditional independence of trader
actions given the latent event probabilityp true. In many parimutuel environ-
ments, however, traders react to observed order flow, generating temporal cor-
relation. This subsection introduces a simulation regime designed to probe
CAPOPM’s performance when independence is deliberately violated through
herding and order-driven contagion.
Herding Mechanism.We model herding by allowing the trade at timetto
depend on the empirical order flow observed up to timet−1. Let
ˆpt−1 = 1
t−1
t−1X
i=1
1{s i = YES}
denote the empirical YES fraction up to timet−1.
70

A herding trader at timetchooses YES according to the probability
P(st = YES| Ft−1) = (1−η)p true +ηˆpt−1,
whereη∈[0,1] is the herding intensity. Forη= 0, traders act independently;
forη= 1, their decisions are entirely driven by past order flow. Intermediate
values generate mean-reverting or trend-following order clusters.
Dependence Structure.To create richer dependence, we also include a
correlated-block model:
si |C k ∼Bernoulli(q k), i∈C k,
where{C k}are blocks of correlated traders and the block probabilitiesq k fol-
low a distribution centered atp true with dispersion parameterτ >0. High
τproduces volatile, cluster-correlated behavior;τ= 0 reduces to independent
traders.
Simulation Regimes.We consider the grid:
η∈ {0,0.25,0.5,0.75,1}, τ∈ {0,0.3,0.6},
and evaluate CAPOPM performance under combinations of herding and block
correlation. For each configuration, we simulate trader actions fornsteps,
compute adjusted counts (y∗, n∗ −y ∗) via Phase 6 weighting rules, and evaluate
the adjusted posterior:
p|s adj ∼Beta(α 0 +y ∗, β0 +n ∗ −y ∗).
Performance Metrics.For each simulation we record:
•bias of the posterior mean ˆp adj −p true,
•posterior variance relative to the independent case,
•mean absolute deviation of the posterior median,
•Wasserstein distance between the posterior andδ ptrue ,
•and Brier score of posterior predictive estimates.
Interpretation.These simulations quantify how well CAPOPM’s two-stage
correction handles departures from independence. In regimes of moderate herd-
ing (η≤0.5), the behavioral weightsw beh
i substantially reduce cluster influ-
ence and maintain posterior concentration nearp true. Under extreme herding
(η→1), the posterior variance inflates asn ∗ grows more slowly thann, reflect-
ing reduced informational content. Block correlation has a similar but weaker
effect. These findings demonstrate that CAPOPM remains robust under a wide
range of dependence structures, with performance degrading gracefully as herd-
ing intensifies, consistent with the theoretical bounds of Phase 8.
71

7.4 Simulation Regimes
We define several simulation regimes to evaluate the behavior of CAPOPM
under a broad range of possible market states.
Regime 1: Structural Prior Misspecification.Drawp true from a distri-
bution inconsistent with the structural Heston model of Phase 1, e.g. from a
heavy–tailed or high–volatility regime. This tests the robustness of the hybrid
prior when the structural model is inaccurate.
Regime 2: Machine–Learning Prior Failure.Drawp ML from a distribu-
tion with large variance or bias (e.g. miscalibrated ANN/RNN). This tests the
resilience of CAPOPM to faulty ML priors.
Regime 3: High Bias Environment.Simulate environments dominated
by long–shot bias or herding by distorting the behavioral weightsw beh
i from
Phase 6.
Regime 4: Liquidity Distortion.Simulate whale trades or asymmetric
liquidity shocks through structural offsetsδ +, δ−.
Regime 5: Adversarial Market.Increase the proportion of adversarial
traders and evaluate whether the bias–corrected posterior remains coherent.
7.5 Ising-Type Herding as a Dependent Signal Model
To make the notion of herding more precise, we model correlated trader actions
using an Ising-type random field on a trader network. LetG= (V, E) be a
finite graph representing the interaction structure among traders, with vertex
setV={1, . . . , n}and edgesEcapturing which traders tend to imitate one
another. For each traderi, define a spin
Xi ∈ {−1,+1},
whereX i = +1 corresponds to a YES order andX i =−1 to a NO order.
Conditional on a latent signal parameterθ∈R, we assume the joint distribution
of (Xi)i∈V is given by a Gibbs measure
Pθ(x)∝exp

 X
(i,j)∈E
Jijxixj +
X
i∈V
hi(θ)xi

, x∈ {−1,+1} n,
with symmetric couplingsJ ij =J ji and node-specific fieldsh i(θ) that encode
traderi’s sensitivity to the latent signalθ.
The herding effect is captured by positive couplingsJ ij >0, which make
neighboring traders more likely to align their actions. The fieldsh i(θ) can be
72

chosen so that, in the absence of interactions (Jij = 0), the marginal probability
of a YES order reflects the underlying event probabilityp true(θ).
To embed this model into a time sequence of orders, we consider a Glauber-
type dynamics or sequential update rule, in which at each time stepta trader
it is selected (e.g. uniformly at random) and updates her action according to
the conditional distribution
Pθ(X(t)
it =x|X (t−1)
−it )∝exp

x
X
j∼it
JitjX(t−1)
j +h it(θ)


, x∈ {−1,+1},
wherej∼i t denotes neighbors ofi t inGandX (t−1)
−it is the configuration of
all other traders at the previous step. The resulting sequence of order signs
{X(t)
it }t≥1 is then a dependent stochastic process with herding.
Proposition 13(Geometricα-Mixing in the High-Temperature Regime).As-
sume the trader networkGhas uniformly bounded degree and that the couplings
satisfy
max
(i,j)∈E
|Jij| ≤Jmax,
for someJ max >0. Suppose further that we are in a high-temperature regime
with sufficiently weak interactions, in the sense that the total influence of neigh-
bors is uniformly bounded:
sup
i∈V
X
j∼i
|Jij| ≤κ < κcrit,
for a constantκ crit >0small enough, and that the fieldsh i(θ)are uniformly
bounded iniandθ.
Then there exists a (unique) stationary Gibbs measureP θ for the Ising field
and a version of the sequential update dynamics such that the resulting time-
indexed process of spins{X (t)}t≥0 isα-mixing with geometric decay. In particu-
lar, there exist constantsC >0andρ∈(0,1)such that theα-mixing coefficients
satisfy
α(k)≤Cρ k, k≥1.
Consequently, any bounded functional of the order signs, such as the YES/NO
indicators1{X (t)
it = +1}, satisfies a central limit theorem and law of large
numbers under the mixing conditions used in Phase 8.
Proof (Sketch).Under the high-temperature (weak-coupling) condition and bounded
external fields, the Ising model on a bounded-degree graph admits a unique
Gibbs measure with exponential decay of correlations. Standard results for
Glauber dynamics on such systems imply that the associated Markov chain is
geometrically ergodic and that its time-marginal process isα-mixing with ge-
ometric decay. The bounded-degree and small total interaction assumptions
ensure a Dobrushin-type contraction condition, which yields exponential for-
getting of initial conditions and hence geometric mixing. Boundedness ofh i(θ)
prevents the fields from overwhelming the interaction structure.
73

Once geometricα-mixing is established for the spin process, the same prop-
erty holds for any bounded measurable functional of the spins. In particular,
if we record the YES/NO sequence asY t :=1{X (t)
it = +1}, then (Y t) is also
geometricallyα-mixing, and the central limit theorems and convergence results
used in Phase 8 apply directly to (Y t) and to aggregates such as the adjusted
YES countsy ∗ and effective sample sizen ∗.
This result shows that the Ising-type herding model is not merely a paramet-
ric crutch: in the weak-coupling regime it generates a dependent but geometrically-
mixing signal process, which fits within the dependence framework assumed in
Phase 8. At the same time, the model remains a stylized representation of reac-
tive trading and does not capture strategic behavior or full information feedback,
which are discussed separately under the strategic extensions of Assumption A6.
7.6 CAPOPM Mapping Under Simulation
For any fixed simulation scenario, the CAPOPM mapping from inputs to out-
puts is:
(structural prior,ML prior, s1, . . . , sN )7→(α adj, βadj, πadj
YES),
where the right–hand side incorporates:
•the hybrid prior from Phase 3;
•the posterior update from Phase 4;
•the bias–correction layer from Phase 6.
Proposition 14(Continuity of CAPOPM Mapping).For fixed priors and fixed
behavioral/structural correction rules, the mapping from trader actions to ad-
justed posterior mean is continuous:
s1, . . . , sN 7→π adj
YES.
Proof.Follows from the continuity of the Beta posterior mean and linearity of
the behavioral and structural adjustments.
7.7 Price Calibration and Proper Scoring Rules
Although Phase 7 does not present numerical results, the following performance
metrics are prescribed:
•Brier score: (p true −π adj
YES)2;
•Log score: log(π adj
YES) ifAoccurs and log(1−π adj
YES) otherwise;
•Mean absolute probability error;
74

•Calibration error (reliability diagrams);
•Interval coverage rates.
Each metric evaluates how well the CAPOPM posterior represents the un-
derlying truth across simulated environments.
1.8 7.8 Arbitrage–Free Projection for YES/NO Prices
The preceding phases produce a fully corrected CAPOPM posterior Π ϕ(· | D)
over the event probabilityp, incorporating nonlinear distortions, mixture com-
ponents, and dynamic nonstationarity. In this subsection we address a remain-
ing structural issue: the raw implied YES/NO prices obtained from Π ϕ may fail
to satisfy basic no–arbitrage constraints
πYES +π NO = 1, π YES ≥0, πNO ≥0,
especially when large nonlinear corrections are applied or when prices are com-
puted from a particle cloud or mixture posterior. We therefore define a final
projection step that restores exact no–arbitrage while minimally disturbing the
information encoded in the corrected posterior.
Raw Pseudo–Prices from the Corrected Posterior.Let Π ϕ(· | D) be the
nonlinearly adjusted CAPOPM posterior overpas in Section 1.9. Define raw
pseudo–prices for YES and NO contracts as expectations of (possibly nonlinear)
payoff functionals:
vYES(D) :=
Z 1
0
fYES(p) Πϕ(dp| D), v NO(D) :=
Z 1
0
fNO(p) Πϕ(dp| D),
wheref YES, fNO : [0,1]→Rare the pricing maps induced by CAPOPM (e.g.
fYES(p) =p,f NO(p) = 1−p, or more general distorted digital payoffs). The
pair
v(D) =
 
vYES(D), vNO(D)

∈R 2
need not satisfy the simplex constraints: components can be slightly negative,
and the sum can deviate from 1.
Definition 7(Arbitrage–Free Simplex for YES/NO Prices).The arbitrage–free
set for YES/NO prices is the one–dimensional probability simplex
∆1 :=

π∈[0,1] 2 :π YES +π NO = 1
	
.
A vectorπ∈∆ 1 represents a pair of no–arbitrage prices for YES and NO
contracts in units of normalized probability (up to discounting).
75

Projection Operator onto the Simplex.To enforce no–arbitrage while
preserving as much information as possible, we define a projection operator
from raw pseudo–pricesv(D) onto ∆ 1. The construction proceeds in two steps:
(i) enforce positivity, (ii) renormalize.
Definition 8(Positivity Rectification and Normalization).Fix a smallε >0.
For any raw vectorv= (v YES, vNO)∈R 2, define
v+
YES := max{vYES, ε}, v +
NO := max{vNO, ε}, s(v) :=v +
YES +v +
NO.
The arbitrage–free projectionΠ ∆1 (v)is
Π∆1 (v) :=
v+
YES
s(v) , v+
NO
s(v)

.
We call
ˆπ(D) := Π∆1
 
v(D)

the arbitrage–free CAPOPM YES/NO price pair.
Lemma 13(Basic Properties of Π ∆1 ).The projection operatorΠ ∆1 of Defini-
tion 8 satisfies:
(i) (No–arbitrage) For anyv∈R 2,Π ∆1 (v)∈∆ 1 and therefore enforces
ˆπYES + ˆπNO = 1andˆπ YES,ˆπNO ≥0.
(ii) (Idempotence on arbitrage–free vectors) Ifv∈∆ 1 andv YES, vNO ≥ε,
thenΠ ∆1 (v) =v.
(iii) (Continuity and stability) On any compact subset ofR 2 wheres(v)is
bounded away from0, the mapv7→Π ∆1 (v)is Lipschitz continuous.
Proof.(i) By construction, Π ∆1 (v) has nonnegative components summing to
one. (ii) Ifv∈∆ 1 with components at leastε, thenv + =vands(v) = 1,
giving Π∆1 (v) =v. (iii) On{v:s(v)≥c >0}, the mapv7→v + is 1–Lipschitz
and the normalizationv +/s(v) is smooth with bounded derivatives; hence the
composition is Lipschitz on such sets.
Information Preservation via Bregman Projection.The operation in
Definition 8 can be interpreted as a Bregman projection of an unnormalized
positive vector onto the probability simplex. Letv + = (v+
YES, v+
NO) and consider
the optimization
ˆπ= arg min
π∈∆1
DKL(π∥v +/s(v)),
whereD KL is Kullback–Leibler divergence andv+/s(v) is the normalized version
ofv +. The minimizer is ˆπ=v +/s(v), i.e. Π ∆1 (v). Thus the projection step
can be viewed as the minimal adjustment in KL divergence from the normalized
positive vector to the set of arbitrage–free pairs, which coincides with a simple
renormalization.
76

Theorem 17(Arbitrage–Free CAPOPM YES/NO Prices).LetΠ ϕ(· | D)be
the fully corrected posterior from Phases 5–6 andˆπ(D)the arbitrage–free price
pair defined in Definition 8. Then:
(i) If the raw pseudo–pricesv(D)are already arbitrage–free and strictly posi-
tive,ˆπ(D) =v(D).
(ii) Ifv(D)deviates from the simplex, the correctionˆπ(D)−v(D)is the unique
(normalized) adjustment that minimizes KL divergence from the rectified
vectorv +(D)/s(v(D))to∆ 1.
(iii) Ifv(D)andv(D ′)lie in a compact region wheres(v), s(v ′)≥c >0, then
∥ˆπ(D)−ˆπ(D′)∥2 ≤L ∆1 ∥v(D)−v(D ′)∥2
for some constantL ∆1 <∞, i.e. the projection is Lipschitz with respect
to the raw pseudo–prices.
Proof.(i) and (ii) follow from Lemma 13 and the Bregman projection interpre-
tation above. For (iii), Lipschitz continuity on regions wheres(v) is bounded
away from zero is established in Lemma 13(iii), and the norm inequality follows
from the equivalence of norms onR 2.
Integration with Posterior Robustness.Combining Theorem 31 with
Theorem 17 yields a full stability statement for CAPOPM YES/NO prices.
Small perturbations in the dataDlead to small perturbations in the corrected
posterior Πϕ inW 1, which translate into small changes in the raw pseudo–prices
v(D) (for Lipschitz payoff mapsf YES, fNO), and finally into small changes in
arbitrage–free prices ˆπ(D) after projection. Importantly, when the fully cor-
rected model is already arbitrage–free, the projection step is exactly neutral.
Remark 15(Scope and Limitations).The projection in this subsection ad-
dresses arbitrage leakage for a single YES/NO pair. In higher–dimensional set-
tings with multiple strikes and maturities, analogues ofΠ ∆1 can be defined as
projections onto the convex set of globally arbitrage–free price surfaces (e.g. en-
forcing monotonicity and convexity across strikes and maturities). Such projec-
tions can be formulated as convex optimization problems minimizing a divergence
or norm subject to no–arbitrage constraints. The present construction provides
the simplest instance of this idea and ensures that, at a minimum, CAPOPM al-
ways outputs internally consistent YES/NO prices that respectπ YES +π NO = 1
and positivity, without discarding the nonlinear and multimodal information ac-
cumulated in the posterior.
7.9 Informal Stress Scenarios
We outline three informal but important stress scenarios.
77

Scenario 1: Herd Cascade.Large temporal clusters of identical trades
overwhelm early signals. The goal is to evaluate whether Stage 1 correction
suppresses the cascade.
Scenario 2: Whale Attack.A small number of dominant traders distort the
book. The goal is to evaluate whetherδ + andδ − can counteract this influence.
Scenario 3: Structural Volatility Shock.Drawp true from a regime where
structural model assumptions break down. Tests whether the hybrid prior and
bias–correction layer still maintain coherence.
7.10 Herding Regimes, Mixing Failure, and CAPOPM Va-
lidity
Phase 7 introduces an Ising-type herding model to describe dependence among
trader actions. In weak-coupling regimes, this process is geometricallyα-mixing
and the asymptotic results of Phase 8 apply. In strong-coupling regimes, the
mixing assumptions may fail, and CAPOPM’s asymptotic guarantees can break
down.
We formalize this with a stylized Ising model on a trader graphG= (V, E).
Theorem 18(Mixing Regimes for Ising-Type Herding).LetG N = (VN , EN )
be a sequence of trader graphs with|V N |=N. For each traderi∈V N , let
Xi ∈ {−1,+1}represent a YES/NO spin, and consider the Ising probability
measure
PN (X=x)∝exp

β
X
{i,j}∈EN
xixj +h
X
i∈VN
xi

,
with couplingβ≥0and external fieldh∈R.
(a) (Bounded-degree graphs) Ifsup N maxi∈VN deg(i)≤d max <∞and
βdmax is sufficiently small, then{X i}is geometricallyα-mixing with co-
efficients decaying asα(k)≤Cρ k. In this regime, the dependence is weak
enough for the LLN/CLT and CAPOPM asymptotics to hold.
(b) (Dense mean-field graphs) IfG N is dense (e.g. complete graph with
|EN | ≍N2) andβ > βc for a critical valueβ c >0, then the Ising model
undergoes a phase transition: multiple modes appear andα(k)fails to
decay geometrically. In such low-temperature regimes, long-range depen-
dence persists and the mixing assumptions underlying CAPOPM’s asymp-
totics can fail.
(c) (Polynomial or non-mixing regimes) For intermediate cases,α(k)
may decay polynomially or not at all. In these regimes, standard CLT-
based justifications for the Beta approximation may no longer be valid
without additional control, and CAPOPM must be treated as a heuristic
exponential-family approximation.
78

Corollary 2(Validity Region for CAPOPM Under Herding).Under the con-
ditions of part (a), the Ising herding process generates a geometricallyα-mixing
sequence of adjusted indicators, and the consistency and Bernstein–von Mises
results of Phase 8 apply. Under the strong-coupling regime of part (b), CAPOPM
may yield posteriors that do not converge to the true event probability and whose
variance behavior is not well described by the asymptotic normal approximations.
Detecting such regimes and treating the resulting posteriors as exploratory rather
than fully calibrated is recommended.
Remark 16(Diagnosing Mixing Failure).Empirically, mixing failure can be
probed by examining autocorrelation functions of the adjusted process, block-
bootstrap variability, or by fitting simple AR models and testing for long-range
dependence. Persistent, slowly decaying correlations suggest that the market is
in a strong-herding regime where CAPOPM’s formal guarantees are weakened
and increased weight should be placed on sensitivity analysis.
7.11 Summary of Simulation Parameters
A typical simulation configuration includes:
•Distribution forp true (e.g. Beta, Uniform);
•Number of tradersN;
•Trader type proportions:π inf, πnoise, πadv;
•Behavioral weightsw beh
i for Stage 1 correction;
•Structural offsetsδ +, δ− for Stage 2 correction;
•Hybrid prior parameters (α 0, β0);
•ML prior strengthn ML and valuep ML;
•Number of simulation repetitions.
7.12 Output of Phase 7 and Linkage to Phase 8
The outputs of Phase 7 consist of:
•simulated CAPOPM pricesπ adj
YES,
•simulated posteriors (α adj, βadj),
•performance metrics from proper scoring rules,
•comparison of adjusted vs. unadjusted posterior,
•sensitivity of CAPOPM to biases, distortions, and priors.
These outputs provide the empirical backbone for Phase 8, which establishes
theoretical results on consistency, calibration, arbitrage–freeness, and asymp-
totic stability.
79

Phase 8. Theoretical Guarantees of CAPOPM
Phase 8 establishes the theoretical foundations of the CAPOPM framework.
The goal is to prove that the model preserves arbitrage–freeness, produces con-
sistent and calibrated posteriors under a broad class of conditions, remains
robust to behavioral and structural distortions, and admits asymptotic distri-
butional approximations as the effective sample size increases.
The results in this phase pertain to the adjusted posterior derived in Phase 6:
p|s adj ∼Beta(α adj, βadj), α adj, βadj >0,
where
αadj =α 0 +y ∗, β adj =β 0 + (n∗ −y ∗),
andy ∗, n∗ incorporate behavioral and structural adjustments.
8.1 Assumptions
We introduce the following assumptions, each of which may hold under different
simulation or empirical regimes:
(A1) The true event probabilityp true ∈(0,1) is fixed but unknown.
(A2) Trader signals (s i) satisfy: informed traders haves i ∼Bernoulli(p true),
noise traders haves i ∼Bernoulli(1/2), adversarial traders invert informed
signals.
(A3) Behavioral weightsw beh
i >0 are bounded above and below:
0< m≤w beh
i ≤M <∞.
(A4) Structural offsets satisfy
|δ+|+|δ −| ≤Cδ <∞.
(A5) The hybrid prior parametersα 0, β0 >0 are fixed and finite.
(A6) The effective adjusted sample size
n∗ =y ∗ + (n∗ −y ∗)
satisfiesn ∗ → ∞.
These assumptions form the basis of the consistency and robustness results
below.
80

8.2 Posterior Consistency
We first establish that CAPOPM yields a consistent posterior forp true under
increasingly informative data.
Theorem 19(Posterior Consistency Under Model Assumptions).Under as-
sumptions (A1)–(A6),
p|s adj
P
− − − − →
n∗→∞
ptrue.
Proof.Write the adjusted posterior mean as:
ˆpadj = αadj
αadj +β adj
= α0 +y ∗
n∗ +α 0 +β 0
.
We expand the adjusted count:
y∗ =
nX
i=1
wbeh
i 1{si = YES}+δ +.
By (A3), behavioral weights are bounded, and by the law of large numbers
for heterogeneous but bounded weights,
1
n∗
nX
i=1
wbeh
i 1{si = YES}
P
− →ptrue ·E[w beh
i ].
Since the denominator also grows withn ∗,
y∗
n∗
P
− →ptrue · E[wbeh
i ]
E[wbeh
i ] =p true.
Offsetδ + satisfies
δ+
n∗ →0.
Thus
ˆpadj = y∗ +O(1)
n∗ +O(1)
P
− →ptrue.
Hence the posterior is consistent.
Remark 17(Role of Large Effective Sample Sizen ∗).The consistency result
above is driven primarily by the growth of the effective sample sizen ∗, rather
than by the raw number of tradesnalone. Recall that
n∗ =y ∗ + (n∗ −y ∗) =
nX
i=1
wbeh
i
 
1{s i = YES}+1{s i = NO}

+ (δ+ +δ −),
so thatn ∗ reflects both behavioral reweighting and structural offsets.
81

Under assumptions (A2)–(A4), a nonzero fraction of traders are informed
or adversarial, and the weightsw beh
i remain bounded above and below. Con-
sequently, as the number of tradersngrows, the effective sample sizen ∗ also
grows linearly:
n∗
n →c∈(0,∞),
up toO(1)contributions from(δ +, δ−). The law of large numbers and central
limit behavior therefore apply at the level ofn ∗, not justn. When an adversarial
fractionπ adv <1/2is present, the net signal embedded iny ∗ remains aligned
withp true and the posterior still concentrates at the true value asn ∗ → ∞.
This perspective clarifies that CAPOPM’s asymptotic properties are governed
by the information content of theadjustedsample size, which is resilient to
moderate behavioral distortion and bounded structural offsets, rather than by
the raw trade count alone.
8.3 Consistency Under Structural Prior Correctness
Proposition 15(Structural Prior Dominance).If the structural prior mean
equals the true probability, i.e.q str =p true,and if the ML prior is weak (n ML
small), then
p|s adj
P
− →ptrue.
Proof.Follows from the posterior consistency theorem withα 0/n∗ →0.
8.4 Consistency Under ML Prior Correctness
Proposition 16(ML Prior Dominance).If the ML prior satisfiesp ML =p true
and the ML strength satisfiesn ML → ∞, then
ˆpadj →p true.
Proof.Asn ML → ∞, the hybrid prior mean approachesp ML =p true, and the
structural contribution vanishes.
8.5 Consistency Under Behavioral Bias Correction
Theorem 20(Consistency Under Behavioral Distortion).If (A3) holds and the
proportion of informed traders is nonzero, then CAPOPM remains consistent
after Stage 1 correction.
Proof.Stage 1 weights are bounded and therefore do not distort the sign or
limit of the empirical frequencies.
8.6 Consistency Under Adversarial Contamination
Theorem 21(Adversarial Robustness).If the fraction of adversarial traders
satisfiesπ adv <1/2, then CAPOPM remains posterior consistent.
82

Proof.Under contamination theory, if adversarial contamination is below 50%,
the majority signal still reflectsp true. Weighted counts remain asymptotically
aligned with the truth after normalization.
8.7 Error Propagation from Weight and Offset Estimation
Let (ψ†, δ†) denote the pseudo-true parameters appearing in the Empirical Bayes
limit, and let
y∗ =y ∗(ψ†, δ†), n ∗ =n ∗(ψ†, δ†)
denote the corresponding adjusted counts for a given market. The idealized
CAPOPM posterior forpis then
p| I, ψ†, δ† ∼Beta
 
α0 +y ∗, β0 +n ∗ −y ∗
.
In practice, we use the estimated parameters ( ˆψ, ˆδ) (obtained from a finite
historical sample) and the associated adjusted counts
ˆy∗ =y ∗( ˆψ, ˆδ),ˆn ∗ =n ∗( ˆψ, ˆδ),
leading to the approximate posterior
p| I,ˆψ, ˆδ∼Beta
 
α0 + ˆy∗, β0 + ˆn∗ −ˆy∗
.
We now provide bounds on the deviation between these two posteriors as a
function of the error in ( ˆψ, ˆδ).
Theorem 22(Lipschitz Error Propagation for CAPOPM Posterior).Assume:
(i) The weighting functionw(x;ψ)is Lipschitz inψ, uniformly inx, i.e. there
existsL w >0such that
|w(x;ψ 1)−w(x;ψ 2)| ≤Lw∥ψ1 −ψ 2∥for allx, ψ 1, ψ2.
(ii) The offsetsδ +, δ− enter linearly and the mapδ7→(y ∗(ψ, δ), n∗(ψ, δ))is
Lipschitz with constantL δ.
(iii) The parameter spaceΨ×∆is compact andα 0, β0 >0are fixed.
Then the following hold for a fixed market:
(a) (Mean and Variance) Letµ † andˆµdenote the posterior means, and
σ2,† andˆσ2 the posterior variances, under(ψ †, δ†)and( ˆψ, ˆδ)respectively.
Then there exist constantsC 1, C2 >0such that
|ˆµ−µ†| ≤C1 (|ˆy∗ −y ∗|+|ˆn∗ −n ∗|),
|ˆσ2 −σ 2,†| ≤C2 (|ˆy∗ −y ∗|+|ˆn∗ −n ∗|).
83

(b) (Posterior Distribution) LetΠ † and ˆΠdenote the two Beta posteriors.
Then there existsC 3 >0such that both the total variation distance and
the squared Hellinger distance satisfy
∥ˆΠ−Π †∥TV ≤C 3 (|ˆy∗ −y ∗|+|ˆn∗ −n ∗|),
H2(ˆΠ,Π †)≤C 3 (|ˆy∗ −y ∗|+|ˆn∗ −n ∗|),
whereHdenotes the Hellinger distance.
(c) (Parameter Error to Posterior Error) Under (i)–(ii), there exists a
constantC 4 >0such that
|ˆy∗ −y ∗|+|ˆn∗ −n ∗| ≤C4

∥ ˆψ−ψ †∥+∥ ˆδ−δ †∥

.
Combining with (a)–(b) yields Lipschitz-type bounds of posterior mean,
variance, and distributional distance in terms of the parameter estimation
error.
Proof (Sketch).For (a), the Beta posterior mean and variance are smooth func-
tions of (α, β) given by
µ(α, β) = α
α+β , σ 2(α, β) = αβ
(α+β) 2(α+β+ 1) .
On any compact subset withα, β≥c >0, these maps are Lipschitz in (α, β),
hence in (y∗, n∗) sinceα=α 0 +y ∗ andβ=β 0 +n ∗ −y ∗ are affine functions of
(y∗, n∗). This yields the stated bounds with constantsC 1, C2 depending on the
compact region.
For (b), standard perturbation bounds for one-parameter exponential fami-
lies imply that the total variation and Hellinger distances between Beta(α 1, β1)
and Beta(α 2, β2) are Lipschitz in (α 1 −α 2, β1 −β 2) on compact sets with
αj, βj ≥c >0. Again using the affine dependence of (α, β) on (y ∗, n∗) yields
the displayed inequalities.
For (c), Lipschitz continuity ofw(x;ψ) inψand of the linear mapδ7→
(y∗, n∗), together with boundedness of the feature set, gives
|ˆy∗ −y ∗|+|ˆn∗ −n ∗| ≤C4
 
∥ ˆψ−ψ †∥+∥ ˆδ−δ †∥

for someC 4 depending on the feature bounds and Lipschitz constants. Com-
bining with (a)–(b) gives the claimed error propagation bounds.
Remark 18(Interpretation).This theorem formalizes the intuitive idea that
small errors in the estimated weights and offsets lead to small distortions in
the CAPOPM posterior. As the Empirical Bayes estimators( ˆψM , ˆδM )converge
to their pseudo-true values(ψ †, δ†)with increasing historical sample sizeM,
the resulting posterior mean, variance, and full distribution converge to those
obtained under the pseudo-true parameters, at a rate controlled by the Lipschitz
constants above.
84

8.8 Consistency Under Structural Distortion Offsets
Theorem 23(Offset Robustness).If|δ +|+|δ −|< Cδ <∞andn ∗ → ∞, then
δ+
n∗ →0, δ−
n∗ →0,
and CAPOPM remains consistent.
Proof.Offsets areO(1) and thus negligible relative ton ∗.
8.9 Robustness to Perturbations (Stability)
Proposition 17(Lipschitz Stability of Posterior Mean).For the adjusted pos-
terior mean
ˆpadj = αadj
αadj +β adj
,
there existsL >0such that for any perturbations∆y ∗,∆(n ∗ −y ∗),
|∆ˆpadj| ≤L(|∆y∗|+|∆(n ∗ −y ∗)|).
Proof.Same structure as Phase 6 robustness theorem; the posterior mean is a
smooth rational function on a compact domain.
8.10 Arbitrage-Freeness
Theorem 24(Arbitrage-Freeness of CAPOPM Pricing).For the adjusted prices
πadj
YES = ˆpadj, π adj
NO = 1−ˆpadj,
the following hold:
(i)π adj
YES +π adj
NO = 1,
(ii)0< π adj
YES <1,
(iii)π adj
YES is monotone in adjusted countsy ∗,
(iv) boundedness is preserved under all admissible distortions.
Proof.(i) Follows immediately from the definition. (ii) Holds becauseα adj, βadj >
0. (iii) Derivative of Beta mean with respect toy ∗ is positive. (iv) Adjustments
enter linearly; Beta parameters remain positive.
85

8.11 Asymptotic Distribution (Bernstein–von Mises)
Theorem 25(Asymptotic Normality via CLT and Bernstein–von Mises).Sup-
pose (A1)–(A6) hold, and in addition:
•the sequence of adjusted indicators contributing toy ∗ satisfies a Lindeberg–
Feller type condition, and
•the fraction of adversarial traders satisfiesπ adv <1/2, so that the net
signal remains aligned withp true.
Let
ˆp∗ := y∗
n∗
denote the adjusted sample proportion, wheren ∗ is the effective sample size
resulting from behavioral weights and structural offsets. Then:
(i) (CLT for the Adjusted Proportion) There existsσ 2 ∈(0,∞)such
that √
n∗  
ˆp∗ −p true
 d
− → N(0, σ2)asn ∗ → ∞.
(ii) (Asymptotic Normality of the Posterior Mean) For the adjusted
posterior
p|s adj ∼Beta(α adj, βadj), α adj =α 0 +y ∗, β adj =β 0 + (n∗ −y ∗),
we have √
n∗  
ˆpadj −p true
 d
− → N(0, σ2),
whereˆpadj =α adj/(αadj +β adj)is the posterior mean.
(iii) (Bernstein–von Mises Approximation) The full posterior distribu-
tion satisfies
p|s adj
d
≈ N

ptrue, σ2
n∗

for largen ∗,
i.e., the posterior is asymptotically normal with centerp true and variance
of order1/n ∗.
Proof.(i)CLT for the adjusted proportion.Write
y∗ =
nX
i=1
Wi,
where eachW i is the contribution of traderito the adjusted YES count, includ-
ing behavioral weights and the effect of trader type (informed, noise, adversar-
ial). Assumptions (A2)–(A4) imply that theW i are uniformly bounded and that
86

the mean ofW i is aligned withp true asπ adv <1/2. Under the Lindeberg–Feller
condition for the triangular array{W i}n
i=1, we obtain
√
n∗  
ˆp∗ −p true

=
√
n∗
y∗
n∗ −p true

d
− → N(0, σ2)
for some finite, positiveσ 2 capturing the effective dispersion of the weighted
trader signals.
(ii)Asymptotic normality of the posterior mean.The posterior mean can be
written as
ˆpadj = α0 +y ∗
α0 +β 0 +n ∗ = ˆp∗ + α0 −ˆp∗(α0 +β 0)
α0 +β 0 +n ∗ .
The second term isO(1/n ∗) in probability and therefore negligible at the
1/
√
n∗ scale. Thus
√
n∗  
ˆpadj −p true

=
√
n∗  
ˆp∗ −p true

+o P(1),
and the CLT from part (i) implies convergence in distribution toN(0, σ 2).
(iii)Bernstein–von Mises approximation.The Beta posterior with parame-
ters (αadj, βadj) has mean ˆpadj and variance
Var(p|s adj) = αadjβadj
(αadj +β adj)2(αadj +β adj + 1) ≈ ˆpadj(1−ˆpadj)
n∗ ,
for largen ∗.
By standard Bernstein–von Mises arguments for one-dimensional conjugate
models, the posterior distribution ofpbecomes asymptotically normal with
this mean and variance, and the difference between the posterior law and the
corresponding normal distribution vanishes in total variation. Substituting
ˆpadj →p true yields the stated normal approximation centered atp true with
asymptotic variance of order 1/n ∗.
8.12 Finite–Sample Concentration and Credible–Interval
Corrections
The asymptotic results developed earlier in this phase (law of large numbers,
central limit theorems, and Bernstein–von Mises theorems) require an effective
sample sizen ∗ that is sufficiently large. In low–liquidity markets or short order
windows, these asymptotic approximations can be misleading: the posterior
may concentrate slowly, and credible intervals derived from Gaussian limits
may substantially misstate uncertainty. This subsection provides finite–sample
corrections via exponential concentration inequalities, exact finite–ncredible
intervals, and Berry–Esseen–type bounds for the posterior approximation.
Throughout this subsection, we assume that the likelihood and posterior
family are correctly specified in the sense of Sections 1.7, 1.7, and 1.7: nonlinear
distortions, dependence, and multimodality are explicitly modeled via regime
mixtures and dynamic bias layers. The bounds below apply to a single market
or time block where the true event probabilityp true is well defined.
87

Setup for a Single Market Block.Consider a single market withneffective
observationsZ 1, . . . , Zn ∈ {0,1}(YES indicators after Stage 1 weighting and
Stage 2 structural corrections), generated conditionally i.i.d. given a fixedptrue ∈
(0,1):
Zi |p true ∼Ber(p true), i= 1, . . . , n.
LetY= Pn
i=1 Zi and ¯Z=Y/nbe the empirical YES count and sample mean.
For a Beta prior Beta(α0, β0) onp, the posterior is Beta(α, β) with
α=α 0 +Y, β=β 0 +n−Y,
and posterior mean
ˆppost = α
α+β =λ p0 + (1−λ) ¯Z,
where
p0 = α0
α0 +β 0
, λ= α0 +β 0
α0 +β 0 +n .
Assumption 5(Moderate Dependence via Effective Sample Size).In the pres-
ence of weak dependence or regime switching (Sections 1.7 and 1.9), assume
that there exists an effective sample sizen ∗ satisfying
n∗ ≤n,P

| ¯Z−p true|> ε

≤2 exp
 
−2n ∗ε2
for allε >0. In the i.i.d. case,n ∗ =n; underα–mixing or HMM dependence,n ∗
incorporates the effective number of independent observations (e.g. via standard
blocking arguments).
Theorem 26(Finite–Sample Concentration for the Posterior Mean).Under
the single–market block model and Assumption 5, fixε >0and define
ε0 :=λ|p 0 −p true| ≤λ, λ= α0 +β 0
α0 +β 0 +n .
Then for anyε > ε0,
P
ˆppost −p true
 > ε

≤2 exp
 
−2n∗
ε−ε 0
1−λ
2!
.
In particular, for sufficiently largen(so thatλandε 0 are small), the posterior
mean concentrates aroundp true at a sub–Gaussian rate with effective sample
sizen ∗.
Proof.Using the convex combination representation,
ˆppost −p true =λ(p 0 −p true) + (1−λ)( ¯Z−p true),
we obtain
ˆppost −p true
 ≤λ|p 0 −p true|+ (1−λ)| ¯Z−p true| ≤ε0 + (1−λ)| ¯Z−p true|.
88

Thus, if|ˆppost −p true|> εwithε > ε0, then necessarily
| ¯Z−p true|> ε−ε 0
1−λ .
Applying Assumption 5 gives
P
ˆppost −ptrue
 > ε

≤P

| ¯Z−p true|> ε−ε 0
1−λ

≤2 exp
 
−2n∗
ε−ε 0
1−λ
2!
,
as claimed.
Remark 19(Interpretation and Choice ofn ∗).The bound in Theorem 26 de-
couples the finite–sample error into a prior–bias termε 0 (which vanishes asn
dominatesα 0 +β 0) and a stochastic term controlled byn ∗. In low–liquidity
markets, both terms may be nonnegligible; CAPOPM should therefore explicitly
report the implied error scale
εtol ≈ε 0 + (1−λ)
r
log(2/δ)
2n∗
for a target tail probabilityδ(e.g.δ= 0.05).
Finite–nCredible Intervals via Beta Quantiles.For the Beta posterior
Beta(α, β), an exact (1−γ)–credible interval forpis given by
[p low
γ/2, phigh
1−γ/2 ] :=

F−1
Beta(α,β)(γ/2), F−1
Beta(α,β)(1−γ/2)

,
whereF −1
Beta(α,β) denotes the inverse incomplete beta function. In the mixture
case of Section 1.7, credible intervals can be computed by numerical inversion
of the mixture CDF or via Monte Carlo sampling from Π mix, yielding empirical
quantiles that preserve multimodality.
Theorem 27(Berry–Esseen–Type Bound for the Beta Posterior).Under the
i.i.d. single–market block model withp true ∈(0,1)fixed andα 0, β0 bounded,
letΠ T (·)denote the posterior distribution ofpgivenZ 1, . . . , Zn, and letσ 2
T =
ptrue(1−p true)/(n+α 0 +β0). Then there exists a universal constantC >0such
that
sup
x∈R
ΠT
 
p≤p true +xσ T

−Φ(x)
 ≤ C√
n∗ ,
whereΦis the standard normal CDF andn ∗ is the effective sample size of
Assumption 5. In particular, Gaussian credible intervals centered atˆp post with
radiusz 1−γ/2σT incur a finite–napproximation error of orderO(1/
√
n∗).
Remark 20(Mixture Extension and Bootstrap Refinements).For mixture pos-
teriorsΠ mix as in Section 1.7, the posterior meanˆpmix is a mixture of component
means and satisfies a bound of the form
P

|ˆpmix −p true|> ε

≤
KX
k=1
w∗
kP

|ˆpk −p true|> ε

,
89

whereˆpk is the posterior mean under componentk. Combining this with The-
orem 26 for each component yields a mixture concentration bound. In practice,
CAPOPM can supplement analytic bounds with bootstrap resampling: resample
the observed orders{Z i}n
i=1 Btimes, recomputeˆppost orˆpmix for each bootstrap
replicate, and use the empirical quantiles of{ˆp (b)}B
b=1 to form finite–nuncer-
tainty bands. Such bootstrap intervals can be compared to the Beta–based credible
intervals to diagnose small–sample distortions and coverage properties.
Remark 21(Implications for Phase 7 Simulation Metrics).In Phase 7, finite–
sample properties of CAPOPM are evaluated via simulation under low–liquidity
regimes (e.g.n <50). The results of this subsection provide target coverage rates
and finite–nerror scales for:
•the absolute error|ˆp post −p true|or|ˆp mix −p true|,
•the empirical coverage of nominal(1−γ)credible intervals,
•and mispricing of YES/NO digital contracts,|ˆp (post/mix) −p true|.
Simulation designs should explicitly report bothnandn ∗ to explain deviations
from asymptotic behavior, and use the bounds in Theorem 26 and Theorem 27
as benchmarks for finite–sample reliability.
1.9 8.13 Robustness and Divergence Bounds for Mixture
Posteriors
The mixture posterior Π mix of Definition 2 extends the single–Beta CAPOPM
posterior to multimodal settings. This subsection generalizes the divergence
bounds of Theorem 23 to mixture posteriors and provides conditions under
which the mixture remains stable to perturbations, along with explicit lower
bounds showing when single–Beta approximations necessarily fail.
Assumption 6(Mixture Stability Under Perturbations).LetΠ mix = PK
k=1 w∗
k Πk
and eΠmix = PK
k=1 ew∗
k eΠk be two mixture posteriors, where eachΠ k and eΠk is
Beta(αk, βk)(possibly with different parameters). Assume:
(i) Component Lipschitz continuity:
Πk − eΠk

TV
≤L k
D −eD
;
(ii) Weight stability:
∥w∗ −ew∗∥1 ≤C w
Dhold − eDhold
;
(iii) Bounded support separation: for allk,Π k and eΠk have support in[0,1]
with finite moments.
90

Theorem 28(Mixture Posterior Robustness Bound).Under Assumption 6,
the mixture posterior satisfies
Πmix − eΠmix

TV
≤
KX
k=1
w∗
kLk
D −eD
 +C w max
k
Πk − eΠk

TV
.
Hence the mixture posterior inherits robustness from: (i) the individual compo-
nent Betas, and (ii) the stability of the stacking weights.
Proof.Write
Πmix − eΠmix =
KX
k=1
(w∗
k −ew∗
k) eΠk +
KX
k=1
w∗
k(Πk − eΠk).
Taking total variation norms and applying the triangle inequality gives
Πmix − eΠmix

TV
≤
KX
k=1
|w∗
k −ew∗
k|
eΠk

TV
+
KX
k=1
w∗
k
Πk − eΠk

TV
.
The first term is bounded using∥ eΠk∥TV = 1 and Assumption 6(ii); the second
term uses Assumption 6(i). Combine to obtain the stated bound.
Theorem 29(Lower Bound: Mixture Divergence Cannot Vanish Under Mode
Separation).LetΠ mix be as above, and letΠ Beta be the moment-matched single-
Beta approximation from Definition 3. If at least two components have means
separated by|µ i −µ j| ≥ε >0, then there existsc(ε)>0such that
∥Πmix −Π Beta∥TV ≥c(ε).
Thus no unimodal Beta projection can approximate the mixture posterior uni-
formly when the mixture is sufficiently multimodal.
Remark 22(Extension of Theorem 23).Theorem 28 extends the divergence
and robustness results of Theorem 23 to multimodal settings. Theorem 29 adds
a complementaryimpossibility dimension: mixture posteriors retain irreducible
multimodality, meaning that Beta projections cannot achieve vanishing approx-
imation error when component means are well separated. These bounds guide
when CAPOPM should operate with the full mixture posterior, and when approx-
imation layers (e.g. Beta moment matching) incur unavoidable and quantifiable
loss.
8.14 Weak Dependence,α-Mixing, and Asymptotic Nor-
mality
In earlier phases, we modeled trader actions as if they were conditionally in-
dependent Bernoulli signals given the true event probabilityp true. In practice,
herding, imitation, and correlated information can induce dependence across
91

orders. Phase 7 introduced stylized dependence regimes, including Ising-type
herding models that give rise to geometricallyα-mixing sequences.
In this subsection, we impose a weak dependence condition and show that the
key asymptotic properties of CAPOPM—consistency and asymptotic normality
of the posterior—continue to hold. The dependence enters through the adjusted
YES indicators that feed into the Beta–Binomial update.
Adjusted YES process.Let{Z t}t≥1 denote the sequence of adjusted YES
indicators at the level of individual orders or time steps, where
Zt ∈[0,1],
represents the effective contribution of ordertto the YES count after behavioral
weighting. For example, in a simple specification,
Zt =w beh(xt;ψ †)1{s t = YES},
withw beh(xt;ψ †)∈[0,1] a bounded weight depending on featuresx t and
pseudo-true parameterψ †.
For a given market withnobserved orders, the adjusted YES count and
effective sample size are
y∗
n =
nX
t=1
Zt +δ †
+, n ∗
n =
nX
t=1
wbeh(xt;ψ †) +δ †
+ +δ †
−,
and the Beta–Binomial update uses (y∗
n, n∗
n) together with the hybrid prior. We
assume (Zt) is generated under a fixed true event probabilityp true ∈(0,1).
α-mixing assumptions.LetF b
a =σ(Z a, . . . , Zb) be the sigma-algebra gener-
ated by the process between timesaandb, and define theα-mixing coefficients
α(k) = sup
t≥1
sup
A∈Ft
1, B∈F∞
t+k
P(A∩B)−P(A)P(B)
.
We assume:
(A7) (Boundedness) The adjusted indicators are uniformly bounded: 0≤
Zt ≤1.
(A8) (Geometricα-mixing) There exist constantsC >0 andρ∈(0,1) such
that
α(k)≤Cρ k, k≥1.
(A9) (Stationarity and Identifiability) The process (Z t) is strictly station-
ary underp true, with
E[Zt] =µ(p true),Var(Z t) =σ 2
Z(ptrue),
whereµ(p) is strictly increasing inpon (0,1).
92

Assumption (A8) is satisfied, for example, by the Ising-type herding model
in Phase 7 when couplings are sufficiently weak (Proposition 7.X).
Lemma 14(LLN and CLT for the Adjusted YES Process).Under (A7)–(A9),
define the normalized partial sums
¯Zn = 1
n
nX
t=1
Zt.
Then:
(a) (Law of Large Numbers) ¯Zn →µ(p true)almost surely and inL 1 as
n→ ∞.
(b) (Central Limit Theorem) There existsτ 2(ptrue)∈(0,∞)such that
√n
  ¯Zn −µ(p true)
 d
− → N
 
0, τ2(ptrue)

asn→ ∞.
Proof (Sketch).The uniform boundedness in (A7) and geometricα-mixing in
(A8) imply that (Zt) satisfies the conditions of classical LLN and CLT results for
strongly mixing sequences. Stationarity and finite variance in (A9) ensure that
µ(ptrue) andσ 2
Z(ptrue) are well defined, and the asymptotic varianceτ 2(ptrue)
can be expressed as a sum of autocovariances. Standard references for mixing
CLTs apply directly under the geometric decay ofα(k).
We now translate this into the asymptotic behavior of the CAPOPM poste-
rior.
Theorem 30(Consistency and Asymptotic Normality of CAPOPM Posterior
Under Dependence).Under (A7)–(A9), suppose that for a sequence of markets
withnorders we use the adjusted counts
y∗
n =
nX
t=1
Zt +δ †
+, n ∗
n =
nX
t=1
wbeh(xt;ψ †) +δ †
+ +δ †
−,
withw beh(xt;ψ †)∈[c w,1]for somec w >0. Letpdenote the event probability
and assume a Beta hybrid prior
p∼Beta(α 0, β0), α 0, β0 >0,
independent of(Z t). Then:
(a) (Posterior Consistency) LetΠ n(·)denote the CAPOPM posterior for
pbased on(y ∗
n, n∗
n). For anyϵ >0,
Πn
 
|p−p true|> ϵ
 P
− →0asn→ ∞.
93

(b) (Asymptotic Normality of Posterior Mean) Letˆp n be the posterior
mean underΠ n. Then
√n
 
ˆpn −p true
 d
− → N
 
0, V(ptrue)

,
for some finiteV(p true)determined byτ 2(ptrue)and the weight structure.
(c) (Bernstein–von Mises Approximation) The posterior distributionΠ n
is asymptotically normal in the sense that
sup
A⊂R
Πn
 √n(p−p true)∈A

− N
 
0, V(ptrue)

(A)

P
− →0.
Proof (Sketch).The effective sample sizen ∗
n grows linearly withndue to the
lower boundw beh(xt;ψ †)≥c w >0. The adjusted mean
¯Z∗
n = y∗
n −δ †
+
n∗n −δ †
+ −δ †
−
is a smooth function of ¯Zn and the average weight, so the LLN and CLT from
the lemma transfer to ¯Z∗
n via the delta method. In particular, ¯Z∗
n →µ(p true)
and √n( ¯Z∗
n −µ(p true))
d
− → N(0,˜τ2(ptrue)).
The Beta posterior based on (y ∗
n, n∗
n) has mean and variance that can be
written as smooth functions of ( ¯Z∗
n, n∗
n). Asn ∗
n ∼c nfor somec >0, stan-
dard Bayesian asymptotics for one-dimensional parameters under weak depen-
dence (together with the mixing LLN/CLT) yield posterior consistency and a
Bernstein–von Mises type result. The asymptotic varianceV(p true) incorpo-
rates both the intrinsic variance ofZ t and the effect of dependence through the
long-run variance ˜τ2(ptrue).
Remark 23(Small Samples and Long-Shot Regimes).The results above are
asymptotic in nature and require the effective sample sizen ∗
n to grow without
bound. In low-liquidity markets (smalln) or extreme-probability regimes (long-
shot events withp true near0or1), the Beta posterior can be highly skewed
and heavy-tailed, and the normal approximations may be poor. In such regimes,
CAPOPM should be used with caution, relying on full posterior credible intervals
rather than Gaussian approximations, and sensitivity analysis with respect to the
prior and adjustment parameters is particularly important.
Proposition 18(Finite-Sample Concentration under Mixing).Let(Z t)be the
adjusted YES process satisfying the boundedness and geometricα-mixing condi-
tions of Phase 8. Let
¯Zn = 1
n
nX
t=1
Zt, µ=E[Z t].
94

Then, for allx >0andn≥1, there exist constantsC 1, C2 >0(depending on
the mixing coefficients and bounds onZ t) such that
P
 
| ¯Zn −µ|> x

≤C 1 exp

−C2nx2

,
i.e. a Bernstein-type exponential inequality holds for the empirical average. In
particular, for any confidence levelδ∈(0,1), with probability at least1−δ,
| ¯Zn −µ|≲
r
log(1/δ)
n ,
up to constants depending on the mixing structure.
Remark 24(Practical Implications in Low-nand Long-Shot Regimes).The
concentration bound above is asymptotic in spirit: it guarantees that, for moder-
atenand weak dependence, ¯Zn will concentrate aroundµat a rate comparable
to the i.i.d. case. However, in low-liquidity markets (smalln), or whenpis very
close to0or1, three issues arise:
•The constantsC 1, C2 may be unfavorable, leading to loose finite-sample
bounds.
•The Beta posterior can be highly skewed, so Gaussian approximations to
credible intervals may be misleading.
•Long-shot events make the empirical process more volatile relative to the
natural scale ofp, further weakening normal approximations.
In these situations, CAPOPM posteriors should be interpreted via full credible
intervals and sensitivity checks, rather than relying solely on asymptotic nor-
mality or point estimates.
8.15 Metric–Based Robustness for Nonlinear Posterior Up-
dates
The original Lipschitz robustness results in this phase implicitly assumed a
linear or affine adjustment of pseudo–counts, so that the posterior mapping
from data to distribution was essentially linear. Once nonlinear distortions,
mixture posteriors, and dynamic bias layers are introduced (Sections 1.7, 1.7,
1.7), the adjustment of the posterior can no longer be modeled as a simple
additive transformation. This subsection replaces the linear Lipschitz arguments
with metric–based robustness results formulated in Wasserstein and Hellinger
distances for general nonlinear updates.
Setup.Let Θ = [0,1] denote the parameter space for the event probabilityp.
For a given datasetD, let Π base(· | D) be thebaseCAPOPM posterior overp,
constructed as in Phases 5–6 (e.g. a mixture of Beta components calibrated via
95

stacking and dynamic bias layers). We model nonlinear bias correction at the
posterior level via a measurable map
ϕ: Θ→Θ,
which acts onpto produce an adjusted parameterϕ(p). 2
Definition 9(Nonlinear Posterior Update via Pushforward).For a given dataset
Dand base posteriorΠ base(· | D), thenonlinearly adjustedposterior is defined
as the pushforward measure
Πϕ(· | D) :=ϕ#Πbase(· | D),
i.e. for any Borel setA⊆Θ,
Πϕ(A| D) = Πbase
 
ϕ−1(A)| D

.
We are interested in how sensitive Π ϕ(· | D) is to small changes inD, mea-
sured in appropriate probability metrics (e.g. WassersteinW 1 or Hellinger dis-
tanced H).
Assumption 7(Base Posterior Robustness in Wasserstein Distance).There
exists a data metricd D on the space of datasets such that for any two datasets
D,D ′,
W1(Πbase(· | D),Πbase(· | D′))≤C base dD(D,D ′),
for some constantC base <∞. HereW 1 is the1–Wasserstein distance onΘ
with respect to the Euclidean metric.
Assumption 7 is a metric formulation of the robustness results established
earlier (e.g. those analogous to Theorem 13 and Theorem 28), expressed at the
level of the base posterior Π base.
Assumption 8(Lipschitz Nonlinear Adjustment).The nonlinear adjustment
mapϕ: Θ→Θis globally Lipschitz with constantL ϕ <∞:
|ϕ(p1)−ϕ(p 2)| ≤Lϕ|p1 −p 2|for allp 1, p2 ∈Θ.
Theorem 31(Wasserstein Robustness of Nonlinear Posterior Updates).Under
Assumptions 7 and 8, the adjusted posterior mappingD 7→Πϕ(· | D)is Lipschitz
inW 1:
W1(Πϕ(· | D),Πϕ(· | D′))≤L ϕ Cbase dD(D,D ′)for allD,D ′.
Proof.By Definition 9, Π ϕ(· | D) =ϕ#Πbase(· | D). The 1–Wasserstein distance
is contracting under Lipschitz maps: for any two measuresµ, νon Θ and any
L–Lipschitz mapϕ,
W1(ϕ#µ, ϕ#ν)≤L W1(µ, ν).
2In applications,ϕmay depend on additional covariates or summaries ofD; here we treat
these as fixed when conditioning onD.
96

Applying this withµ= Π base(· | D),ν= Πbase(· | D′), andL=L ϕ, we get
W1(Πϕ(· | D),Πϕ(· | D′))≤L ϕ W1(Πbase(· | D),Πbase(· | D′)).
Combining this with Assumption 7 yields the claimed bound.
Remark 25(Lipschitz Regularization in Calibration).Theorem 31 shows that
the robustness constant for the adjusted posterior is the productL ϕCbase. Thus,
in calibratingϕ(e.g. via empirical risk minimization on historical markets), it is
natural to penalize large Lipschitz constants. A practical approach is to include
a regularization term of the form
λLip(ϕ)orλ∥∇ϕ∥ L2 ,
in the calibration objective, trading off fit against robustness. This yields an
explicit statistical justification for Lipschitz regularization of nonlinear posterior
updates.
Extension to Hellinger Distance.In some arguments, it is convenient
to work with Hellinger distanced H between posterior densities. For one–
dimensional models with sufficiently smooth densities and strictly monotoneϕ,
we can relate Hellinger distances before and after the nonlinear transformation.
Assumption 9(Smooth Monotone Nonlinear Adjustment).Assumeϕ: (0,1)→
(0,1)is aC 1 diffeomorphism with derivative bounded away from0and∞:
0< m≤ϕ ′(p)≤M <∞for allp∈(0,1).
Letπ base(p| D)andπ base(p| D′)denote posterior densities, andπ ϕ(q| D)the
density ofΠ ϕ(· | D)under the change of variablesq=ϕ(p).
Theorem 32(Hellinger Stability Under Smooth Monotone Transformations).
Under Assumption 9, there exists a constantC H =C H(m, M)such that for any
two datasetsD,D ′,
dH(Πϕ(· | D),Πϕ(· | D′))≤C H dH(Πbase(· | D),Πbase(· | D′)).
In particular, if the base posterior mapping is Hellinger–Lipschitz inD, then so
is the adjusted posterior mapping.
Proof Sketch.Under the change of variablesq=ϕ(p), densities transform via
πϕ(q| D) =πbase(ϕ−1(q)| D)
ϕ−1′
(q)
.
The Hellinger distance between the transformed densities is controlled by the
Hellinger distance between the original densities and the Jacobian factors, with
constants depending only on bounds forϕ ′ and (ϕ−1)′. Usingm≤ϕ ′(p)≤M
and standard change-of-variable bounds, one obtains the stated inequality with
CH depending only on (m, M).
97

Implications for Pricing Functionals.Letf: Θ→Rbe a 1–Lipschitz
payoff functional ofp(e.g.f(p) =1{p > K}smoothed, or a bounded Lipschitz
proxy for digital pricing). Then for any two datasetsD,D ′,

Z
f(p) Πϕ(dp| D)−
Z
f(p) Πϕ(dp| D′)
 ≤W 1(Πϕ(· | D),Πϕ(· | D′)).
Combining with Theorem 31 yields a bound on the sensitivity of CAPOPM
prices to data perturbations under nonlinear adjustments:
 ˆCϕ(D)− ˆCϕ(D′)
 ≤L ϕCbase dD(D,D ′),
for any Lipschitz payoff functionalfused in price computation.
Remark 26(Failure Modes and Relation to F2–F3).The robustness results
in this subsection rely critically on two properties: (i) the base posterior must
be stable in Wasserstein or Hellinger distance, and (ii) the nonlinear updateϕ
must be Lipschitz (or smooth monotone with bounded derivative). In the strong
herding and fast regime–switching regimes (Sections 1.9 and 1.9), condition (i)
fails: small changes in data can produce large changes in the base posterior.
In such regimes, no choice ofϕ(Lipschitz or otherwise) can restore uniform
robustness. Thus the metric–based Lipschitz results here apply to the “regular”
region of the model space where dependence and nonstationarity remain within
the bounds of the CAPOPM assumptions.
8.16 Failure of Robustness Under Strong Herding: A Thresh-
old Auto–Regressive Counterexample
This section establishes that the robustness guarantees of CAPOPM fail under
strong herding, understood as threshold–based majority–following behavior in
which order flow becomes nearly deterministic once the fraction of YES orders
crosses a critical threshold. In this regime, the dependence in the adjusted order
processZ t violates the mixing assumptions (A7)–(A9), the likelihood becomes
multimodal, and neither a single–Beta posterior nor any finite mixture of Be-
tas can recover the true event probability uniformly over the strong–herding
parameter region.
Definition 10(Threshold Auto–Regressive Herding Model).Let(Z t)t≥1 de-
note the effective YES–indicator process entering Stage 1 after behavioral weight-
ing. For parameters(θ, ρ)withθ∈(0,1)andρ∈[0,1], define
Zt+1 =



1with probabilityρif 1
t
tX
i=1
Zi ≥θ,
0with probabilityρif 1
t
tX
i=1
Zi < θ,
Ber(ptrue)with probability1−ρ.
98

Whenρis close to1, the process follows a majority rule as soon as the empirical
fraction of YES exceeds the thresholdθ, and otherwise follows a pure “NO–
cascade”. We call this thestrong–herding regime.
Lemma 15(Loss of Mixing Under Strong Herding).Fixθ∈(0,1)and let
ρ→1. Under the TAR model of Definition 10, the process(Z t)fails to satisfy
α–mixing with any summable mixing rate. In particular,
αZ(k)̸→0ask→ ∞
whenever the event{ 1
t
Pt
i=1 Zi ≥θ}occurs with positive probability.
Proof.Whenρ→1, the transition becomes
Zt+1 =1
(
1
t
tX
i=1
Zi ≥θ
)
a.s.,
so once the empirical mean crossesθ, the process becomes identically 1 there-
after. Similarly, if the empirical mean remains belowθ, the process becomes
identically 0. Hence (Z t) becomes asymptotically constant and perfectly pre-
dictable from events arbitrarily far in the past, implyingα Z(k) = 1 for allk.
Summability fails, completing the proof.
Lemma 16(Bimodal Likelihood Under Strong Herding).LetL n(p)denote the
likelihood ofp true based onZ 1, . . . , Zn under the TAR model. Ifρis sufficiently
close to1, then with positive probability
Ln(p)is asymptotically bimodal on[0,1],
with modes concentrated near0and1corresponding to NO–cascades and YES–
cascades respectively.
Proof.From Lemma 15, the process enters a deterministic regime once the
empirical mean crossesθ. If it crosses upward,Z t = 1 for all larget, which for
the Bernoulli likelihood behaves as ifp true = 1. If it crosses downward,Z t = 0
eventually, behaving as ifp true = 0. Since both cascades occur with positive
probability wheneverp true is not exactly equal toθ, the likelihood assigns mass
to neighborhoods of both 0 and 1. Bimodality follows.
Theorem 33(Impossibility of Uniform Consistency Under Strong Herding).
LetΠ CAPOPM(· | D1,D2)denote the Stage 2 posterior of CAPOPM, modeled as
a finite mixture of Beta distributions. Under the TAR herding model (Defini-
tion 10), the following statements hold.
(i) (No uniform consistency) For any finite mixture of Beta distributions with
R <∞components,
sup
ptrue∈(0,1)
Eptrue [|E[p|Z 1, . . . , Zn]−p true|]̸→0asρ→1.
99

(ii) (Failure of finite–mixture representation) The bimodality of the likelihood
(Lemma 16) implies that any finite mixture of Betas fails to approximate
the true posterior uniformly over the strong–herding region{ρ > ρ ∗}for
any fixedρ ∗ <1.
(iii) (Breakdown of robustness theorems) The Lipschitz–type continuity results
of Phase 8 cannot hold under strong herding: no constantC <∞can
satisfy
∥ΠCAPOPM(· | D)−ΠCAPOPM(· | D′)∥TV ≤C∥D − D′∥
for allρsufficiently close to1. The deterministic cascades in strong herd-
ing force the total variation distance to jump by1whenDcrosses the
thresholdθ.
Proof.(i) From Lemma 16, the likelihood assigns mass to neighborhoods of 0
and 1 with positive probability even whenptrue ∈(θ−ε, θ+ε). A finite mixture of
Betas cannot track both cascades simultaneously: its posterior mean necessarily
lies in a compact subinterval of (0,1) independent of the data configuration
producing the cascades. Thus, the posterior mean cannot converge uniformly
top true asρ→1, proving (i).
(ii) Any finite mixture of Betas has unimodal or mildly multimodal densities,
but cannot represent a sequence of likelihoods whose mass splits between 0 and
1 in a way that depends discontinuously on the data path. Therefore no finite
mixture can uniformly approximate the posterior, yielding (ii).
(iii) LetDandD ′ differ only by whether the empirical mean crossesθat
timet 0. Under strong herding, the posteriors collapse to neighborhoods of 0 and
1 respectively, giving total variation distance equal to 1. Since∥D −D′∥can be
arbitrarily small (e.g. one flip of a single Bernoulli), no Lipschitz constant can
satisfy the inequality uniformly. This proves (iii).
Remark 27(Interpretation).The strong–herding regime invalidates the data–
generating assumptions required for CAPOPM’s robustness theory. Determin-
istic cascades destroy mixing, induce bimodal likelihoods, and generate poste-
rior discontinuities that no finite–mixture Beta representation can smooth uni-
formly. Thus the impossibility theorem above provides a fundamental limita-
tion: CAPOPM can be consistent and Lipschitz–robust only on subregions of
the parameter space where dependence is sufficiently weak and mixing condi-
tions (A7)–(A9) hold.
8.17 Boundary Behavior, Long-Shot Events, and Stabiliza-
tion
The CAPOPM posterior for the event probabilitypis Beta with parameters
p| I ∼Beta(αn, βn), α n =α 0 +y ∗
n, β n =β 0 +n ∗
n −y ∗
n,
100

where (y ∗
n, n∗
n) are the adjusted counts and (α 0, β0) are the hybrid prior pa-
rameters. In long-shot regimes (p true near 0 or 1) or in very small samples, it
is possible for the posterior mass to concentrate near 0 or 1, leading to heavy
tails and numerical instability for functions ofp(e.g. log-odds or certain risk
measures).
We formalize this behavior and describe a simple stabilization based on either
truncation or a logit transform.
Lemma 17(Tail Behavior of Beta Posteriors Near the Boundaries).LetΠ n
denote theBeta(α n, βn)posterior forp.
(a) Ifα n ≤1, then the density ofΠ n behaves like
πn(p)∝p αn−1(1−p) βn−1 ∼p αn−1 asp↓0,
so that the left tail near0is heavy wheneverα n <1.
(b) Ifβ n ≤1, then
πn(p)∼(1−p) βn−1 asp↑1,
and the right tail near1is heavy wheneverβ n <1.
(c) Ifα n, βn ≥c >1uniformly inn, then there existsε >0such that
Πn
 
[0, ε)∪(1−ε,1]

≤Cε c,
for some constantC >0independent ofn. In particular, the posterior
places vanishing mass near0and1asε↓0.
Proof (Sketch).Parts (a) and (b) follow from the Beta density
πn(p) = 1
B(αn, βn) pαn−1(1−p) βn−1,
and standard asymptotics asp→0 andp→1. Whenα n <1, the factorp αn−1
diverges asp↓0, indicating a heavy left tail; an analogous statement holds for
βn <1 near 1.
For (c), ifα n, βn ≥c >1 andp∈(0, ε), then
πn(p)≤ 1
B(αn, βn)pc−1,
and integrating on (0, ε) gives
Πn([0, ε))≤C1εc
for someC 1 >0 that can be chosen uniformly inndue to the compactness of
the parameter region. A symmetric argument holds near 1.
101

The lemma shows that heavy tails near 0 and 1 arise precisely when the
posterior parametersα n andβ n are small, which can occur under three circum-
stances: (i) very small effective sample sizen ∗
n, (ii) extreme long-shot outcomes
(e.g. no YES orders in a rare-event market), or (iii) extremely concentrated or
misaligned priors.
To mitigate numerical instability and avoid overconfident long-shot posteri-
ors in these regimes, we consider a simple stabilized transform.
Proposition 19(Stabilized Posterior via Truncation or Logit Transform).Fix
a truncation parameterε∈(0,1/2)and define the truncated interval
Iε = [ε,1−ε].
LetΠ n be theBeta(α n, βn)posterior and define the truncated posteriorΠ ε
n by
Πε
n(A) = Πn(A∩I ε)
Πn(Iε) , A⊆[0,1]measurable,
wheneverΠ n(Iε)>0.
Then:
(a) Ifα n, βn ≥c >1uniformly inn, then
∥Πn −Π ε
n∥TV = Πn
 
[0, ε)∪(1−ε,1]

≤Cε c,
for someC >0independent ofn. Thus, for smallε, the truncated poste-
rior is close in total variation to the original posterior.
(b) Define the logit transform
θ= log p
1−p ,
and letΛ n be the induced posterior distribution forθunderΠ ε
n. Then
moments of all orders exist forΛ n, andΛ n is supported on a compact
interval
Θε =

log ε
1−ε ,log 1−ε
ε

.
Consequently, functionals ofpthat are Lipschitz inθare uniformly bounded
and well-behaved underΛ n.
(c) Ifε=ε n ↓0is chosen such thatε c
n →0andn ∗
n → ∞, then the truncated
posteriorΠ εn
n remains asymptotically equivalent toΠ n for inference about
ptrue while preventing extreme numerical instability near0and1at finite
n.
Proof (Sketch).For (a), observe that truncation only removes mass near 0 and
1, so the total variation distance equals the probability of the removed regions:
∥Πn −Π ε
n∥TV = Πn
 
[0, ε)∪(1−ε,1]

.
102

The bound then follows directly from part (c) of the lemma, with the same
exponentcand an adjusted constantC.
For (b), the truncation ensures thatp∈I ε almost surely under Π ε
n. The
logit mapp7→θ= log(p/(1−p)) sendsI ε to Θε, a compact interval inR, and is
smooth on (0,1). As a result, all moments ofθunder Λ n exist and are bounded
uniformly inn. Any functional ofpthat can be expressed as a Lipschitz function
ofθthus inherits uniform boundedness and stability.
For (c), ifε c
n →0, then the total variation distance between Π n and Πεn
n
tends to zero by (a). At the same time, the growing effective sample sizen ∗
n
drives the posterior mass towardsp true, and the truncation can be chosen small
enough that it does not distort the asymptotic concentration in typical cases
(wherep true ∈(0,1)). Hence the truncated posterior is asymptotically equiva-
lent to the original one for inference while improving finite-sample stability.
Remark 28(Practical Guidance for Long-Shot Markets).The analysis above
suggests a simple stabilization strategy for CAPOPM in long-shot or low-liquidity
markets:
•Choose a small truncation levelε(e.g.10 −4 or10 −3) and work with the
truncated posteriorΠ ε
n.
•When transforming probabilities (e.g. to log-odds), perform the transform
onθ= log(p/(1−p))underΛ n rather than directly onpnear the bound-
aries.
•Report both the truncated posterior summaries and the original Beta sum-
maries, especially in cases whereα n orβ n are close to1or below.
This keeps the asymptotic properties intact while explicitly addressing the finite-
sample instabilities associated with heavy Beta tails near0and1.
8.18 CAPOPM as a KL Projection: A Formal Information-
Theoretic Interpretation
The adjusted CAPOPM posterior for the event probabilitypis a Beta distribu-
tion of the form
p| I ∼Beta(αn, βn), α n =α 0 +y ∗
n, β n =β 0 +n ∗
n −y ∗
n,
constructed from the hybrid prior and adjusted parimutuel counts. Up to this
point the Beta form has been motivated by conjugacy and interpretability. Here
we show that it also admits a fundamentalinformation-theoretic characteriza-
tion: it is the KL-projection of a general posterior onto the Beta family.
Let Π ⋆ denote the (hypothetical) posterior distribution forpthat would
arise under a fully specified, potentially nonparametric data-generating model
with dependence, behavioral distortions, or heterogeneous signals. In a general
market this Π⋆ may not be Beta, and may not be computationally tractable.
103

CAPOPM provides a tractable Beta posterior. The following theorem shows
that the CAPOPM posterior coincides with theI-projection(information pro-
jection) of Π⋆ onto the Beta familyB={Beta(a, b) :a, b >0}.
Theorem 34(KL Projection Theorem for CAPOPM).LetΠ ⋆ be any posterior
distribution onp∈(0,1)with finite mean and finitelog-moment, and letBbe
the family of Beta distributions. Consider the KL divergence
DKL(Π⋆ ∥Beta(a, b)) =
Z 1
0
log dΠ⋆
dBeta(a, b)(p)dΠ ⋆(p),
defined wheneverΠ ⋆ is absolutely continuous with respect toBeta(a, b).
Define the KL-projection ofΠ ⋆ ontoBas
(a†, b†)∈arg min
a,b>0
DKL(Π⋆ ∥Beta(a, b)).
Assume thatΠ ⋆ has meanm ⋆ and inverse second momentM ⋆ =E Π⋆[p−1 +
(1−p) −1]<∞. Then:
(a) The minimizer(a †, b†)exists and is unique.
(b) The KL-projection satisfies
a†
a† +b † =m ⋆, a†b†
(a† +b †)2(a† +b † + 1) = VarΠ⋆(p),
i.e. the minimizing Beta distribution matches the mean and variance of
Π⋆.
(c) IfΠ ⋆ is generated by a hybrid prior and adjusted countsy∗
n, n∗
n (with mixing
or dependence), then the CAPOPM posteriorBeta(α n, βn)coincides with
the KL-projection:
(αn, βn) = (a†, b†).
(d) In particular, for large effective sample sizen ∗
n, CAPOPM selects, among
all Beta distributions, the one closest in KL sense to the ideal but in-
tractableΠ ⋆.
Proof (Sketch).Part (a) follows from strict convexity of the KL divergence in
(a, b) on the Beta family. For (b), writing the KL divergence explicitly and
differentiating under the integral yields two first-order conditions:
∂
∂a DKL = 0⇒E Π⋆[logp] =ψ(a)−ψ(a+b),
∂
∂b DKL = 0⇒E Π⋆[log(1−p)] =ψ(b)−ψ(a+b),
whereψis the digamma function. Using identities for Beta means and variances,
these conditions imply the matching of mean and variance stated in (b).
104

For (c), under a Beta–Binomial model (even with adjusted counts), the exact
posterior forpis Beta(α n, βn). Thus if Π ⋆ arises from such updating under a
fully specified likelihood, Π ⋆ is already inBand the unique KL minimizer is
precisely Beta(αn, βn).
When Π ⋆ is more general (due to dependence, behavioral structures, or
nonparametric components), the CAPOPM posterior can be interpreted as the
projection of Π ⋆ onto the Beta family using the adjusted empirical mean and
variance (m⋆,VarΠ⋆) induced by the CAPOPM adjustment rules. This identifies
(αn, βn) with the unique minimizer (a †, b†).
Remark 29(Interpretation and Novelty).This theorem provides an information-
theoretic justification for the CAPOPM posterior beyond conjugacy. Even when
the true posteriorΠ ⋆ is nonparametric or analytically intractable, CAPOPM
delivers theclosest Beta distribution in KL divergence.
Thus the Beta form is not merely a convenient algebraic choice, but the
information-projection that preserves the two most important moments of the
ideal posterior under the CAPOPM-adjusted signal process.
This interpretation also helps explain why the hybrid prior and adjustment
mechanism remain stable even under dependence or behavioral distortions: CAPOPM
selects the “least distorted” Beta posterior compatible with the adjusted empirical
information.
Remark 30(Limitations of the KL Projection View).Interpreting CAPOPM
as a KL projection onto the Beta family is useful but also restrictive. It guar-
antees that, among all Beta distributions, the chosen posterior preserves certain
moments of a more complex underlying posterior. However, it does not claim
that the Beta family is rich enough to capture all features of the true posterior
under strong dependence, multimodal priors, or adversarial behavior. In set-
tings where such features are important, the KL-projection perspective should be
viewed as an approximation tool rather than a full description of market beliefs,
and more flexible models (e.g. MCMC-based or variational) may be warranted.
8.19 Alternatives to Beta Conjugacy under Dependence
While CAPOPM deliberately uses the Beta–Binomial structure for tractability
and interpretability, other approaches can, in principle, accommodate richer
dependence at the cost of computational complexity:
•MCMC with dependent likelihoods.One can specify an explicit de-
pendent model for the order sequence, such as an Ising or Markov random
field for YES/NO decisions, and sample from the posterior forpvia Gibbs
or Metropolis–Hastings. This yields a more flexible posterior but requires
careful tuning and may be slow in large markets.
•Variational approximations.Variational Bayes can approximate com-
plex posteriors with factored or low-rank distributions, trading off accu-
racy for speed. In this setting, one could approximate the joint posterior
105

over (p,latent states) with a product of a Beta distribution forpand a
tractable family for the latent dependence structure.
•Composite likelihoods.Composite or pseudo-likelihood methods re-
place the full joint likelihood with products of low-dimensional marginals
or conditionals, offering a compromise between full MCMC and the single-
sufficient-statistic approach of CAPOPM.
CAPOPM chooses Beta conjugacy as a deliberate design decision: it pro-
vides closed-form updates, interpretable pseudo-counts, and an information-
projection interpretation, while recognizing that more flexible likelihood-based
methods are possible when computational resources and data volume permit.
8.20 Asymptotics Under Ergodic Regime Switching and an
Impossibility Result
The dynamic bias layer of Section 1.7 models (δ t, ψt) as a hidden Markov chain
(St) with a finite state spaceS={1, . . . , R}and regime–specific parameters
(δr, ψr). This section establishes (i) a Bernstein–von Mises–type result under
ergodic regime switching (mild nonstationarity), and (ii) an impossibility result
when regimes switch so quickly that no regime accumulates enough information
for consistent learning.
Assumption 10(Ergodic Regime Switching and Regularity).Let(S t)t≥1 be
an irreducible, aperiodic Markov chain onS={1, . . . , R}with transition matrix
Pand unique stationary distributionπ= (π 1, . . . , πR). Assume:
(i) (Ergodicity) For eachr, s∈ Sthere existsk≥1such that(P k)rs >0,
and the chain is aperiodic. Consequently, for any initial distribution,
P(St =r)→π r ast→ ∞.
(ii) (True parameter vector) Each regimerhas a true parameterθ ⋆
r = (δ⋆
r , ψ⋆
r )
in a compact subsetΘ r ⊂R dr , and the emission modelp(D t |S t =r, θ⋆
r)
coincides with the Stage 1/Stage 2 correction structure of CAPOPM.
(iii) (Identifiability and smoothness) The mappingθ r 7→p(D t |S t =r, θr)
is identifiable andC 2 in a neighborhood ofθ ⋆
r, with Fisher information
matrixI r(θ⋆
r)positive definite.
(iv) (Finite moments) The emission log-likelihoods have finite second moments
under the true model.
We are interested in a smooth functional of the regime parameters, such as
the long–run average bias
¯δ⋆ :=
RX
r=1
πr δ⋆
r ,
or more generally a differentiable functionalφ(θ 1, . . . , θR) withφ: Θ 1 × ··· ×
ΘR →R.
106

Theorem 35(Bernstein–von Mises Theorem under Ergodic Regime Switch-
ing).Under Assumption 10, suppose the CAPOPM posterior over(θ 1, . . . , θR)
is proper and assigns positive prior density in a neighborhood of the true param-
eter vector(θ ⋆
1, . . . , θ⋆
R). Let
ˆθr(T) (r= 1, . . . , R)
denote the (quasi–)maximum likelihood or posterior mean estimator ofθ r based
on dataD 1:T = (D1, . . . ,DT ), and defineˆφ(T) :=φ( ˆθ1(T), . . . ,ˆθR(T)). Then,
asT→ ∞,
(i) (Consistency)
ˆφ(T)
P
− →φ(θ⋆
1, . . . , θ⋆
R).
(ii) (Asymptotic normality)
√
T

ˆφ(T)−φ(θ⋆
1, . . . , θ⋆
R)
 d
− → N(0, Vφ),
whereV φ is the asymptotic variance obtained by the delta method applied
to the joint asymptotic distribution of( ˆθ1(T), . . . ,ˆθR(T)).
(iii) (Bernstein–von Mises) The posterior distribution ofφ(θ 1, . . . , θR)given
D1:T converges in total variation to the normal lawN( ˆφ(T), Vφ/T):
Π
 
φ(θ1, . . . , θR)∈ ·
D1:T

− N
 
ˆφ(T), Vφ/T

TV
T→∞
− − − − →0.
Proof Sketch.Under Assumption 10, the hidden Markov model (S t,Dt) is an
ergodic HMM with finite state space and regular parametric emission densities.
Standard results for HMMs imply that the (quasi–)maximum likelihood esti-
mator of eachθ r is consistent and asymptotically normal, with an information
matrix determined by long–run frequenciesπ r and the per–regime Fisher infor-
mationI r(θ⋆
r). The joint asymptotic normality of (ˆθ1(T), . . . ,ˆθR(T)) then yields
asymptotic normality of ˆφ(T) via the delta method. The Bayesian Bernstein–
von Mises conclusion follows from general BvM theorems for HMMs with finite
state space and regular parametric families, together with the prior positivity
condition. Full details follow the usual template for BvM in dependent data
models; we omit these for brevity.
Remark 31(Application to Dynamic CAPOPM).In the dynamic CAPOPM
setting of Section 1.7, the regime–specific parametersθ ⋆
r encode bias corrections
(δ⋆
r , ψ⋆
r )and possibly additional structural quantities. The functionalφmay be
taken as the long–run average distortion ¯δ⋆ = P
r πrδ⋆
r , or as a mapping from
the collection(θ ⋆
r)to a long–run effective event probabilityp ⋆
eff under dynamic
corrections. Theorem 35 then ensures that the CAPOPM posterior for ¯δ(orp eff)
concentrates and becomes asymptotically normal despite mild nonstationarity
induced by regime switching.
107

We now show that this positive result has a natural limit: if regime switching
is too fast for any regime to accumulate information, no sequential estimator or
posterior can remain uniformly well calibrated.
Assumption 11(Fast Regime Switching with Vanishing Dwell Time).Let(S t)
be a Markov chain onSwith transition matrixPdepending onTsuch that:
(i) The minimum expected dwell time in each state is uniformly bounded:
sup
T
max
r∈S
ET

time spent in staterup toT

< C <∞.
(ii) The regime–specific parametersθ ⋆
r(T)are allowed to vary withT, remain-
ing in a compact set, and emissions are generated fromp(D t |S t, θ⋆
St(T)).
In words, regimes switch so frequently that the average number of visits to any
given state does not grow withT.
Theorem 36(Impossibility of Uniform Consistency Under Fast Regime Switch-
ing).Under Assumption 11, consider any sequential estimator sequence ˆθ(T)
or any Bayesian posterior sequenceΠ T (· | D1:T )for a regime–dependent pa-
rameter functionalφ(θ ⋆
1(T), . . . , θ⋆
R(T))(e.g. a regime–specific biasδ ⋆
r (T)or a
regime–specific event probabilityp⋆
r(T)). Then there exists a choice of parameter
sequences{θ ⋆
r(T)}such that
lim sup
T→∞
sup
{θ⋆r(T)}
E
hˆφ(T)−φ(θ⋆
1(T), . . . , θ⋆
R(T))

i
>0,
and similarly, no sequence of posteriorsΠ T can concentrate around the true
functional uniformly over{θ ⋆
r(T)}. In particular, there is no uniformly consis-
tent dynamic CAPOPM estimator or posterior in this fast switching regime.
Proof Sketch.Under Assumption 11, the expected number of observations gen-
erated in any fixed regimerup to timeTis uniformly bounded byC. Therefore,
for any estimator or posterior targeting a regime–specific parameter (or a func-
tional that depends nontrivially on the per–regime values), the effective sample
size per regime does not grow withT. By standard parametric lower bounds,
no estimator can achieve vanishing risk uniformly over the parameter sequences
{θ⋆
r(T)}when each regime is observed onlyO(1) times. One can construct
pairs of parameter sequences that are indistinguishable from the data but in-
duce separated values of the functionalφ, forcing a nonzero lower bound on
the estimation error. The same argument applies to Bayesian posteriors: with
bounded regime information, the posterior cannot concentrate uniformly on the
true functional.
Remark 32(Interpretation for Dynamic CAPOPM).Theorem 35 and Theo-
rem 36 identify afeasibleand aninfeasiblenonstationary regime for CAPOPM.
Under ergodic regime switching with growing effective sample size per regime,
dynamic CAPOPM remains consistent and asymptotically normal for smooth
108

functionals of the regime parameters. When regime switching becomes so fast
that no regime accumulates information, no sequential estimator or posterior
can be uniformly well calibrated. In practice, CAPOPM should therefore treat
fast, high–frequency structural breaks as a regime where the bias layer and pos-
terior must be explicitly flagged as fragile, and any pricing output should carry
a warning about nonstationary uncertainty that cannot be statistically resolved.
8.21 Summary
Phase 8 demonstrates that the CAPOPM framework yields:
•posterior consistency across structural, ML, behavioral, and adversarial
regimes;
•asymptotic normality of the posterior distribution;
•robustness to behavioral biases and structural distortions;
•preservation of arbitrage–freeness; and
•stability of the posterior under small perturbations.
These results complete the theoretical foundation of CAPOPM.
109

Ethical and Computational Considerations
Although CAPOPM is primarily a statistical and market-microstructure frame-
work, its practical deployment raises two considerations that fall outside the
purely mathematical scope: (i) ethical issues related to information interpreta-
tion and trader impact, and (ii) computational scalability in high-frequency or
large-market environments.
Ethical considerations.The CAPOPM adjustment mechanism is designed
to extract latent information from market behavior, not to infer personal traits
or to profile individual traders. All behavioral weights and offsets operate on
aggregated orders or coarse feature categories (such as order size, time of sub-
mission, or broad trader cohorts). In typical parimutuel settings, these inputs
are public, event-level signals rather than personally identifying data.
Two ethical concerns still warrant attention:
•Misinterpretation of crowd behavior.Behavioral adjustments such
as herding corrections, long-shot bias offsets, or systematic weight mod-
ulation must be disclosed transparently. CAPOPM does not judge the
rationality of individual traders; it merely models aggregate effects. Mis-
representing these adjustments as normative statements about trader com-
petence should be avoided.
•Incentive effects.If CAPOPM were used in a live market to set prices
or guide payouts, the knowledge of the adjustment mechanism might influ-
ence trader strategies. The framework is therefore intended as a research
and belief-elicitation tool rather than a prescriptive mechanism for market
design or individual decision-making.
These considerations do not modify the mathematics of the model but help
clarify its appropriate scope.
Computational scalability.In its simplest form, CAPOPM requires main-
taining weighted sums
y∗
n =
nX
t=1
Zt +δ +, n ∗
n =
nX
t=1
w(xt;ψ) +δ + +δ −,
which can be updated inO(1) time per order. However, certain extensions—
notably those involving dynamic behavioral weights or dependence structures
that require recalculating pairwise interactions—may scale asO(n) per update.
Over a long sequence of orders, this can become burdensome.
A few practical strategies mitigate this issue:
•Streaming updates.Most CAPOPM quantities reduce to running sums
or averages. Whenw(x;ψ) is fixed within a market, updates remainO(1).
110

•Batch weighting.If behavioral weights depend on slowly varying fea-
tures (e.g. time of day or volatility regime), weights can be precomputed
for batches, reducing per-order cost.
•Sparse interaction structures.In dependence models such as the Ising-
type herding formulation, restricting the trader graph to a bounded-degree
or sparse structure ensures that the effective cost of updating conditional
probabilities remains manageable.
•Approximate inference.Whennis very large, the limiting results
of Phase 8 (LLN/CLT) support the use of approximate moment-based
updates rather than exact counts, allowing amortizedO(1) updates.
In summary, the core CAPOPM update remains computationally inexpen-
sive, while more elaborate behavioral or dependence components require thought-
ful implementation to avoid unnecessary overhead. Nothing in the framework
requires storing all historical orders or performing pairwise computations across
traders.
Remark 33(Scope).The ethical and computational considerations presented
here serve as practical guidance rather than limitations of the theory. They
outline appropriate use cases and clarify that CAPOPM is best viewed as a
research tool for structured belief elicitation, not a real-time trading protocol.
111

Conclusion and Future Directions
This paper has developed CAPOPM, a crowd-adjusted parimutuel option pric-
ing mechanism designed to extract beliefs from a combination of structural
financial modeling, machine learning signals, and observed parimutuel order
flow. The framework is organized into eight phases that separate structural
assumptions, prior construction, behavioral and structural correction, Bayesian
updating, and theoretical evaluation. The intention is not to assert that any
single component of the model is definitive, but rather to provide a structured
setting in which these components can be examined individually and in combi-
nation.
A key feature of the approach is the construction of a hybrid prior that
integrates a tempered fractional Heston model with neural-network-based prob-
ability estimates. This combination leverages the interpretability and market-
consistent properties of the structural approach while allowing data-driven mod-
els to influence the prior when their predictive quality is high. The parimutuel
order flow then acts as a sequence of informational signals, updating the hybrid
prior through a Beta–Binomial mechanism. Behavioral and structural correc-
tions are incorporated through a two-stage adjustment that preserves conjugacy
and provides analytic clarity when analyzing robustness, sensitivity, and asymp-
totic behavior.
The theoretical analysis in Phase 8 shows that, under reasonable conditions
on trader populations and bounded behavioral distortions, the adjusted pos-
terior is both robust and consistent. Extensions of the robustness results to
variance and Hellinger distance ensure that the entire posterior distribution re-
mains stable under small perturbations in the adjusted counts. A CLT-based
argument establishes that, as the effective sample size grows, the posterior be-
comes asymptotically normal and concentrates near the true event probability.
These results are not presented as definitive guarantees for all market settings,
but as formal properties that hold within the controlled environment defined in
the model.
There are several natural directions for further development. First, the struc-
tural offsets used in the bias-correction layer could be estimated via an empirical
Bayes procedure, using historical panels of markets to infer the systematic com-
ponent of crowd miscalibration. This would provide a more principled basis
for the correction terms while maintaining the closed-form posterior structure.
Second, the behavioral weights applied to order flow could be generalized to
incorporate time-varying or kernel-based decay, possibly using the same tem-
pered fractional kernels that appear in the Heston variance dynamics. Such
an extension would more closely align the informational and structural memory
components of the model and create a unified kernel-based approach to volatility
and information flow.
Third, the likelihood specification could be extended to admit correlated or
exchangeable sequences of trades, replacing the independent Bernoulli model
with an overdispersed or block-correlated alternative. This would allow the
model to more directly represent herding, clustered trading, and contagion ef-
112

fects. In conjunction with the empirical Bayes approach described above, such a
specification could be evaluated using hierarchical or mixed-effects models that
incorporate trader heterogeneity at a more granular level.
Fourth, the machine learning component could be expanded beyond point
estimates ofp ML to incorporate uncertainty quantification, such as Bayesian
neural networks or ensemble variance. This would allow the hybrid prior to
more flexibly represent model uncertainty rather than treatingp ML as purely
deterministic.
Finally, an applied implementation would necessarily involve empirical val-
idation. While real-world data analysis lies outside the scope of this paper,
the eight-phase design is intended to make such studies modular. Researchers
can select specific phases to refine, test, or replace, while preserving the over-
all Bayesian updating structure. For example, empirical studies could examine
how different trader populations affect posterior concentration, how severe herd-
ing impacts inference, or how the hybrid prior adapts to markets with rapidly
changing volatility.
Taken together, these observations highlight that CAPOPM should be viewed
as a framework rather than a fixed model. Its value lies in the organization
of structural, behavioral, and machine learning components into a coherent
Bayesian mechanism for belief extraction in a parimutuel environment. The
results presented here establish theoretical foundations upon which empirical
studies and more detailed structural extensions can be developed.
113

Glossary of Symbols and Notation
This glossary summarizes the notation used throughout the CAPOPM frame-
work, including the structural model, hybrid prior, behavioral adjustments, and
asymptotic results. Symbols are defined once and used consistently in all phases.
A. Probability, Events, and Measures
•Ω,F,(F t)t≥0 — underlying filtered probability space.
•Q— risk-neutral measure.
•P— physical (real-world) measure when needed.
•A={S T > K}— CAPOPM event (“YES event”).
•θ∈ {0,1}— binary state of nature (Koessler–Noussair convention).
•p=Q(S T > K) — digital event probability.
•p true — true event probability under the data-generating process.
•p 0 — CAPOPM hybrid prior mean.
B. Asset Price and Structural Model Parameters
•S t — asset price process.
•V t — variance process.
•(W t, Bt) — correlated Brownian motions withρ∈[−1,1].
•γ— mean-reversion intensity in the variance equation.
•θ(Heston) — long-run variance level (not to be confused with event state).
•σ— volatility-of-volatility.
•α∈(1/2,1) — fractional exponent in the tempered fractional kernel.
•λ≥0 — tempering parameter.
•K α,λ(t−s) =e −λ(t−s)(t−s) α−1 — Volterra kernel.
•h(t), g(t) — Riccati–Volterra solution pair (Shi).
•Φ(u;T) — characteristic function of lnS T .
•f Θ,α(s;T) — structural risk-neutral density ofS T .
•q Shi(K, T; Θ, α) — structural digital tail probability.
114

C. Machine-Learning Prior and Hybrid Prior
•x— feature vector (ANN) or input sequence (RNN).
•g NN(x) — ML model output estimatingp.
•p ML =g NN(x) — ML-implied event probability.
•n ML — pseudo-sample size for ML prior (performance-weighted).
•η str — structural prior strength.
•η=η str +n ML — effective hybrid prior mass.
•α 0 =ηp 0,β 0 =η(1−p 0) — hybrid Beta prior parameters.
D. Parimutuel Market Quantities
•s i ∈ {YES,NO}— traderi’s action.
•N— total number of traders (market size).
•Π N — empirical YES fraction in market of sizeN.
•π N — parimutuel odds derived from Π N .
•Φ(·) — odds mapping (aggregate-to-price function).
•π ∗ — fixed-point odds in the large-market limit.
E. Behavioral Adjustments (Phase 5–6)
•w beh
i =w(x i;ψ) — behavioral weight for orderibased on featuresx i and
parametersψ.
•ψ— parameter vector governing the behavioral weighting function.
•δ +,δ − — structural offsets correcting systematic directional bias.
•δ= (δ +, δ−) — combined structural adjustment vector.
• ˆψM , ˆδM — Empirical Bayes estimates fromMhistorical markets.
•Z t — adjusted YES contribution at time/ordert.
•y ∗
n — adjusted YES pseudo-count, including offsets.
•n ∗
n — adjusted total pseudo-count.
115

F. Posterior Quantities
•Π n — posterior distribution forpgivennadjusted observations.
•α n =α 0 +y ∗
n — posterior YES parameter.
•β n =β 0 +n ∗
n −y ∗
n — posterior NO parameter.
•ˆpn — posterior mean.
•σ 2
n — posterior variance.
•Π ε
n — truncated posterior (boundary-stabilized).
•θ= log(p/(1−p)) — logit-transformed probability.
G. Dependence, Mixing, and Asymptotics
•Z t — adjusted YES process (bounded).
•α(k) —α-mixing coefficient of the dependent sequence (Z t).
•C, ρ— constants for geometric mixing:α(k)≤Cρ k.
• ¯Zn — empirical average of adjusted indicators.
•µ(p true) — mean ofZ t under true probabilityp true.
•τ 2(ptrue) — long-run variance for CLT under dependence.
•V(p true) — asymptotic variance for posterior mean (Bernstein–von Mises).
H. Option Pricing and Pricing Kernel
•D CAP(K, T) =e−rT ˆp(K, T) — CAPOPM-implied digital price.
•F Q
CAP(K, T) — CAPOPM-implied risk-neutral CDF.
•f Q
CAP(s, T) — CAPOPM-implied risk-neutral density.
•C CAP(K, T) — CAPOPM-implied call price.
•ϕ CAP(s, T) =e−rT fQ
CAP(s, T) — state-price density.
•m CAP(s, T)∝f Q
CAP(s, T)/fP (s) — pricing kernel whenf P is known.
116

References
[1] Boris S. Axelrod, Ben J. Kulick, Charles R. Plott, and Kevin A. Roust. The
design of improved parimutuel-type information aggregation mechanisms:
Inaccuracies and the long-shot bias as disequilibrium phenomena.Journal
of Economic Behavior & Organization, 69:170–181, 2009.
[2] Mikkel Bennedsen, Asger Lunde, and Mikko S. Pakkanen. Decoupling the
short- and long-term behavior of stochastic volatility.Journal of Financial
Econometrics, 15(2):233–259, 2017.
[3] James O. Berger.Statistical Decision Theory and Bayesian Analysis.
Springer, New York, 2 edition, 1985.
[4] Olivier Capp´ e, Eric Moulines, and Tobias Ryd´ en.Inference in Hidden
Markov Models. Springer, New York, 2005.
[5] Kai Diethelm.The Analysis of Fractional Differential Equations, volume
2004 ofLecture Notes in Mathematics. Springer, Berlin, 2004.
[6] Paul Doukhan.Mixing: Properties and Examples, volume 85 ofLecture
Notes in Statistics. Springer, New York, 1994.
[7] Angela Maria D’Uggento, Marta Biancardi, and Domenico Ciriello. Pre-
dicting option prices: From the black-scholes model to machine learning
methods.Big Data Research, 40:100518, 2025.
[8] Bradley Efron and Robert J. Tibshirani.An Introduction to the Bootstrap.
Chapman and Hall, New York, 1994.
[9] Omar El Euch and Mathieu Rosenbaum. The characteristic function of
rough heston models.Mathematical Finance, 29(1):3–38, 2019.
[10] Ken-Ichi Funahashi and Yuichi Nakamura. Approximation of dynamical
systems by continuous time recurrent neural networks.Neural Networks,
6(6):801–806, 1993.
[11] Yarin Gal and Zoubin Ghahramani. Dropout as a bayesian approximation:
Representing model uncertainty in deep learning. InProceedings of the 33rd
International Conference on Machine Learning, volume 48, pages 1050–
1059. PMLR, 2016.
[12] Hans U. Gerber and Elias S. Shiu. Option pricing by esscher transforms.
Transactions of the Society of Actuaries, 46:99–191, 1994.
[13] Subhashis Ghosal and Aad W. van der Vaart.Fundamentals of Nonpara-
metric Bayesian Inference. Cambridge University Press, Cambridge, 2017.
[14] Kurt Hornik. Approximation capabilities of multilayer feedforward net-
works.Neural Networks, 4(2):251–257, 1991.
117

[15] Emad Jaber, Michael Larsson, and Mathieu Rosenbaum. Affine volterra
processes.The Annals of Applied Probability, 29(5):3155–3200, 2019.
[16] Ioannis Karatzas and Steven E. Shreve.Brownian Motion and Stochastic
Calculus, volume 113 ofGraduate Texts in Mathematics. Springer, New
York, 1991.
[17] Fr´ ed´ eric Koessler, Charles Noussair, and Anthony Ziegelmeyer. Informa-
tion aggregation and belief elicitation in experimental parimutuel betting
markets.Journal of Economic Behavior & Organization, 83:195–208, 2012.
[18] Alan L. Lewis.Option Valuation under Stochastic Volatility. Finance Press,
Newport Beach, CA, 2000.
[19] David Pollard.A User’s Guide to Measure Theoretic Probability. Cam-
bridge University Press, Cambridge, 2001.
[20] Herbert Robbins. An empirical bayes approach to statistics.Proceedings of
the Third Berkeley Symposium on Mathematical Statistics and Probability,
1:157–163, 1956.
[21] Zhengguang Shi. Option pricing for heston model with tempered fractional
brownian motion.Results in Applied Mathematics, 27:100623, 2025.
[22] Aad W. van der Vaart.Asymptotic Statistics. Cambridge University Press,
Cambridge, 1998.
[23] Cristiano Varin, Nancy Reid, and David Firth. An overview of composite
likelihood methods.Statistica Sinica, 21(1):5–42, 2011.
[24] Yuling Yao, Aki Vehtari, Daniel Simpson, and Andrew Gelman. Using
stacking to average bayesian predictive distributions.Bayesian Analysis,
13(3):917–1003, 2018.
[25] Bernt Øksendal.Stochastic Differential Equations: An Introduction with
Applications. Springer, Berlin, 6 edition, 2003.
118