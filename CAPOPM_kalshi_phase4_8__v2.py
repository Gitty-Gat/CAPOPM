"""
CAPOPM_kalshi_phase4_8.py
========================

This script implements Phases 4–8 of the CAPOPM (Crowd‑Adjusted Parimutuel
Option Pricing Mechanism) workflow for markets on the Kalshi exchange.  It
takes as input:

* ``kalshi_capopm_events.csv`` – event‑level data produced from the Kalshi API,
  containing market metadata (strike intervals, times, outcomes) and aggregated
  ticket counts.
* ``orderbook_data.csv`` – order book snapshots for each market (as
  generated by ``kalshi_api_fetch.py``).
* ``trades_data.csv`` – full trade history for each market (as generated by
  ``kalshi_api_fetch.py``).

It produces crowd‑adjusted posterior probabilities for each market through
several steps:

1. **Feature Engineering** – Microstructure features are extracted from order
   book and trade data.  These include depth, best price, order imbalance,
   trade counts, volume, price statistics, and time-to-expiry.  The features
   are merged with event‑level data.
2. **Model Selection (Phase 4)** – Several machine learning classifiers
   (logistic regression, random forest, gradient boosting, XGBoost if
   available) are compared via cross‑validated Brier score.  The best model
   predicts bucket win probabilities which form the ML prior.
3. **Structural Prior (Phase 4)** – An approximate tempered fractional
   Heston fractional Brownian motion prior is implemented via a grid search
   over scaling exponent ``alpha`` and volatility scale ``sigma``.  The
   structural prior computes the probability that the underlying index ends
   within each strike interval, assuming log returns follow a normal
   distribution whose variance scales as ``(T*1_year)**alpha`` and whose mean
   is inferred from winning buckets.  Parameter constraints on ``alpha`` and
   ``sigma`` are inspired by the admissible parameter set for the tempered
   fractional Heston model【205128327162733†L404-L407】.
4. **Hybrid Prior (Phase 4)** – The structural and ML priors are fused
   through reliability weighting: the inverse Brier scores on the training
   set determine how much weight each component receives.  The hybrid
   probability ``q_hybrid`` is converted to Beta prior pseudo‑counts
   ``alpha0`` and ``beta0`` using a user‑specified ``ETA_TOTAL``.
5. **Posterior Update (Phases 5–6)** – Crowd ticket counts are corrected
   behaviorally (mapping market price to effective probability) and
   structurally (liquidity scaling).  The corrected counts are added to the
   Beta prior to form the posterior Beta distribution.  Posterior means are
   computed both with and without corrections.
6. **Evaluation (Phase 7)** – Brier score, log loss, and calibration are
   computed on the training markets to assess the performance of Kalshi
   prices, the ML prior, the structural prior, and the CAPOPM posteriors.
7. **Prediction (Phase 8)** – Posterior probabilities are produced for all
   markets, including those that are still open (test set).

To run the script::

    python CAPOPM_kalshi_phase4_8.py \
        --events kalshi_capopm_events.csv \
        --orderbook orderbook_data.csv \
        --trades trades_data.csv \
        --series KXINXY \
        --eta-total 30

This will print evaluation metrics on the training set and output a table of
posterior probabilities for the open markets.  You can adjust the series,
pseudo‑sample size, and minimum trade filtering via command‑line options.

Notes
-----
* The structural prior here is an approximation.  A full implementation of
  the tempered fractional Heston fractional Brownian motion model would
  involve calibrating several parameters (gamma, theta, sigma, alpha,
  lambda, rho, V0) under the constraints given in the CAPOPM paper
  【205128327162733†L404-L407】 and computing bucket probabilities via
  characteristic function inversion.  That level of detail is beyond the
  scope of this script, but the grid‑search procedure captures the rough
  long‑memory behaviour by scaling variance as ``t**alpha``.
* The candidate ML models can be extended.  If the ``xgboost`` library is
  installed, an XGBoost classifier will automatically be included in the
  comparison.
* This script assumes all input CSV files use UTC timestamps in ISO8601
  format and that price columns are in dollars (0–1 for digital options).

"""

from __future__ import annotations

import argparse
import re
import json
import math
from dataclasses import dataclass
from pathlib import Path
from typing import Any, Dict, Iterable, List, Optional, Tuple

import numpy as np
import pandas as pd
from sklearn.compose import ColumnTransformer
from sklearn.ensemble import GradientBoostingClassifier, RandomForestClassifier
from sklearn.linear_model import LogisticRegression
from sklearn.metrics import brier_score_loss, log_loss, make_scorer
from sklearn.model_selection import cross_val_score, train_test_split
from sklearn.pipeline import Pipeline
from sklearn.preprocessing import StandardScaler
from scipy.special import erf
import scipy
from scipy.special import erf
import numpy as np
from sklearn.metrics import brier_score_loss
import yfinance as yf

###############################################################################
# Configuration
###############################################################################

# Candidate machine learning models.  Each tuple contains a scikit‑learn
# estimator and a short name.  If ``xgboost`` is available, it will be
# imported at runtime and appended to this list.
CANDIDATE_MODELS: List[Tuple[str, Any]] = [
    (
        "logistic",
        Pipeline(
            [
                ("scaler", StandardScaler()),
                (
                    "classifier",
                    LogisticRegression(
                        max_iter=2000,
                        class_weight="balanced",
                        solver="lbfgs",
                    ),
                ),
            ]
        ),
    ),
    (
        "random_forest",
        RandomForestClassifier(
            n_estimators=300,
            min_samples_leaf=2,
            random_state=42,
            n_jobs=-1,
        ),
    ),
    (
        "gradient_boosting",
        GradientBoostingClassifier(
            n_estimators=300,
            learning_rate=0.05,
            max_depth=3,
            random_state=42,
        ),
    ),
]

# Attempt to import XGBoost.  If available, include it in the model list.
try:
    from xgboost import XGBClassifier  # type: ignore

    CANDIDATE_MODELS.append(
        (
            "xgboost",
            XGBClassifier(
                n_estimators=300,
                learning_rate=0.05,
                max_depth=3,
                subsample=0.8,
                colsample_bytree=0.8,
                eval_metric="logloss",
                random_state=42,
            ),
        )
    )
except ImportError:
    pass


###############################################################################
# Utility functions
###############################################################################

def parse_iso(ts: str) -> pd.Timestamp:
    """Parse ISO8601 timestamp into pandas Timestamp."""
    return pd.to_datetime(ts, utc=True)


def microstructure_features(
    orderbook_df: pd.DataFrame, trades_df: pd.DataFrame
) -> pd.DataFrame:
    """Compute microstructure features per market.

    Parameters
    ----------
    orderbook_df : pandas.DataFrame
        Rows of order book snapshots with columns: 'market_ticker',
        'yes_prices', 'yes_sizes', 'no_prices', 'no_sizes'.
    trades_df : pandas.DataFrame
        Rows of trade data with columns: 'ticker', 'count', 'yes_price_dollars',
        'no_price_dollars', 'taker_side', 'created_time'.

    Returns
    -------
    pandas.DataFrame
        One row per market with microstructure features.
    """
    # Aggregate orderbook features
    ob_features: Dict[str, Dict[str, Any]] = {}
    for _, row in orderbook_df.iterrows():
        ticker = row["market_ticker"]
        try:
            yes_prices = json.loads(row["yes_prices"])
            yes_sizes = json.loads(row["yes_sizes"])
            no_prices = json.loads(row["no_prices"])
            no_sizes = json.loads(row["no_sizes"])
        except (TypeError, json.JSONDecodeError):
            yes_prices, yes_sizes, no_prices, no_sizes = [], [], [], []
        if not yes_prices and not no_prices:
            continue
        # Compute best (lowest) price for yes and no sides
        yes_best_price = min(yes_prices) if yes_prices else np.nan
        no_best_price = min(no_prices) if no_prices else np.nan
        # Compute total depth
        yes_depth = float(sum(yes_sizes)) if yes_sizes else 0.0
        no_depth = float(sum(no_sizes)) if no_sizes else 0.0
        # Imbalance
        if yes_depth + no_depth > 0:
            order_imbalance = (yes_depth - no_depth) / (yes_depth + no_depth)
        else:
            order_imbalance = 0.0
        ob_features[ticker] = {
            "yes_best_price_ob": yes_best_price,
            "no_best_price_ob": no_best_price,
            "yes_depth_ob": yes_depth,
            "no_depth_ob": no_depth,
            "orderbook_imbalance": order_imbalance,
        }
    ob_df = pd.DataFrame.from_dict(ob_features, orient="index").reset_index().rename(columns={"index": "market_ticker"})

    # Aggregate trade features
    trade_features: List[Dict[str, Any]] = []
    trades_df["created_ts"] = trades_df["created_time"].apply(lambda x: parse_iso(x) if isinstance(x, str) else pd.NaT)
    grouped = trades_df.groupby("ticker")
    for ticker, grp in grouped:
        grp_valid = grp.dropna(subset=["created_ts"])
        if grp_valid.empty:
            continue
        yes_trades = grp_valid[grp_valid["taker_side"] == "yes"]
        no_trades = grp_valid[grp_valid["taker_side"] == "no"]
        total_volume = grp_valid["count"].sum()
        yes_volume = yes_trades["count"].sum()
        no_volume = no_trades["count"].sum()
        total_notional = (grp_valid["price_cents"] * grp_valid["count"]).sum() / 100.0
        yes_notional = (yes_trades["yes_price_cents"] * yes_trades["count"]).sum() / 100.0
        no_notional = (no_trades["no_price_cents"] * no_trades["count"]).sum() / 100.0
        # Weighted average price (dollars)
        avg_price = 0.0
        if total_volume > 0:
            avg_price = (grp_valid["price_cents"] * grp_valid["count"]).sum() / (total_volume * 100.0)
        yes_share = yes_volume / total_volume if total_volume > 0 else 0.5
        trade_imbalance = (yes_volume - no_volume) / total_volume if total_volume > 0 else 0.0
        # Price statistics
        prices_dollars = grp_valid["price_cents"].astype(float) / 100.0
        price_min = prices_dollars.min() if not prices_dollars.empty else np.nan
        price_max = prices_dollars.max() if not prices_dollars.empty else np.nan
        price_std = prices_dollars.std(ddof=0) if not prices_dollars.empty else 0.0
        # Time span
        time_span = (grp_valid["created_ts"].max() - grp_valid["created_ts"].min()).total_seconds() if len(grp_valid) > 1 else 0.0
        trade_features.append(
            {
                "market_ticker": ticker,
                "total_trades": len(grp_valid),
                "total_volume": total_volume,
                "yes_volume": yes_volume,
                "no_volume": no_volume,
                "yes_notional": yes_notional,
                "no_notional": no_notional,
                "total_notional": total_notional,
                "avg_price_trades": avg_price,
                "yes_share_volume": yes_share,
                "trade_imbalance": trade_imbalance,
                "price_min_trades": price_min,
                "price_max_trades": price_max,
                "price_std_trades": price_std,
                "trade_time_span": time_span,
            }
        )
    trades_aggr_df = pd.DataFrame(trade_features)

    # Merge orderbook and trades features
    features_df = pd.merge(trades_aggr_df, ob_df, on="market_ticker", how="left")
    return features_df


###############################################################################
# Build proper historical training data for ML model selection
###############################################################################

def build_historical_training_set(
    events_df_full: pd.DataFrame,
    orderbook_df: pd.DataFrame,
    trades_df: pd.DataFrame,
) -> tuple[np.ndarray, np.ndarray]:
    """
    Build a non-empty training set using ALL resolved yearly S&P 500 binary markets
    from past years (INXD-24DEC31, INXY-22DEC30, etc.). These are structurally identical
    to KXINXY-25DEC31 → perfect for training the ML prior.
    """
    print("Building historical training set from past resolved S&P 500 yearly markets...")

    # 1. Filter to only yearly S&P 500 markets (same structure as KXINXY)
    hist_df = events_df_full[
        events_df_full["series_ticker"].str.contains(r"^IN[XY]D?-\d{2}DEC\d{2}$", na=False, regex=True)
    ].copy()

    # 2. Keep only resolved markets
    hist_df = hist_df[hist_df["result"].isin(["yes", "no"])].copy()
    if hist_df.empty:
        raise ValueError("No resolved yearly S&P 500 markets found in events file!")

    # 3. Target: 1 if yes won
    hist_df["target"] = (hist_df["result"] == "yes").astype(int)

    # 4. Extract strike (e.g., B7500 → 7500)
    import re
    def extract_strike(ticker: str) -> float:
        m = re.search(r"-B(\d+)", ticker) or re.search(r"-T(\d+)", ticker)
        return float(m.group(1)) if m else np.nan
    hist_df["strike"] = hist_df["market_ticker"].apply(extract_strike)

    # 5. Compute microstructure features (same as current markets)
    micro_all = microstructure_features(orderbook_df, trades_df)

    # 6. Merge features
    train_df = hist_df.merge(micro_all, on="market_ticker", how="inner")

    if train_df.empty:
        print("No historical markets have orderbook/trade data. Using dummy training.")
        X_dummy = np.zeros((10, 15))
        y_dummy = np.array([1]*5 + [0]*5)
        return X_dummy, y_dummy

    print(f"Success: {len(train_df)} resolved historical contracts for training")

    # 7. Feature columns (same as you use later)
    feature_cols = [
        "mid_strike", "width", "time_to_expiry_days",
        "yes_tickets", "no_tickets", "total_tickets",
        "avg_price_trades", "yes_share_volume", "trade_imbalance",
        "price_std_trades", "yes_best_price_ob", "no_best_price_ob",
        "yes_depth_ob", "no_depth_ob", "orderbook_imbalance", "strike"
    ]

    # Add missing columns with defaults
    for col in feature_cols:
        if col not in train_df.columns:
            if col == "mid_strike":
                train_df[col] = 0.5 * (train_df["floor_strike"] + train_df["cap_strike"])
            elif col == "width":
                train_df[col] = train_df["cap_strike"] - train_df["floor_strike"]
            elif col == "time_to_expiry_days":
                train_df[col] = 30.0  # rough average
            elif col == "strike":
                train_df[col] = train_df["market_ticker"].apply(extract_strike)
            else:
                train_df[col] = 0.0

    X_train = train_df[feature_cols].fillna(0).values
    y_train = train_df["target"].values

    return X_train, y_train


###############################################################################
# ML and structural priors
###############################################################################

def select_best_model(X: pd.DataFrame, y: np.ndarray) -> Tuple[Any, str]:
    """Select the best ML model using cross‑validated Brier score.

    Parameters
    ----------
    X : pandas.DataFrame or numpy.ndarray
        Feature matrix for training.
    y : numpy.ndarray
        Binary target vector (1 for winning bucket, 0 otherwise).

    Returns
    -------
    tuple
        The fitted best estimator and its name.
    """
    best_score = float("inf")
    best_model = None
    best_name = ""
    brier_scorer = make_scorer(brier_score_loss, greater_is_better=False)
    for name, model in CANDIDATE_MODELS:
        try:
            cv = max(2, min(5, len(y)))  # desired CV folds
            if cv < 2:
                print("Too few training samples; using baseline Log-Reg")  # enforce at least 2 folds
                model = LogisticRegression(max_iter=2000, class_weight="balanced", solver="lbfgs")
                model.fit(X, y)
                return model, "logistic"
            scores = cross_val_score(model, X, y, cv=cv, scoring=brier_scorer, n_jobs=-1)

            mean_brier = -scores.mean()  # negative because greater_is_better=False
            print(f"Model {name}: CV Brier={mean_brier:.6f}")
            if mean_brier < best_score:
                best_score = mean_brier
                best_model = model
                best_name = name
        except Exception as exc:
            print(f"Model {name} failed during cross‑validation: {exc}")
            continue
    if best_model is None:
        raise RuntimeError("All candidate models failed during cross‑validation")
    # Fit the best model on the full training data
    best_model.fit(X, y)
    print(f"Selected model: {best_name} (Brier={best_score:.6f})")
    return best_model, best_name






from scipy.special import erf
import numpy as np
from sklearn.metrics import brier_score_loss

# -------------------------------------------------------------------
# SPX history helpers for structural prior (fractional lognormal)
# -------------------------------------------------------------------

_SPX_CACHE: Optional[pd.DataFrame] = None

def get_spx_history(
    start: str = "2000-01-01",
    end: str = "2025-12-04",
) -> Optional[pd.DataFrame]:
    """
    Download and cache SPX (^GSPC) daily close prices via yfinance.

    Under Q we only care about the volatility structure, so we use
    25 years of history to estimate a global daily vol.
    """
    global _SPX_CACHE
    if _SPX_CACHE is not None:
        return _SPX_CACHE

    try:
        print(f"Downloading SPX history from {start} to {end} via yfinance (^GSPC)…")
        spx = yf.download("^GSPC", start=start, end=end, progress=False)
    except Exception as exc:
        print(f"WARNING: failed to download SPX history: {exc}")
        _SPX_CACHE = None
        return None

    if spx is None or spx.empty:
        print("WARNING: empty SPX history; falling back to flat structural prior.")
        _SPX_CACHE = None
        return None

    spx = spx[["Close"]].rename(columns={"Close": "close"})
    # yfinance returns naive timestamps; attach UTC tz to be consistent
    spx.index = pd.to_datetime(spx.index).tz_localize("UTC")
    _SPX_CACHE = spx
    return spx


def get_spx_close_on_or_before(spx: pd.DataFrame, ts: pd.Timestamp) -> float:
    """
    Get SPX close on or before a given timestamp.

    If ts has no tz, assume UTC. If ts is before the SPX start date,
    return the first available close. This is used as S0 in the
    risk-neutral lognormal prior.
    """
    if ts.tzinfo is None:
        ts = ts.tz_localize("UTC")
    # Align to date
    date = ts.normalize()
    # All closes up to that date
    s = spx.loc[:date, "close"]
    if s.empty:
        return float(spx["close"].iloc[0])
    return float(s.iloc[-1])


from scipy.special import erf
import numpy as np
from sklearn.metrics import brier_score_loss

def tempered_fractional_structural_probs(
    df: pd.DataFrame,
    train_mask: np.ndarray,
) -> np.ndarray:
    """
    Fractional-lognormal structural prior based on 25 years of SPX history.

    Under risk-neutral measure Q, we assume:
        ln(S_T) ~ N( ln(S0) - 0.5 * σ_0^2 * T^α,  σ_0^2 * T^α )

    where:
        - S0 is the SPX level at (or just before) the market's creation time,
        - T   is time-to-expiry in YEARS for that bucket,
        - σ_0 is the annualised volatility estimated from SPX,
        - α ∈ (0.5, 1) is a fractional scaling exponent (long-memory).

    For each bucket [K_i, K_{i+1}), the structural probability is:
        q_i = P( K_i ≤ S_T < K_{i+1} )

    We calibrate (α, scale) on the training buckets by minimising Brier
    between q_i and realized_outcome, subject to α ∈ (0.5, 1).
    """
    print("Computing structural prior from SPX (fractional lognormal)…")

    # If there is no training data, revert to a flat prior
    if not train_mask.any():
        print("No training rows found → structural prior = 0.5 for all buckets.")
        return np.full(len(df), 0.5, dtype=float)

    # ------------------------------------------------------------------
    # 1) Load SPX (^GSPC) and estimate global annual volatility
    # ------------------------------------------------------------------
    spx = get_spx_history()
    if spx is None:
        return np.full(len(df), 0.5, dtype=float)

    spx["log_ret"] = np.log(spx["close"]).diff()
    vol_daily = spx["log_ret"].std(skipna=True)
    if not np.isfinite(vol_daily) or vol_daily <= 0:
        print("WARNING: invalid SPX daily vol → structural prior = 0.5")
        return np.full(len(df), 0.5, dtype=float)

    sigma_annual = float(vol_daily * math.sqrt(252.0))
    print(f"Estimated SPX annual vol from history: σ_annual ≈ {sigma_annual:.4f}")

    # ------------------------------------------------------------------
    # 2) Per-bucket S0 and time-to-expiry in years
    # ------------------------------------------------------------------
    created_ts = pd.to_datetime(df["created_ts"], utc=True, errors="coerce")
    expiration_ts = pd.to_datetime(df["expiration_ts"], utc=True, errors="coerce")

    # If created_ts is missing, reconstruct from expiration and time_to_expiry_days
    if "time_to_expiry_days" in df.columns:
        tte_days = df["time_to_expiry_days"]
        mask_missing_created = created_ts.isna() & expiration_ts.notna() & tte_days.notna()
        created_ts.loc[mask_missing_created] = (
            expiration_ts[mask_missing_created]
            - pd.to_timedelta(tte_days[mask_missing_created], unit="D")
        )

    # Any remaining NaT: use earliest SPX date to avoid NaNs
    created_ts = created_ts.fillna(spx.index[0])

    # S0_i: SPX level at or before created_ts
    S0_all = np.array(
        [get_spx_close_on_or_before(spx, ts) for ts in created_ts],
        dtype=float,
    )

    # Time to expiry in years (fallback to 1 day if non-positive)
    if "time_to_expiry_days" in df.columns:
        T_days = df["time_to_expiry_days"].to_numpy(dtype=float)
    else:
        # As a last resort, use expiration - created in days
        dt = (expiration_ts - created_ts).dt.total_seconds() / (24.0 * 3600.0)
        T_days = dt.to_numpy(dtype=float)

    T_days = np.where(~np.isfinite(T_days) | (T_days <= 0.0), 1.0, T_days)
    T_years = T_days / 365.25

    # Strikes
    floor = df["floor_strike"].to_numpy(dtype=float)
    cap = df["cap_strike"].to_numpy(dtype=float)

    # ------------------------------------------------------------------
    # 3) Grid-search over α and volatility scale to minimise Brier
    # ------------------------------------------------------------------
    y_train = df.loc[train_mask, "realized_outcome"].astype(int).to_numpy()

    alpha_grid = np.linspace(0.6, 0.95, 8)   # constrained to (0.5, 1)
    scale_grid = np.linspace(0.5, 1.5, 5)    # scales σ_annual

    best_brier = float("inf")
    best_alpha = 0.8
    best_scale = 1.0

    # Pre-compute logs of strikes for efficiency
    log_floor_all = np.log(np.maximum(floor, 1e-8))
    log_cap_all = np.log(np.maximum(cap, 1e-8))

    for alpha in alpha_grid:
        if alpha <= 0.5 or alpha >= 1.0:
            continue

        for scale in scale_grid:
            sigma0 = sigma_annual * scale
            # Var[ln S_T] = σ0^2 * T^α
            var_all = (sigma0 ** 2) * (T_years ** alpha)
            std_all = np.sqrt(var_all)
            std_all = np.where(std_all <= 1e-8, 1e-8, std_all)

            # Under Q, choose μ_i so that E_Q[S_T] = S0_i
            # For a lognormal: E[S_T] = exp(μ_i + 0.5 * var_i) = S0_i
            # ⇒ μ_i = ln S0_i - 0.5 * var_i
            mu_all = np.log(np.maximum(S0_all, 1e-8)) - 0.5 * var_all

            # Bucket probabilities for all rows
            z_hi = (log_cap_all - mu_all) / std_all
            z_lo = (log_floor_all - mu_all) / std_all
            Phi_hi = 0.5 * (1.0 + erf(z_hi / math.sqrt(2.0)))
            Phi_lo = 0.5 * (1.0 + erf(z_lo / math.sqrt(2.0)))
            q_all = np.clip(Phi_hi - Phi_lo, 1e-8, 1.0 - 1e-8)

            # Brier on training rows
            try:
                brier = brier_score_loss(y_train, q_all[train_mask])
            except Exception:
                continue

            if np.isfinite(brier) and brier < best_brier:
                best_brier = float(brier)
                best_alpha = float(alpha)
                best_scale = float(scale)

    print(
        f"Selected structural params: alpha={best_alpha:.3f}, "
        f"scale={best_scale:.3f}, σ_annual={sigma_annual:.4f}, "
        f"TRAIN Brier (struct only)={best_brier:.6f}"
    )

    # ------------------------------------------------------------------
    # 4) Final structural probabilities q_struct for ALL rows
    # ------------------------------------------------------------------
    sigma0 = sigma_annual * best_scale
    var_all = (sigma0 ** 2) * (T_years ** best_alpha)
    std_all = np.sqrt(var_all)
    std_all = np.where(std_all <= 1e-8, 1e-8, std_all)
    mu_all = np.log(np.maximum(S0_all, 1e-8)) - 0.5 * var_all

    z_hi = (log_cap_all - mu_all) / std_all
    z_lo = (log_floor_all - mu_all) / std_all
    Phi_hi = 0.5 * (1.0 + erf(z_hi / math.sqrt(2.0)))
    Phi_lo = 0.5 * (1.0 + erf(z_lo / math.sqrt(2.0)))
    q_struct = np.clip(Phi_hi - Phi_lo, 1e-8, 1.0 - 1e-8)

    if np.any(~np.isfinite(q_struct)):
        print("WARNING: non-finite q_struct encountered → reverting to 0.5")
        q_struct = np.full(len(df), 0.5, dtype=float)

    return q_struct

# -------------------------------------------------------------------
# VIX history + non-Kalshi ML prior (SPX / VIX based)
# -------------------------------------------------------------------

def get_vix_history(
    start: str = "2000-01-01",
    end: str = "2025-12-04",
) -> Optional[pd.DataFrame]:
    """
    Download and cache VIX (^VIX) daily close prices via yfinance.

    Used as a macro / volatility factor for the non-Kalshi ML prior.
    """
    global _VIX_CACHE
    try:
        _ = _VIX_CACHE
    except NameError:
        _VIX_CACHE = None

    if _VIX_CACHE is not None:
        return _VIX_CACHE

    try:
        print(f"Downloading VIX history from {start} to {end} via yfinance (^VIX)…")
        vix = yf.download("^VIX", start=start, end=end, progress=False)
    except Exception as exc:
        print(f"WARNING: failed to download VIX history: {exc}")
        _VIX_CACHE = None
        return None

    if vix is None or vix.empty:
        print("WARNING: empty VIX history; continuing without VIX features.")
        _VIX_CACHE = None
        return None

    vix = vix[["Close"]].rename(columns={"Close": "close"})
    vix.index = pd.to_datetime(vix.index).tz_localize("UTC")
    _VIX_CACHE = vix
    return vix


def build_non_kalshi_ml_prior(data_df: pd.DataFrame) -> np.ndarray:
    """
    Train an ML prior on NON-Kalshi features (Option B):

    - Uses historical SPX (^GSPC) and VIX (^VIX) as FEATURES ONLY.
    - Uses Kalshi bucket outcomes (realized_outcome) as labels.

    Training rule (per your spec):
        * Prefer resolved events with expiration year <= 2024.
        * If none exist yet, fall back to all resolved events.

    Features per bucket:
        - mid_strike, width, rel_strike (vs realised SPX level at expiry)
        - realised vol (30d, 252d), VIX close, VIX 30d avg, expiry year

    The model predicts P(bucket wins) for each row; these are used as the
    ML prior probabilities q_ml_all.
    """
    # ---- Load SPX and VIX histories ----
    spx = get_spx_history()
    if spx is None:
        print("SPX history unavailable → ML prior = 0.5")
        return np.full(len(data_df), 0.5, dtype=float)

    vix = get_vix_history()

    # Ensure log returns present for realised vol features
    if "log_ret" not in spx.columns:
        spx["log_ret"] = np.log(spx["close"]).diff()

    # Ensure timestamps parsed
    expiration_ts = pd.to_datetime(
        data_df["expiration_ts"], utc=True, errors="coerce"
    )
    created_ts = pd.to_datetime(
        data_df["created_ts"], utc=True, errors="coerce"
    )

    # If created_ts missing but we have time_to_expiry_days, back out
    if "time_to_expiry_days" in data_df.columns:
        tte_days = data_df["time_to_expiry_days"]
        mask_missing_created = created_ts.isna() & expiration_ts.notna() & tte_days.notna()
        created_ts.loc[mask_missing_created] = (
            expiration_ts[mask_missing_created]
            - pd.to_timedelta(tte_days[mask_missing_created], unit="D")
        )

    # Fill any remaining missing times with simple defaults
    created_ts = created_ts.fillna(spx.index[0])
    expiration_ts = expiration_ts.fillna(created_ts + pd.to_timedelta(365, unit="D"))

    df = data_df.copy()
    df["created_ts"] = created_ts
    df["expiration_ts"] = expiration_ts

    # ---- Identify training rows according to your rule ----
    resolved_mask = df["realized_outcome"].notna()
    expiry_year = df["expiration_ts"].dt.year

    pre2025_mask = resolved_mask & (expiry_year <= 2024)
    if pre2025_mask.any():
        train_mask = pre2025_mask
        print(f"Using {train_mask.sum()} resolved pre-2025 buckets for ML prior training.")
    else:
        # Your current CSV has only 2025 expiries; fall back gracefully.
        train_mask = resolved_mask
        print(
            "No resolved pre-2025 events in this file → "
            "training ML prior on all resolved rows instead."
        )

    if not train_mask.any():
        print("No resolved events at all → ML prior = 0.5")
        return np.full(len(df), 0.5, dtype=float)

    # ---------- Helper: SPX/VIX features at a given expiration date ----------
    def _features_for_date(exp_ts: pd.Timestamp) -> dict:
        # Normalize to UTC date
        if exp_ts.tzinfo is None:
            exp_ts_local = exp_ts.tz_localize("UTC")
        else:
            exp_ts_local = exp_ts.tz_convert("UTC")
        date = exp_ts_local.normalize()

        # SPX slice up to this date
        spx_slice = spx.loc[:date]
        if spx_slice.empty:
            spx_slice = spx

        S_T = float(spx_slice["close"].iloc[0])

        # realised vols
        log_ret_slice = spx_slice["log_ret"].dropna()
        if log_ret_slice.empty:
            base_log_ret = spx["log_ret"].dropna()
            vol30 = vol252 = float(base_log_ret.std()) if not base_log_ret.empty else 0.2
        else:
            vol30 = float(log_ret_slice.tail(30).std()) if len(log_ret_slice) >= 2 else float(log_ret_slice.std())
            vol252 = float(log_ret_slice.tail(252).std()) if len(log_ret_slice) >= 2 else float(log_ret_slice.std())

        # VIX features (optional)
        vix_close = 0.0
        vix_avg30 = 0.0
        if vix is not None and not vix.empty:
            vix_slice = vix.loc[:date]
            if not vix_slice.empty:
                vix_close = float(vix_slice["close"].iloc[-1])
                vix_avg30 = float(vix_slice["close"].tail(30).mean())

        return {
            "S_T": S_T,
            "vol30": vol30,
            "vol252": vol252,
            "vix_close": vix_close,
            "vix_avg30": vix_avg30,
            "year": float(exp_ts_local.year),
        }

    # -------------------------
    # Build TRAIN feature set
    # -------------------------
    train_rows = df[train_mask].copy()

    feat_records = []
    y_train = []
    for idx, row in train_rows.iterrows():
        K_lo = float(row["floor_strike"])
        K_hi = float(row["cap_strike"])
        exp_ts = row["expiration_ts"]

        feats_date = _features_for_date(exp_ts)
        S_T = feats_date["S_T"]

        mid = 0.5 * (K_lo + K_hi)
        width = K_hi - K_lo
        rel_strike = (mid / S_T) - 1.0 if S_T > 0 else 0.0

        feat_records.append(
            {
                "mid_strike": mid,
                "width": width,
                "rel_strike": rel_strike,
                "S_T": S_T,
                "vol30": feats_date["vol30"],
                "vol252": feats_date["vol252"],
                "vix_close": feats_date["vix_close"],
                "vix_avg30": feats_date["vix_avg30"],
                "year": feats_date["year"],
            }
        )
        # LABEL: actual Kalshi outcome (non-price, but still from the market)
        y_train.append(int(row["realized_outcome"]))

    X_train_df = pd.DataFrame(feat_records)
    y_train = np.array(y_train, dtype=int)

    # If somehow all labels are the same, bail out gracefully
    unique_y = np.unique(y_train)
    if unique_y.size < 2:
        print(
            f"WARNING: ML training labels have only one class {unique_y}; "
            "returning flat prior 0.5."
        )
        return np.full(len(df), 0.5, dtype=float)

    # Fill NaNs with column medians
    X_train = X_train_df.fillna(X_train_df.median()).to_numpy(dtype=float)
    print(f"Non-Kalshi ML training set: {X_train.shape[0]} buckets, {X_train.shape[1]} features")

    # Use existing model selector (logistic / RF / GB / XGB)
    ml_model, ml_name = select_best_model(X_train, y_train)
    print(f"Non-Kalshi ML prior selected model: {ml_name}")

    # -------------------------
    # Build ALL-row feature set
    # -------------------------
    all_feat_records = []
    for idx, row in df.iterrows():
        K_lo = float(row["floor_strike"])
        K_hi = float(row["cap_strike"])
        exp_ts = row["expiration_ts"]

        feats_date = _features_for_date(exp_ts)
        S_T = feats_date["S_T"]

        mid = 0.5 * (K_lo + K_hi)
        width = K_hi - K_lo
        rel_strike = (mid / S_T) - 1.0 if S_T > 0 else 0.0

        all_feat_records.append(
            {
                "mid_strike": mid,
                "width": width,
                "rel_strike": rel_strike,
                "S_T": S_T,
                "vol30": feats_date["vol30"],
                "vol252": feats_date["vol252"],
                "vix_close": feats_date["vix_close"],
                "vix_avg30": feats_date["vix_avg30"],
                "year": feats_date["year"],
            }
        )

    X_all_df = pd.DataFrame(all_feat_records)
    X_all = X_all_df.fillna(X_all_df.median()).to_numpy(dtype=float)

    # Map to prob of class "1" robustly
    proba = ml_model.predict_proba(X_all)
    if proba.shape[1] == 1:
        # Single-class classifier; classes_[0] is either 0 or 1
        cls = int(getattr(ml_model, "classes_", [0])[0])
        if cls == 1:
            q_ml_all = proba[:, 0]
        else:  # cls == 0
            q_ml_all = 1.0 - proba[:, 0]
    else:
        # Standard binary case; find column index for class 1
        classes = list(getattr(ml_model, "classes_", [0, 1]))
        if 1 in classes:
            idx1 = classes.index(1)
        else:
            idx1 = 1  # fallback
        q_ml_all = proba[:, idx1]

    q_ml_all = np.clip(q_ml_all, 1e-6, 1.0 - 1e-6)
    return q_ml_all



###############################################################################
# Posterior update and corrections
###############################################################################

@dataclass
class Priors:
    alpha0: np.ndarray
    beta0: np.ndarray
    q_struct: np.ndarray
    q_ml: np.ndarray
    q_hybrid: np.ndarray
    lambda_struct: float
    lambda_ml: float
    brier_struct: float
    brier_ml: float


def compute_hybrid_priors(
    df: pd.DataFrame,
    q_struct: np.ndarray,
    q_ml: np.ndarray,
    train_mask: np.ndarray,
    eta_total: float = 30.0,
) -> Priors:
    """Compute hybrid priors via reliability weighting.

    On the training rows indicated by ``train_mask``, Brier scores are
    computed for both the structural and ML probabilities.  Weights are
    assigned as the inverse of the Brier scores and normalised.  These
    weights determine the mix of structural and ML probabilities used to
    compute the hybrid probability for every row.  The hybrid probability
    ``q_hybrid`` is then converted into Beta prior pseudo‑counts
    ``alpha0`` and ``beta0`` based on ``eta_total``.

    Parameters
    ----------
    df : pandas.DataFrame
        Full dataset.  Must include ``realized_outcome`` column.
    q_struct : numpy.ndarray
        Structural probabilities for all rows.
    q_ml : numpy.ndarray
        ML prior probabilities for all rows.
    train_mask : numpy.ndarray
        Boolean array of length equal to the number of rows in ``df``,
        indicating which rows are used for reliability evaluation.
    eta_total : float
        Total pseudo‑sample size assigned to the hybrid prior.  Larger
        values give more weight to the prior relative to the crowd.

    Returns
    -------
    Priors
        Dataclass containing pseudo‑counts and diagnostic information.
    """
    y_train = df.loc[train_mask, "realized_outcome"].astype(int).to_numpy()
    brier_struct = brier_score_loss(y_train, q_struct[train_mask])
    brier_ml = brier_score_loss(y_train, q_ml[train_mask])
    # Avoid division by zero
    w_struct = 1.0 / max(brier_struct, 1e-8)
    w_ml = 1.0 / max(brier_ml, 1e-8)
    lambda_struct = w_struct / (w_struct + w_ml)
    lambda_ml = w_ml / (w_struct + w_ml)
    q_hybrid = np.clip(lambda_struct * q_struct + lambda_ml * q_ml, 1e-6, 1.0 - 1e-6)
    alpha0 = eta_total * q_hybrid
    beta0 = eta_total * (1.0 - q_hybrid)
    return Priors(
        alpha0=alpha0,
        beta0=beta0,
        q_struct=q_struct,
        q_ml=q_ml,
        q_hybrid=q_hybrid,
        lambda_struct=float(lambda_struct),
        lambda_ml=float(lambda_ml),
        brier_struct=float(brier_struct),
        brier_ml=float(brier_ml),
    )


@dataclass
class PosteriorResults:
    p_raw: np.ndarray
    p_full: np.ndarray


def update_posteriors(
    df: pd.DataFrame,
    priors: Priors,
    train_mask: np.ndarray,
) -> PosteriorResults:
    """Update Beta priors to posteriors using crowd data with corrections.

    Stage 1: Fit a logistic regression mapping market price to empirical
    probabilities on the training set.  Use this mapping to adjust the
    yes/no ticket counts for each market: count_yes_stage1 = total_tickets *
    w(price), count_no_stage1 = total_tickets - count_yes_stage1.

    Stage 2: Liquidity scaling – compute the ratio of total tickets to the
    median total tickets on the training set.  Clip this ratio to [0.3, 1.0]
    and multiply the stage1 counts by the ratio.  The resulting counts are
    used to update the Beta prior.

    Two posterior means are returned:

    * ``p_raw`` – Beta mean using the raw yes/no ticket counts without any
      behavioural or liquidity corrections.
    * ``p_full`` – Beta mean using the corrected counts.

    Parameters
    ----------
    df : pandas.DataFrame
        Full dataset.  Must include 'kalshi_implied_yes_prob', 'yes_tickets',
        'no_tickets', and 'total_tickets'.
    priors : Priors
        Hybrid priors produced by ``compute_hybrid_priors``.
    train_mask : numpy.ndarray
        Boolean array indicating which rows belong to training events.

    Returns
    -------
    PosteriorResults
        A dataclass with raw and full posterior mean arrays.
    """
    # Raw Beta posterior (no corrections)
    yes_counts_raw = df["yes_tickets"].astype(float).to_numpy()
    no_counts_raw = df["no_tickets"].astype(float).to_numpy()
    alpha_raw = priors.alpha0 + yes_counts_raw
    beta_raw = priors.beta0 + no_counts_raw
    p_raw = alpha_raw / (alpha_raw + beta_raw)

    # Behavioural calibrator: fit logistic regression on training rows
    train_df = df.loc[train_mask]
    # price may contain NaNs; fill with 0.5 for calibration
    price_train = train_df["kalshi_implied_yes_prob"].fillna(0.5).to_numpy().reshape(-1, 1)
    outcome_train = train_df["realized_outcome"].astype(int).to_numpy()
    price_cal = LogisticRegression(max_iter=1000)
    try:
        price_cal.fit(price_train, outcome_train)
    except Exception:
        # fall back to identity mapping if fit fails
        price_cal = None

    # Stage 1: behavioural adjustment
    prices_full = df["kalshi_implied_yes_prob"].fillna(0.5).to_numpy().reshape(-1, 1)
    if price_cal is not None:
        w = price_cal.predict_proba(prices_full)[:, 1]
    else:
        w = prices_full.ravel()
    # Ensure weights in (0,1)
    w = np.clip(w, 1e-6, 1.0 - 1e-6)
    total_tickets = df["total_tickets"].astype(float).to_numpy()
    yes_stage1 = total_tickets * w
    no_stage1 = total_tickets - yes_stage1

    # Stage 2: liquidity scaling
    median_tickets = df.loc[train_mask, "total_tickets"].median() if train_mask.any() else df["total_tickets"].median()
    median_tickets = max(median_tickets, 1.0)
    liquidity_ratio = total_tickets / median_tickets
    lambda_liq = np.clip(liquidity_ratio, 0.3, 1.0)
    yes_corr = yes_stage1 * lambda_liq
    no_corr = no_stage1 * lambda_liq

    alpha_full = priors.alpha0 + yes_corr
    beta_full = priors.beta0 + no_corr
    p_full = alpha_full / (alpha_full + beta_full)
    return PosteriorResults(p_raw=p_raw, p_full=p_full)


###############################################################################
# Evaluation and reporting
###############################################################################

def evaluate_models(
    df: pd.DataFrame,
    train_mask: np.ndarray,
    priors: Priors,
    post: PosteriorResults,
) -> None:
    """Print evaluation metrics on the training set.

    Parameters
    ----------
    df : pandas.DataFrame
        The full dataset with 'realized_outcome' column.
    train_mask : numpy.ndarray
        Boolean mask identifying training rows.
    priors : Priors
        Hybrid priors.
    post : PosteriorResults
        Posterior results with raw and full means.
    """
    y_true = df.loc[train_mask, "realized_outcome"].astype(int).to_numpy()
    p_k = df.loc[train_mask, "kalshi_implied_yes_prob"].astype(float).to_numpy()
    p_prior = priors.q_hybrid[train_mask]
    p_raw = post.p_raw[train_mask]
    p_full = post.p_full[train_mask]
    # Brier scores
    brier_scores = {
        "kalshi": brier_score_loss(y_true, np.clip(p_k, 1e-6, 1.0 - 1e-6)),
        "prior": brier_score_loss(y_true, p_prior),
        "capopm_raw": brier_score_loss(y_true, p_raw),
        "capopm_full": brier_score_loss(y_true, p_full),
    }
    # Log losses
    log_losses = {
        "kalshi": log_loss(y_true, np.clip(p_k, 1e-6, 1.0 - 1e-6)),
        "prior": log_loss(y_true, p_prior),
        "capopm_raw": log_loss(y_true, p_raw),
        "capopm_full": log_loss(y_true, p_full),
    }
    print("\n--- Brier scores (TRAIN, lower is better) ---")
    for k, v in brier_scores.items():
        print(f"{k:12s}: {v:.6f}")
    print("\n--- Log losses (TRAIN, lower is better) ---")
    for k, v in log_losses.items():
        print(f"{k:12s}: {v:.6f}")
    # Calibration for CAPOPM full posterior
    bins = np.linspace(0.0, 1.0, 11)
    bin_ids = np.digitize(p_full, bins) - 1
    calib_records = []
    for i in range(len(bins) - 1):
        mask = bin_ids == i
        if not np.any(mask):
            continue
        avg_pred = float(p_full[mask].mean())
        emp_freq = float(y_true[mask].mean())
        count = int(mask.sum())
        calib_records.append({
            "bin_low": float(bins[i]),
            "bin_high": float(bins[i + 1]),
            "n": count,
            "avg_pred": avg_pred,
            "emp_freq": emp_freq,
        })
    calib_df = pd.DataFrame(calib_records)
    print("\n--- CAPOPM full calibration (TRAIN) ---")
    if not calib_df.empty:
        print(calib_df.to_string(index=False))
    else:
        print("No calibration data available")


###############################################################################
# Main routine
###############################################################################

def main() -> None:
    parser = argparse.ArgumentParser(description="CAPOPM Phases 4–8 on Kalshi data")
    parser.add_argument("--events", type=Path, required=True,
                        help="CSV file with event‑level data (kalshi_capopm_events.csv)")
    parser.add_argument("--orderbook", type=Path, required=True,
                        help="CSV file with orderbook snapshots")
    parser.add_argument("--trades", type=Path, required=True,
                        help="CSV file with trade data")
    parser.add_argument("--series", type=str, required=True,
                        help="Series ticker (e.g., KXINXY)")
    parser.add_argument("--eta-total", type=float, default=30.0,
                        help="Total pseudo‑sample size for the hybrid prior")
    parser.add_argument("--min-trades", type=int, default=1,
                        help="Minimum number of trades required to keep a market")
    args = parser.parse_args()

    # Load datasets
    events_df = pd.read_csv(args.events)
    orderbook_df = pd.read_csv(args.orderbook)
    trades_df = pd.read_csv(args.trades)

    # Filter to the specified series
    if "series_ticker" in events_df.columns:
        events_df = events_df[events_df["series_ticker"] == args.series].copy()
    # Compute microstructure features
    micro_df = microstructure_features(orderbook_df, trades_df)
    # Merge with event data
    data_df = pd.merge(events_df, micro_df, left_on="market_ticker", right_on="market_ticker", how="left")
    # Convert timestamps to datetime
    data_df["created_ts"] = pd.to_datetime(data_df["created_ts"], utc=True, errors="coerce")
    data_df["expiration_ts"] = pd.to_datetime(data_df["expiration_ts"], utc=True, errors="coerce")
    # Compute bucket mid, width, time to expiry (days)
    data_df["mid_strike"] = 0.5 * (data_df["floor_strike"] + data_df["cap_strike"])
    data_df["width"] = data_df["cap_strike"] - data_df["floor_strike"]
    data_df["time_to_expiry_days"] = (data_df["expiration_ts"] - data_df["created_ts"]).dt.total_seconds() / (24 * 3600)
    # Fill missing microstructure features with zeros or sensible defaults
    micro_cols = [col for col in micro_df.columns if col != "market_ticker"]
    for col in micro_cols:
        if col not in data_df.columns:
            continue
        data_df[col] = data_df[col].fillna(0.0)

    # Drop markets with too few trades
    data_df = data_df[data_df["total_trades"] >= args.min_trades].copy()

    # Identify training (settled) vs test (open) markets
    train_mask = data_df["realized_outcome"].notna().to_numpy()

    # ===================================================================
    # NON-KALSHI ML PRIOR (SPX / VIX based, Option B)
    # ===================================================================
    print("Building non-Kalshi ML prior from SPX/VIX…")
    q_ml_all = build_non_kalshi_ml_prior(data_df)
    print("Non-Kalshi ML prior complete — ready for hybrid fusion!")

    

    # Structural prior
    print("Computing structural prior…")
    q_struct_all = tempered_fractional_structural_probs(data_df, train_mask)

    # Hybrid prior via reliability weighting
    priors = compute_hybrid_priors(data_df, q_struct_all, q_ml_all, train_mask, eta_total=args.eta_total)
    print(
        f"Hybrid weights: lambda_struct={priors.lambda_struct:.3f}, "
        f"lambda_ml={priors.lambda_ml:.3f}, "
        f"TRAIN Brier: struct={priors.brier_struct:.6f}, ML={priors.brier_ml:.6f}"
    )

    # Posterior updates
    post = update_posteriors(data_df, priors, train_mask)

    # Evaluation on training set
    evaluate_models(data_df, train_mask, priors, post)

    # Add posterior probabilities to dataframe
    data_df["p_capopm_raw"] = post.p_raw
    data_df["p_capopm_full"] = post.p_full
    data_df["p_prior"] = priors.q_hybrid

    # Sort and print predictions for test markets
    test_df = data_df[~train_mask].copy()
    if not test_df.empty:
        test_cols = [
            "event_ticker",
            "market_ticker",
            "floor_strike",
            "cap_strike",
            "kalshi_implied_yes_prob",
            "p_prior",
            "p_capopm_raw",
            "p_capopm_full",
            "total_tickets",
        ]
        print("\n--- Predictions for open markets ---")
        print(test_df[test_cols].sort_values("market_ticker").to_string(index=False))
    else:
        print("\nNo open markets to predict.")


if __name__ == "__main__":
    main()