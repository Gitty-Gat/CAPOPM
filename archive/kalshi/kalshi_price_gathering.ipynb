{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from datetime import date, timedelta\n",
    "import pandas as pd\n",
    "\n",
    "try:\n",
    "    import requests\n",
    "    HAS_REQUESTS = True\n",
    "except ImportError:\n",
    "    import urllib.request\n",
    "    HAS_REQUESTS = False\n",
    "\n",
    "\n",
    "def daterange(start_date: date, end_date: date):\n",
    "    \n",
    "    d = start_date\n",
    "    while d <= end_date:\n",
    "        yield d\n",
    "        d += timedelta(days=1)\n",
    "\n",
    "def build_kalshi_url(kind: str, d: date) -> str:\n",
    "    \"\"\"\n",
    "    kind: 'market_data' or 'trade_data'\n",
    "    \"\"\"\n",
    "    ds = d.strftime(\"%Y-%m-%d\")\n",
    "    return f\"https://kalshi-public-docs.s3.amazonaws.com/reporting/{kind}_{ds}.json\"\n",
    "\n",
    "def fetch_json(url: str, timeout: int = 30):\n",
    "    \"\"\"Fetch JSON from URL, return parsed object, or None if not found / error.\"\"\"\n",
    "\n",
    "    try:\n",
    "        if HAS_REQUESTS:\n",
    "            r = requests.get(url, timeout=timeout)\n",
    "            if r.status_code != 200:\n",
    "                return None\n",
    "            return r.json()\n",
    "        else:\n",
    "            with urllib.request.urlopen(url, timeout=timeout) as resp:\n",
    "                if resp.status != 200:\n",
    "                    return None\n",
    "                return json.loads(resp.read().decode(\"utf-8\"))\n",
    "    except Exception:\n",
    "        return None\n",
    "    \n",
    "\n",
    "def json_to_frames(obj):\n",
    "\n",
    "    \"\"\"\n",
    "    converts kalshi json data to one or more dataframes\n",
    "    \"\"\"\n",
    "\n",
    "    frames = {}\n",
    "\n",
    "    if obj is None:\n",
    "        return frames\n",
    "    \n",
    "    if isinstance(obj, list):\n",
    "        frames[\"root\"] = pd.json_normalize(obj)\n",
    "        return frames\n",
    "    if isinstance(obj, dict):\n",
    "        found_list_tables = False\n",
    "        for k, v in obj.items():\n",
    "            if isinstance(v, list) and len(v) > 0 and isinstance(v[0], dict):\n",
    "                frames[k] = pd.json_normalize(v)\n",
    "                found_list_tables = True\n",
    "\n",
    "        if found_list_tables:\n",
    "            return frames\n",
    "        \n",
    "        frames[\"root\"] = pd.json_normalize(obj)\n",
    "        return frames\n",
    "    \n",
    "    frames[\"root\"] = pd.DataFrame({\"raw\": [str(obj)]})\n",
    "    return frames\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#---CONFIG---\n",
    "KIND = \"market_data\"\n",
    "START_DATE = date(2025, 9, 17)\n",
    "END_DATE = date(2025, 12, 7)\n",
    "\n",
    "TICKERS = [\"KXNCAAFPLAYOFF-25\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_daily_frames = []\n",
    "missing_days = []\n",
    "\n",
    "for d in daterange(START_DATE, END_DATE):\n",
    "    url = build_kalshi_url(KIND, d)\n",
    "    obj = fetch_json(url)\n",
    "\n",
    "    if obj is None:\n",
    "        missing_days.append(d)\n",
    "        continue\n",
    "\n",
    "    frames = json_to_frames(obj)\n",
    "\n",
    "\n",
    "\n",
    "    if len(frames) == 0:\n",
    "        missing_days.append(d)\n",
    "        continue\n",
    "\n",
    "    best_key = max(frames.keys(), key=lambda k: len(frames[k]))\n",
    "    df = frames[best_key].copy()\n",
    "    df[\"as_of_date\"] = pd.to_datetime(d)\n",
    "\n",
    "    raw_daily_frames.append(df)\n",
    "\n",
    "all_data = pd.concat(raw_daily_frames, ignore_index=True) if raw_daily_frames else pd.DataFrame()\n",
    "\n",
    "print(\"Rows:\", len(all_data))\n",
    "print(\"Cols:\", len(all_data.columns))\n",
    "print(\"Missing days:\", len(missing_days))\n",
    "\n",
    "if missing_days:\n",
    "    print(\"First few missing:\", missing_days[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_data.columns.tolist()[:60]\n",
    "all_data.head(3) \n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "possible_ticker_cols = [\"ticker_name\", \"ticker\", \"market_ticker\", \"event_ticker\"]\n",
    "\n",
    "ticker_col = next((c for c in possible_ticker_cols if c in all_data.columns), None)\n",
    "if ticker_col is None:\n",
    "    raise ValueError(\n",
    "        f\"Couldn't find a ticker column. Looked for {possible_ticker_cols}. \"\n",
    "        f\"Available columns include: {all_data.columns.tolist()[:50]}\"\n",
    "    )\n",
    "\n",
    "filtered = all_data[all_data[ticker_col].isin(TICKERS)].copy()\n",
    "\n",
    "print(\"Ticker column used:\", ticker_col)\n",
    "print(\"Filtered rows:\", len(filtered))\n",
    "filtered.head(3)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "out_path = f\"kalshi_{KIND}_{START_DATE}_{END_DATE}_filtered.csv\".replace(\":\", \"-\")\n",
    "filtered.to_csv(out_path, index=False)\n",
    "out_path\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
