\section{Extended Module A: Likelihood and Conjugate Updating}

In this phase, we derive the likelihood generated by parimutuel YES/NO
trader actions and combine it with the hybrid prior from Phase~2
to construct the posterior distribution of the event probability
\[
p := Q(A) = Q(S_T > K).
\]
All notation, including the hybrid prior parameters \((\alpha_0,\beta_0)\),
follows Phase~2.


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection*{4.1 Trader Actions and the Likelihood Model}

Let \(n\) denote the number of traders participating in the parimutuel book,
and let \(y\) denote the number of YES positions.
A NO position is treated as a vote for the complement \(A^c\).

Traders may act strategically: some may exaggerate their signals,
herd behind early order flow, or attempt to manipulate the book.
We acknowledge the possibility of such distortions but defer
correction to Phase~6.
For the purposes of likelihood construction,
we treat the realized counts \((y,n-y)\) as the observable actions
that the mechanism must interpret.

Although traders' private signals may be correlated,
and their actions may be strategically dependent,
we assume conditional independence given the latent probability \(p\)
for the sole purpose of deriving the Beta--Binomial conjugacy.
This follows standard practice in Bayesian market microfoundations.


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection*{4.2 Binomial Likelihood of the Parimutuel Order Flow}

Conditionally on the latent event probability \(p\),
we model the YES count \(y\) as
\[
y \mid p \sim \mathrm{Binomial}(n,p),
\]
with likelihood
\begin{equation}
L(y\mid p)
= \binom{n}{y}\, p^y (1-p)^{n-y}.
\label{eq:BinomLik}
\end{equation}

Although later phases introduce liquidity-adjusted counts \(y^\ast\)
and \(n^\ast\), the present phase uses the raw counts
\((y,n-y)\) for the purpose of deriving the conjugate update.


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection*{4.3 Conjugate Updating with the Hybrid Prior}

The hybrid prior from Phase~2 is
\[
p \sim \mathrm{Beta}(\alpha_0,\beta_0),
\qquad
\alpha_0, \beta_0 > 0.
\]

\begin{lemma}[Posterior Form]
Combining the Beta prior with the Binomial likelihood 
(equation reference) yields the posterior
\[
p \mid y \sim 
\mathrm{Beta}(\alpha_{\mathrm{post}},\beta_{\mathrm{post}}),
\]
where
\[
\alpha_{\mathrm{post}} = \alpha_0 + y,
\qquad
\beta_{\mathrm{post}} = \beta_0 + (n-y).
\]
\end{lemma}

\begin{proof}
The Beta density is proportional to
\(p^{\alpha_0-1}(1-p)^{\beta_0-1}\).
Multiplying by (equation reference) gives a kernel proportional to
\[
p^{\alpha_0 + y -1}(1-p)^{\beta_0 + (n-y) -1},
\]
    the kernel of a Beta\((\alpha_{\mathrm{post}},\beta_{\mathrm{post}})\).
\end{proof}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection*{4.4 Posterior Mean and Variance}

\begin{prop}[Posterior Moments]
For the posterior Beta distribution above,
\[
\mathbb{E}[p\mid y]
= 
\frac{\alpha_{\mathrm{post}}}
     {\alpha_{\mathrm{post}}+\beta_{\mathrm{post}}},
\]
and
\[
\operatorname{Var}(p\mid y)
=
\frac{\alpha_{\mathrm{post}}\beta_{\mathrm{post}}}
     {(\alpha_{\mathrm{post}}+\beta_{\mathrm{post}})^2
      (\alpha_{\mathrm{post}}+\beta_{\mathrm{post}}+1)}.
\]
\end{prop}

\begin{proof}
These are standard properties of the Beta distribution.
\end{proof}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection*{4.5 Posterior Predictive Distribution (Beta--Binomial Form)}

The posterior predictive distribution of observing \(y\) YES votes
under the prior \(\mathrm{Beta}(\alpha_0,\beta_0)\) is
\[
P(y \mid \alpha_0,\beta_0)
=
\binom{n}{y}\,
\frac{B(\alpha_0+y,\ \beta_0+n-y)}
     {B(\alpha_0,\beta_0)},
\]
where \(B(\cdot,\cdot)\) is the Beta function.

\begin{theorem}[Posterior Predictive Distribution]
Let \(p\sim \mathrm{Beta}(\alpha_0,\beta_0)\)
and \(y\mid p\sim \mathrm{Binomial}(n,p)\).
Then the marginal distribution of \(y\) is Beta--Binomial with pmf above.
\end{theorem}

\begin{proof}
Integrate the joint distribution 
\(P(y\mid p)f(p)\)
over \(p\in[0,1]\),
and use the identity
\[
\int_0^1 p^{a-1}(1-p)^{b-1}\,dp = B(a,b).
\]
\end{proof}

\subsection*{4.6 Conditional Independence as a Modeling Approximation}

The Beta--Binomial updating step presented above relies on the assumption that,
conditional on the latent event probability $p$, individual trader actions
$s_i \in \{\mathrm{YES},\mathrm{NO}\}$ are independent Bernoulli draws.  
Such conditional independence is standard in Bayesian aggregation models, but it
is not expected to hold exactly in parimutuel markets where traders may observe
and react to earlier order flow. In particular, herding behavior generates
temporal dependence among trades: late traders may overweight recent order
patterns even when those patterns do not reflect new private information.

In this framework, conditional independence is therefore best interpreted as a
\emph{modeling approximation} rather than a literal behavioral assumption.
Empirically observed violations of independence are handled in two ways:

\begin{enumerate}
\item \textbf{Behavioral Adjustment (Phase~6).}  
      The Stage~1 correction introduces weights $w_i^{\mathrm{beh}}$ applied to
      individual orders. When herding creates clusters of correlated trades,
      these weights reduce the effective contribution of late correlated orders,
      mitigating departures from independence.

\item \textbf{Sensitivity Analysis (Phase~7).}  
      Simulation regimes in Phase~7 introduce explicit dependence structures
      among trades, including herding and correlated decision rules. These
      regimes allow us to evaluate how violations of independence affect the
      PRISM posterior and how effectively the two-stage bias-correction layer
      controls such deviations.
\end{enumerate}

This modeling approximation preserves conjugacy and analytic tractability while
acknowledging that the empirical behavior of order flow contains richer
dependence patterns. Phases~6 and~7 are specifically designed to examine,
interpret, and correct these dependencies.


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection*{4.7 Incorporation of Strategic Distortion}

Because traders may submit exaggerated or strategically distorted orders,
the raw counts \((y,n-y)\) encode:
\begin{enumerate}
\item private signals,
\item beliefs about other traders' signals,
\item strategic considerations,
\item liquidity constraints.
\end{enumerate}

Phase~6 will introduce formal bias adjustments using 
\emph{effective} counts \((y^\ast, n^\ast-y^\ast)\)
and distortion offsets \((\delta_+,\delta_-)\).
For now, the posterior above represents the
\emph{unadjusted} Bayesian update based on the observable order flow.

\subsection*{4.8 Conditional i.i.d., Dependence, and the Role of Weights}

Assumption~A4 models the adjusted order contributions as conditionally i.i.d.\
Bernoulli (or bounded) signals given the latent event probability $p$.
This is a deliberate simplification.  In realistic markets, herding,
order-splitting, and informational cascades induce dependence across orders.

There are two conceptually distinct modeling choices:
\begin{itemize}
\item \emph{Fully dependent likelihood.}
One could specify an explicit joint law for the order sequence
$(Z_1,\dots,Z_n)$, for example via an Ising model or a Markov random field.
This yields a non-factorizing likelihood and a non-conjugate posterior that
typically requires MCMC or variational methods.

\item \emph{Weighted pseudo-likelihood.}
PRISM instead uses a Beta--Binomial update based on adjusted counts
$(y_n^\ast,n_n^\ast)$, together with mixing-based asymptotics (Phase~8).
This implicitly replaces the true dependent likelihood with an exponential
family surrogate whose sufficient statistics are the weighted sums.  The
resulting Beta posterior is then interpreted as the KL-projection of the
intractable posterior onto the Beta family (Phase~8ZZ).
\end{itemize}

In this sense, the behavioral weighting layer is not merely a ``patch'' on an
i.i.d.\ model, but a way to summarize dependence and heterogeneity into
effective counts that remain compatible with a tractable exponential family
update.  The trade-off is explicit: PRISM sacrifices an exact likelihood for
closed-form inference plus an information-theoretic guarantee that, within the
Beta family, the posterior is as close as possible (in Kullback--Leibler
sense) to the ideal but intractable posterior.

\subsection*{4.9 Why PRISM Uses Beta Conjugacy}

There are many ways to model belief aggregation in markets.  PRISM uses a
Beta--Binomial structure for a simple reason: it gives closed-form updates and
keeps the link between data and parameters transparent.

\begin{itemize}
\item Each component of the prior can be read as a pseudo-count: structural
      information contributes $\eta_{\mathrm{str}}$ virtual observations,
      the ML model contributes $n_{\mathrm{ML}}$, and the crowd contributes
      adjusted effective counts $n_n^\ast$.
\item Updating is a matter of adding these counts, with no numerical
      integration or sampling required.
\item The resulting posterior has a clear interpretation: it is the Beta
      distribution that best matches, in KL sense, the information contained
      in the adjusted counts and the hybrid prior.
\end{itemize}

More flexible approaches, such as MCMC over a fully dependent likelihood, can
capture richer structures but at the cost of interpretability and computation.
PRISM is intentionally positioned as a tractable, interpretable baseline: it
prioritizes closed-form inference and moment-based robustness over fully
nonparametric modeling.  This makes it easier to diagnose, explain, and test
in simulation before considering heavier alternatives.


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection*{4.10 Output of Phase 4}

The output of this phase is the posterior hyperparameter pair
\[
(\alpha_{\mathrm{post}},\beta_{\mathrm{post}}),
\]
which becomes the foundation for Phase~5 (posterior predictive pricing)
and is later refined in Phase~6 (bias–corrected posterior).


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section*{Phase 5. Posterior Predictive event-probability inference}

Given the posterior distribution of the event probability
\[
p := Q(A)=Q(S_T>K)
\]
from Phase~4, we now derive the posterior summaries
of YES/NO parimutuel contracts and digital derivatives.
We additionally establish no--arbitrage properties, uncertainty bounds,
and risk--adjusted pricing rules.

\[
p\mid y \sim \mathrm{Beta}(\alpha_{\mathrm{post}},\beta_{\mathrm{post}}),
\]
with
\[
\alpha_{\mathrm{post}}=\alpha_0+y,\qquad
\beta_{\mathrm{post}}=\beta_0+(n-y).
\]


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\subsection*{5.1 From PRISM Posteriors to projection kernels and Option Prices}

In previous phases, PRISM produces, for a fixed maturity $T$, a posterior 
distribution for the event probability
\[
p(K,T)
=
Q(S_T > K \mid \mathcal{I}),
\]
where $\mathcal{I}$ denotes the combined information from the structural 
model, the machine learning prior, and the adjusted parimutuel order flow.
Let $\hat{p}(K,T)$ denote the posterior mean, so that the PRISM-implied 
digital price at strike $K$ is
\[
D_{\\mathrm{PRISM}}(K,T)
=
e^{-rT} \hat{p}(K,T),
\]
with $r$ the risk-free rate.

We now show how the entire reference-measure distribution and projection kernel can be 
recovered (at least formally) from the strike-dependent posterior, and how 
vanilla option prices follow via standard integral transforms.

\begin{theorem}[PRISM-Implied reference-measure CDF, Density, and projection kernel]
Fix a maturity $T>0$ and suppose that, for each strike $K\ge 0$, PRISM 
produces a posterior distribution for $p(K,T)$ with mean $\hat{p}(K,T)$.  
Assume:

\begin{enumerate}
\item[(i)] (\textbf{Smoothness in Strike})  
The map $K\mapsto \hat{p}(K,T)$ is differentiable almost everywhere, with 
$\hat{p}(K,T)$ non-increasing in $K$ and
\[
\lim_{K\to 0} \hat{p}(K,T) = 1,
\qquad
\lim_{K\to\infty} \hat{p}(K,T) = 0.
\]
\item[(ii)] (\textbf{No-Arbitrage Regularity})  
The function $\hat{p}(K,T)$ is right-continuous with left limits and defines a 
valid tail function for a probability distribution on $[0,\infty)$.
\end{enumerate}

Then:

\begin{enumerate}
\item[(a)] (\textbf{reference-measure CDF and Density})  
The PRISM-implied reference-measure CDF and density at maturity $T$ are given by
\[
F^{Q}_{\\mathrm{PRISM}}(K,T)
=
Q(S_T \le K \mid \mathcal{I})
=
1 - \hat{p}(K,T),
\]
and, wherever differentiable,
\[
f^{Q}_{\\mathrm{PRISM}}(K,T)
=
\frac{\partial}{\partial K} F^{Q}_{\\mathrm{PRISM}}(K,T)
=
-\,\frac{\partial}{\partial K} \hat{p}(K,T).
\]

\item[(b)] (\textbf{Call Prices and Breeden--Litzenberger})  
The PRISM-implied call price at strike $K$ and maturity $T$ is
\[
C_{\\mathrm{PRISM}}(K,T)
=
e^{-rT}
\int_K^\infty (s-K)\, f^{Q}_{\\mathrm{PRISM}}(s,T)\, ds.
\]
Equivalently, if we define
\[
C_{\\mathrm{PRISM}}(K,T)
=
e^{-rT} \mathbb{E}^Q[(S_T-K)^+ \mid \mathcal{I}],
\]
then the Breeden--Litzenberger relation holds:
\[
\frac{\partial^2}{\partial K^2} C_{\\mathrm{PRISM}}(K,T)
=
e^{-rT} f^{Q}_{\\mathrm{PRISM}}(K,T),
\]
whenever the derivatives exist.

\item[(c)] (\textbf{State-Price Density and projection kernel})  
The state-price density associated with PRISM at maturity $T$ is
\[
\phi_{\\mathrm{PRISM}}(s,T)
=
e^{-rT} f^{Q}_{\\mathrm{PRISM}}(s,T),
\]
so that
\[
C_{\\mathrm{PRISM}}(K,T)
=
\int_K^\infty (s-K)\, \phi_{\\mathrm{PRISM}}(s,T)\, ds.
\]
If $P$ denotes a physical measure under which $S_T$ has density $f^{P}$, and
if $f^{P}$ is strictly positive on the support of $f^{Q}_{\\mathrm{PRISM}}$, then 
the PRISM-implied projection kernel can be written (up to normalization) as
\[
m_{\\mathrm{PRISM}}(s,T)
\propto
\frac{f^{Q}_{\\mathrm{PRISM}}(s,T)}{f^{P}(s)}.
\]
\end{enumerate}
\end{theorem}

\begin{proof}[Proof (Sketch)]
Under (i) and (ii), the function $K\mapsto \hat{p}(K,T)$ satisfies the basic 
properties of a strike-tail function:
it is non-increasing, right-continuous, and converges to 1 and 0 at the 
boundaries.  Thus it defines
\[
F^{Q}_{\\mathrm{PRISM}}(K,T) = 1 - \hat{p}(K,T)
\]
as a valid CDF on $[0,\infty)$, and the density 
$f^{Q}_{\\mathrm{PRISM}} = \partial_K F^{Q}_{\\mathrm{PRISM}}$ exists almost 
everywhere, yielding part (a).

For part (b), the standard reference-measure pricing relation gives
\[
C_{\\mathrm{PRISM}}(K,T)
=
e^{-rT} \int_K^\infty (s-K)\, f^{Q}_{\\mathrm{PRISM}}(s,T)\, ds.
\]
Differentiating once with respect to $K$ yields
\[
\frac{\partial}{\partial K} C_{\\mathrm{PRISM}}(K,T)
=
- e^{-rT} \int_K^\infty f^{Q}_{\\mathrm{PRISM}}(s,T)\, ds
=
- e^{-rT} (1 - F^{Q}_{\\mathrm{PRISM}}(K,T)),
\]
and differentiating a second time gives
\[
\frac{\partial^2}{\partial K^2} C_{\\mathrm{PRISM}}(K,T)
=
e^{-rT} f^{Q}_{\\mathrm{PRISM}}(K,T),
\]
which is the Breeden--Litzenberger formula applied to the PRISM-implied 
density.

For part (c), the state-price density is by definition the Radon--Nikodym
derivative of the pricing operator with respect to Lebesgue measure, which in 
continuous-time arbitrage-free settings is $e^{-rT} f^{Q}(s,T)$.  Substituting 
$f^{Q}_{\\mathrm{PRISM}}$ yields $\phi_{\\mathrm{PRISM}}$.  When a physical density 
$f^{P}$ is available, absolute continuity of $Q$ with respect to $P$ on the 
relevant support implies
\[
\frac{dQ}{dP}(s)
\propto
\frac{f^{Q}_{\\mathrm{PRISM}}(s,T)}{f^{P}(s)},
\]
so that
$m_{\\mathrm{PRISM}}(s,T) \propto dQ/dP$ has the indicated form.
\end{proof}

\begin{remark}[Discrete Approximation from a Strike Grid]
In practice, PRISM will produce posterior means 
$\hat{p}(K_j,T)$ on a discrete grid of strikes $\{K_j\}_{j=1}^J$.  The 
PRISM-implied CDF and density can then be approximated by
\[
F^{Q}_{\\mathrm{PRISM}}(K_j,T)
\approx
1 - \hat{p}(K_j,T),
\]
and
\[
f^{Q}_{\\mathrm{PRISM}}(K_j,T)
\approx
-\,\frac{\hat{p}(K_{j+1},T) - \hat{p}(K_j,T)}{K_{j+1}-K_j},
\]
for $j=1,\dots,J-1$.  Similarly, the call price curve can be approximated via
\[
C_{\\mathrm{PRISM}}(K_j,T)
\approx
\sum_{l=j}^{J-1}
e^{-rT}\,
\hat{p}(K_l,T)\,
\Delta K_l,
\qquad
\Delta K_l = K_{l+1}-K_l,
\]
which implements the integral in (b) as a Riemann sum.  These discrete 
approximations provide a direct pathway from PRISM posteriors to implied 
call prices and densities on a finite strike grid.
\end{remark}

\subsubsection*{5.1.1 PRISM-Implied Kernels and Asset Pricing Puzzles}

The PRISM framework produces a reference-measure distribution
$f^Q_{\\mathrm{PRISM}}(\cdot,T)$ and associated state-price density
$\phi_{\\mathrm{PRISM}}(s,T) = e^{-rT} f^Q_{\\mathrm{PRISM}}(s,T)$.
This section connects these objects to empirical projection kernels and the
stylized facts of asset pricing puzzles.

\label{subsec:kernel_regularization}

The final PRISM outputs in Phase~5 are option prices computed under a
reference-measure measure $\mathbb{Q}$ induced by a projection kernel (state--price
density) $M_T = \frac{d\mathbb{Q}}{d\mathbb{P}}\big\vert_{\mathcal{F}_T}$. If
the kernel derived from a misspecified model or from a raw mixture/particle
posterior is not carefully regularized, it may violate basic conditions:
\emph{positivity}, \emph{unit expectation}, or the \emph{martingale} property
for discounted asset prices. This subsection introduces a kernel
enforces no--arbitrage while preserving the posterior information accumulated
in Phases~2--6.

Recall from Phase~1 that under the physical measure $\mathbb{P}$ the asset
model:
\begin{align}
\end{align}
where $(W_t^S, W_t^v)$ are Brownian motions under $\mathbb{P}$ with correlation
$\rho \in [-1,1]$, and parameters $(\mu,\kappa,\theta,\sigma)$ lie in the
$\Pi_\phi(\cdot \mid \mathcal{D})$ from Phases~5--6 is interpreted as
providing information about the terminal distribution of $S_T$ and related
events (e.g.\ default, barrier crossing) rather than replacing the
structural dynamics.

\paragraph{Exponential Martingale projection kernel.}
We construct a projection kernel as an exponential martingale with a market--price
of risk process $\lambda_t$:
\begin{equation}
  M_t^\lambda
  := \exp\!\left(
        - \int_0^t \lambda_s \,dW_s^S
        - \frac{1}{2} \int_0^t \lambda_s^2 \,ds
      \right),
  \qquad t \in [0,T],
  \label{eq:kernel_M_lambda}
\end{equation}
where $(\lambda_t)_{t \in [0,T]}$ is progressively measurable and adapted to
$(\mathcal{F}_t)$.

\begin{assumption}[Novikov Condition for the Kernel]
\label{assumption:novikov}
The process $\lambda_t$ satisfies Novikov's condition:
\[
  \mathbb{E}_{\mathbb{P}}\!\left[
    \exp\!\left(
       \frac{1}{2} \int_0^T \lambda_s^2 \,ds
    \right)
  \right] < \infty.
\]
\end{assumption}

\begin{lemma}[Positivity and Normalization of the Kernel]
\label{lemma:kernel_martingale}
Under Assumption~reference, the process $M_t^\lambda$ defined
in~(equation reference) is a positive $\mathbb{P}$--martingale with
$M_0^\lambda = 1$ and
\[
  \mathbb{E}_{\mathbb{P}}[M_T^\lambda \mid \mathcal{F}_0] = 1.
\]
Consequently, defining $\mathbb{Q}$ by
\[
  \frac{d\mathbb{Q}}{d\mathbb{P}}
  \Big\vert_{\mathcal{F}_T}
  = M_T^\lambda
\]
yields a probability measure equivalent to $\mathbb{P}$.
\end{lemma}

\begin{proof}
By Assumption~reference, the stochastic exponential
$M_t^\lambda$ is a true martingale (Novikov's condition). It is strictly
positive by definition and satisfies $M_0^\lambda = 1$. Thus
$\mathbb{E}_{\mathbb{P}}[M_T^\lambda \mid \mathcal{F}_0] = M_0^\lambda =1$,
and $M_T^\lambda$ defines a Radon--Nikodym derivative of a probability measure
$\mathbb{Q}$ equivalent to $\mathbb{P}$.
\end{proof}

Under $\mathbb{Q}$, Girsanov's theorem implies that
\[
  W_t^{S,\mathbb{Q}} = W_t^S + \int_0^t \lambda_s \,ds
\]
\[
  dS_t = S_t \big( (\mu - \lambda_t \sqrt{v_t})\,dt
                 + \sqrt{v_t}\, dW_t^{S,\mathbb{Q}} \big).
\]

\begin{definition}[Drift Adjustment and No--Arbitrage]
\label{def:drift_adjustment_noarb}
Let $r_t$ denote the short rate. To ensure that the discounted price
$\tilde{S}_t := e^{-\int_0^t r_s ds} S_t$ is a $\mathbb{Q}$--martingale, we
choose $\lambda_t$ such that
\[
  \mu - \lambda_t \sqrt{v_t} = r_t,
  \quad\text{i.e.}\quad
  \lambda_t = \frac{\mu - r_t}{\sqrt{v_t}}.
\]
With this choice, the $S_t$ dynamics under $\mathbb{Q}$ become
\[
  dS_t = S_t \big( r_t \,dt + \sqrt{v_t}\, dW_t^{S,\mathbb{Q}} \big),
\]
ensuring no--arbitrage in the usual sense.
\end{definition}

\paragraph{Esscher-Type Calibration to PRISM Posterior Moments.}
The specification in Definition~reference yields a
one--parameter family of kernels indexed by the physical drift $\mu$ and the
short rate $r_t$. To incorporate the information contained in the PRISM
posterior $\Pi_{\phi}(\cdot \mid \mathcal{D})$, we calibrate $\lambda_t$
(or, equivalently, an Esscher tilt parameter) so that selected posterior moments
posterior.

Let $X_T := \log S_T$ and define an Esscher--type kernel based on $X_T$:
\[
  M_T^\theta
  := \frac{\exp(\theta X_T)}{
        \mathbb{E}_{\mathbb{P}}[\exp(\theta X_T) \mid \mathcal{F}_0]
      }.
\]
By construction, $M_T^\theta > 0$ and
$\mathbb{E}_{\mathbb{P}}[M_T^\theta \mid \mathcal{F}_0] = 1$. For a given
Esscher parameter $\theta$, this defines a reference-measure measure
$\mathbb{Q}^\theta$ via $\frac{d\mathbb{Q}^\theta}{d\mathbb{P}} = M_T^\theta$.
We choose $\theta$ (or a small vector of tilting parameters) to match
posterior--implied moments from PRISM, e.g.
\[
  \mathbb{E}_{\mathbb{Q}^\theta}[S_T \mid \mathcal{F}_0]
  = \mathbb{E}_{\Pi_\phi}[S_T \mid \mathcal{D}],
  \qquad
  \mathbb{E}_{\mathbb{Q}^\theta}[\mathbf{1}\{S_T > K\} \mid \mathcal{F}_0]
  = \mathbb{E}_{\Pi_\phi}[\mathbf{1}\{S_T > K\} \mid \mathcal{D}],
\]
for one or more strikes $K$. This calibration step projects the raw PRISM
without violating positivity or the martingale property.

\begin{theorem}[Kernel Regularization and No--Arbitrage Preservation]
\label{thm:kernel_regularization_noarb}
Let $\Pi_\phi(\cdot \mid \mathcal{D})$ be the fully corrected PRISM posterior
from Phases~5--6. Define a regularized projection kernel either as:
\begin{enumerate}
  \item[(a)] an exponential martingale $M_t^\lambda$ as in
        (equation reference) with $\lambda_t$ chosen according to
        Definition~reference, or
  \item[(b)] an Esscher--type density $M_T^\theta$ based on $X_T = \log S_T$ with
        $\theta$ calibrated to match a set of PRISM posterior moments.
\end{enumerate}
Assume Novikov's condition (Assumption~reference) holds for the
chosen $\lambda_t$ or that $M_T^\theta$ has finite exponential moments under
$\mathbb{P}$. Then:
\begin{enumerate}
  \item[(i)] $M_T$ is strictly positive and satisfies
        $\mathbb{E}_{\mathbb{P}}[M_T \mid \mathcal{F}_0] = 1$,
        so it defines a valid projection kernel.
  \item[(ii)] The discounted price process $\tilde{S}_t = e^{-\int_0^t r_s ds} S_t$
        is a $\mathbb{Q}$--martingale for the induced reference-measure measure
        $\mathbb{Q}$.
  \item[(iii)] Option prices computed as
        \[
          C(K,T)
          = e^{-\int_0^T r_s ds}
            \mathbb{E}_{\mathbb{Q}}\big[(S_T - K)^+ \mid \mathcal{F}_0\big]
        \]
        PRISM posterior in the sense that their moments agree with the
        posterior--implied targets used in calibration.
\end{enumerate}
\end{theorem}

\begin{proof}[Proof Sketch]
(i) Positivity and normalization follow from
Lemma~reference in the exponential martingale case and by
construction in the Esscher case. (ii) The drift adjustment in
Definition~reference ensures that $S_t$ has drift $r_t$
under $\mathbb{Q}$, so $\tilde{S}_t$ is a local martingale; integrability
conditions (e.g.\ Novikov, uniform integrability) promote it to a true
martingale. (iii) Option prices under $\mathbb{Q}$ inherit no--arbitrage from
the standard reference-measure valuation framework. The Esscher calibration
conditions guarantee that chosen moments (e.g.\ of $S_T$ or digital payoffs)
with the PRISM posterior without violating the martingale and positivity
constraints.
\end{proof}

\begin{remark}[Integration with Simulation and Robustness Phases]
\label{remark:kernel_integration}
In Phase~7, Monte Carlo simulations of $(S_t,v_t)$ under the regularized kernel
$M^\lambda$ or $M^\theta$ can be used to assess the stability of option prices
and digital probabilities under parameter uncertainty and posterior
perturbations. In Phase~8, the robustness results for the posterior (e.g.\ in
Wasserstein or Hellinger distance) combined with the exponential kernel
representation yield explicit bounds on the sensitivity of reference-measure prices
to data and model perturbations. Crucially, kernel regularization is applied
\emph{after} the event--probability posterior has been corrected for nonlinear,
dependent, and multimodal effects, so that enforcing no--arbitrage does not
undo the informational gains of PRISM but instead embeds them into a
structurally consistent, coherence-preserving projection measure.
\end{remark}


\subsection*{5.2 Posterior Predictive Mean and Variance}

\begin{lemma}[Posterior Mean and Variance]
For a Beta\((\alpha_{\mathrm{post}},\beta_{\mathrm{post}})\) posterior,
\[
\hat{p}_{\mathrm{post}}
:= \mathbb{E}[p\mid y]
=
\frac{\alpha_{\mathrm{post}}}
     {\alpha_{\mathrm{post}}+\beta_{\mathrm{post}}},
\]
\[
\operatorname{Var}(p\mid y)
=
\frac{\alpha_{\mathrm{post}}\beta_{\mathrm{post}}}
     {(\alpha_{\mathrm{post}}+\beta_{\mathrm{post}})^2
      (\alpha_{\mathrm{post}}+\beta_{\mathrm{post}}+1)}.
\]
\end{lemma}

\begin{proof}
Standard Beta distribution identities.
\end{proof}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection*{5.3 Posterior Predictive Distribution for Digital Outcomes}

Let \(Z\) denote the payoff of a YES contract:
\[
Z = \mathbbm{1}\{A\}.
\]

\begin{prop}[Posterior Predictive Distribution of Digital Payoff]
The posterior--predictive distribution of \(Z\) is Bernoulli with mean
\[
\pi_{\mathrm{pred}}
=
\mathbb{E}[Z\mid y]
=
\hat{p}_{\mathrm{post}}
=
\frac{\alpha_{\mathrm{post}}}
     {\alpha_{\mathrm{post}}+\beta_{\mathrm{post}}}.
\]
\end{prop}

\begin{proof}
Since \(Z\mid p \sim \mathrm{Bernoulli}(p)\),
\[
\mathbb{E}[Z\mid y]
=
\mathbb{E}[\mathbb{E}[Z\mid p,y]\mid y]
=
\mathbb{E}[p\mid y]
=
\hat{p}_{\mathrm{post}}.
\]
\end{proof}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection*{5.4 Posterior Predictive Prices of Parimutuel YES/NO Contracts}

A YES contract pays \(1\) if \(A\) occurs and \(0\) otherwise.  
Under reference-measure valuation, its fair price is the posterior predictive mean.

\begin{theorem}[Arbitrage--Free YES/NO Pricing]
The posterior--predictive fair prices of YES and NO contracts are
\[
\pi_{\mathrm{YES}} = \hat{p}_{\mathrm{post}}, \qquad
\pi_{\mathrm{NO}} = 1 - \hat{p}_{\mathrm{post}}.
\]
They satisfy the no--arbitrage identity:
\[
\pi_{\mathrm{YES}} + \pi_{\mathrm{NO}} = 1.
\]
\end{theorem}

\begin{proof}
Follows immediately from 
\(\pi_{\mathrm{YES}}=\mathbb{E}[Z\mid y]\)
and \(1-Z=\mathbbm{1}\{A^c\}\).
\end{proof}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection*{5.5 Properties of posterior summaries}

\begin{prop}[Monotonicity in YES Votes]
The price \(\pi_{\mathrm{YES}}=\alpha_{\mathrm{post}}/(\alpha_{\mathrm{post}}+\beta_{\mathrm{post}})\)
is strictly increasing in the count \(y\).
\end{prop}

\begin{proof}
Since \(\alpha_{\mathrm{post}}=\alpha_0+y\),
\[
\frac{\partial}{\partial y}
\frac{\alpha_0+y}{\alpha_0+\beta_0+n}
>0.
\]
\end{proof}

\begin{prop}[Continuity]
The price is continuous in both \(\alpha_{\mathrm{post}}\) and \(\beta_{\mathrm{post}}\)
and therefore in \(y\) and \(n\).
\end{prop}

\begin{proof}
Rational function of continuous arguments.
\end{proof}




\subsection*{5.6 Mixture Posterior Extension for Multimodal Beliefs}
\label{subsec:mixture_posterior_extension}

The baseline PRISM framework represents the posterior distribution of the
event probability $p$ by a single Beta distribution. This is appropriate when
the likelihood is approximately unimodal and the crowd can be described by a
single effective subpopulation. Once nonlinear structural distortions
(Phase~6) and heterogeneous trader behavior are admitted, the true posterior
often becomes \emph{multimodal}. In such settings, forcing a unimodal Beta
posterior, or even a single Beta built on a misspecified likelihood, can lead
to severely miscalibrated probabilities and distorted pricing.

To represent multimodality explicitly, we extend the PRISM posterior to a
finite mixture of Betas constructed via stacking of multiple candidate
submodels or strata (e.g.\ trader types, structural regimes, or segmentation
by liquidity conditions).

\begin{assumption}[Candidate Submodels and Stratified Posteriors]
\label{assumption:candidate_models}
Let $\{ \mathcal{M}_k : k = 1,\dots,K \}$ be a finite collection of candidate
PRISM submodels. Each $\mathcal{M}_k$ is defined by:
\begin{enumerate}
  \item[(i)] a data subset or stratum $\mathcal{D}_k$ (e.g.\ orders from a given
        trader type, structural regime, or filtered particle trajectory);
  \item[(ii)] a Beta posterior on $p$ of the form
  \[
    p \mid \mathcal{D}_k \sim \mathrm{Beta}(\alpha_k, \beta_k),
  \]
  obtained from the usual Beta--Binomial update under $\mathcal{M}_k$;
  \item[(iii)] a corresponding predictive density $m_k(y)$ for new Bernoulli data
        $Y \in \{0,1\}$, given by the Beta--Binomial predictive:
  \[
    m_k(y) \;=\;
    \int_0^1 p^y (1-p)^{1-y} \,
      \mathrm{Beta}(\alpha_k,\beta_k)(dp)
    \;=\;
    \begin{cases}
      \displaystyle
      \frac{\beta_k}{\alpha_k + \beta_k} & \text{if } y = 0,\\[0.8em]
      \displaystyle
      \frac{\alpha_k}{\alpha_k + \beta_k} & \text{if } y = 1.
    \end{cases}
  \]
\end{enumerate}
We interpret each $\mathcal{M}_k$ as capturing one coherent ``mode'' of the
crowd's beliefs or structural environment.
\end{assumption}

\begin{definition}[Stacked Mixture Posterior over $p$]
\label{def:stacked_mixture_posterior}
Let $\mathcal{D}^{\mathrm{hold}} = \{Y_i^{\mathrm{hold}} : i=1,\dots,n_{\mathrm{hold}}\}$
be a holdout dataset (e.g.\ a subset of orders or markets not used to fit
$\alpha_k,\beta_k$). Define stacking weights $w = (w_1,\dots,w_K)$ in the
probability simplex
\[
  \Delta^{K-1} := \left\{ w \in [0,1]^K : \sum_{k=1}^K w_k = 1 \right\}
\]
by minimizing the negative log predictive likelihood of the stacked model:
\[
  w^\ast \;:=\;
  \operatorname*{arg\,min}_{w \in \Delta^{K-1}}
  \left\{
    - \sum_{i=1}^{n_{\mathrm{hold}}}
      \log \left(
        \sum_{k=1}^K w_k \, m_k\big(Y_i^{\mathrm{hold}}\big)
      \right)
  \right\}.
\]
The \emph{stacked mixture posterior} for $p$ is then defined as
\[
  \Pi_{\mathrm{mix}}(dp)
  \;:=\;
  \sum_{k=1}^K w_k^\ast \,
  \mathrm{Beta}(\alpha_k,\beta_k)(dp).
\]
\end{definition}

\begin{prop}[Posterior Mean and Predictive under the Mixture]
\label{prop:mixture_posterior_mean}
Under Assumption~reference and
Definition~reference, the stacked mixture posterior
$\Pi_{\mathrm{mix}}$ is a finite mixture of Beta distributions. Its mean and
predictive distribution are given by:
\begin{enumerate}
  \item[(i)] \textbf{Posterior mean of $p$:}
  \[
    \hat{p}_{\mathrm{mix}}
    \;:=\;
    \mathbb{E}_{\Pi_{\mathrm{mix}}}[p]
    \;=\;
    \sum_{k=1}^K w_k^\ast \,
      \frac{\alpha_k}{\alpha_k + \beta_k}.
  \]
  \item[(ii)] \textbf{Predictive distribution for a new Bernoulli outcome $Y$:}
  \[
    \mathbb{P}(Y=1)
    \;=\;
    \sum_{k=1}^K w_k^\ast \,
      \frac{\alpha_k}{\alpha_k + \beta_k},
    \qquad
    \mathbb{P}(Y=0)
    \;=\;
    \sum_{k=1}^K w_k^\ast \,
      \frac{\beta_k}{\alpha_k + \beta_k}.
  \]
\end{enumerate}
In particular, the mixture mean $\hat{p}_{\mathrm{mix}}$ can be used as the
crowd--adjusted event probability for YES/NO contracts, and the full mixture
distribution $\Pi_{\mathrm{mix}}$ can be propagated into posterior predictive
option prices as in the baseline PRISM construction.
\end{prop}

\begin{proof}
Since $\Pi_{\mathrm{mix}}$ is a finite convex combination of Beta distributions,
it is a well--defined probability measure on $[0,1]$. The expression for
$\hat{p}_{\mathrm{mix}}$ follows by linearity of expectation:
\[
  \hat{p}_{\mathrm{mix}}
  = \int_0^1 p \,\Pi_{\mathrm{mix}}(dp)
  = \sum_{k=1}^K w_k^\ast \int_0^1 p \,\mathrm{Beta}(\alpha_k,\beta_k)(dp)
  = \sum_{k=1}^K w_k^\ast \frac{\alpha_k}{\alpha_k + \beta_k}.
\]
The predictive probabilities follow similarly by integrating $p^y(1-p)^{1-y}$
with respect to $\Pi_{\mathrm{mix}}$ and using the Beta--Binomial formulas.
\end{proof}

The mixture posterior $\Pi_{\mathrm{mix}}$ explicitly retains multimodality when
the component posteriors $\mathrm{Beta}(\alpha_k,\beta_k)$ are well separated.
In particular, if some strata correspond to optimistic trader types and others
to pessimistic types (or different structural regimes), then $\Pi_{\mathrm{mix}}$
can exhibit multiple modes. The stacking weights $w_k^\ast$ tilt the mixture
toward components that perform better on the holdout set
$\mathcal{D}^{\mathrm{hold}}$, reducing sensitivity to any single misspecified
submodel.

\begin{definition}[Moment--Matched Single--Beta Approximation]
\label{def:moment_matched_beta}
For interpretability and analytical convenience, one may define a
moment--matched single--Beta approximation
$\mathrm{Beta}(\tilde{\alpha},\tilde{\beta})$ to the mixture posterior by
matching the first two moments:
\[
  \mathbb{E}[p] = \hat{p}_{\mathrm{mix}},
  \qquad
  \mathbb{V}\mathrm{ar}[p]
    = \sum_{k=1}^K w_k^\ast
        \frac{\alpha_k \beta_k}{(\alpha_k + \beta_k)^2 (\alpha_k + \beta_k + 1)}
      + \sum_{k=1}^K w_k^\ast
          \left(
            \frac{\alpha_k}{\alpha_k + \beta_k}
            - \hat{p}_{\mathrm{mix}}
          \right)^2.
\]
The approximation $\mathrm{Beta}(\tilde{\alpha},\tilde{\beta})$ is then chosen
such that
\[
  \frac{\tilde{\alpha}}{\tilde{\alpha}+\tilde{\beta}}
    = \hat{p}_{\mathrm{mix}},
  \qquad
  \frac{\tilde{\alpha}\tilde{\beta}}
       {(\tilde{\alpha}+\tilde{\beta})^2 (\tilde{\alpha}+\tilde{\beta}+1)}
    = \mathbb{V}\mathrm{ar}_{\Pi_{\mathrm{mix}}}[p].
\]
We emphasize that this is an \emph{approximation layer} used for convenience,
not an exact representation of $\Pi_{\mathrm{mix}}$.
\end{definition}

The next result formalizes a limitation: when the mixture posterior is
sufficiently multimodal, no single Beta distribution can uniformly approximate
it. This provides a theoretical warning against collapsing $\Pi_{\mathrm{mix}}$
to a single Beta in regimes of strong heterogeneity.

\begin{theorem}[No Unimodal Beta Can Uniformly Approximate a Strongly Multimodal Mixture]
\label{thm:beta_projection_impossibility}
Let $\Pi_{\mathrm{mix}}$ be a mixture of two Betas
\[
  \Pi_{\mathrm{mix}}(dp)
  = \tfrac{1}{2}\,\mathrm{Beta}(\alpha_1,\beta_1)(dp)
    + \tfrac{1}{2}\,\mathrm{Beta}(\alpha_2,\beta_2)(dp),
\]
with means $\mu_1 \neq \mu_2$ and variances $\sigma_1^2,\sigma_2^2$ bounded.
Assume that $|\mu_1 - \mu_2| \ge \varepsilon$ for some $\varepsilon > 0$, and
that each component is sharply concentrated around its mean (e.g.\ 
$\alpha_k+\beta_k$ large). Then there exists a constant $c(\varepsilon) > 0$
such that for any single Beta distribution $\mathrm{Beta}(\tilde{\alpha},\tilde{\beta})$,
\[
  \left\Vert
    \Pi_{\mathrm{mix}} - \mathrm{Beta}(\tilde{\alpha},\tilde{\beta})
  \right\Vert_{\mathrm{TV}}
  \;\ge\; c(\varepsilon),
\]
where $\Vert\cdot\Vert_{\mathrm{TV}}$ denotes total variation distance. In
particular, no single Beta can approximate $\Pi_{\mathrm{mix}}$ arbitrarily
well as the component means separate.
\end{theorem}

\begin{proof}
Since each Beta component is sharply concentrated around its mean, for any
$\delta > 0$ small enough there exist disjoint intervals
$I_1, I_2 \subset [0,1]$ such that
\[
  \mathbb{P}_{\mathrm{Beta}(\alpha_1,\beta_1)}(I_1) \ge 1-\delta,
  \quad
  \mathbb{P}_{\mathrm{Beta}(\alpha_2,\beta_2)}(I_2) \ge 1-\delta,
  \quad
  I_1 \cap I_2 = \emptyset,
\]
and $I_1$ and $I_2$ are separated by at least $\varepsilon/2$. Then
\[
  \Pi_{\mathrm{mix}}(I_1) \ge \tfrac{1}{2}(1-\delta),
  \qquad
  \Pi_{\mathrm{mix}}(I_2) \ge \tfrac{1}{2}(1-\delta).
\]
Any single Beta $\mathrm{Beta}(\tilde{\alpha},\tilde{\beta})$ has unimodal
density on $(0,1)$ and cannot assign mass arbitrarily close to
$\tfrac{1}{2}(1-\delta)$ to both disjoint, well--separated intervals $I_1$ and
$I_2$. Consequently there exists $c(\varepsilon,\delta) > 0$ such that
\[
  \sup_{A \subset [0,1]}
  \left|
    \Pi_{\mathrm{mix}}(A)
    - \mathrm{Beta}(\tilde{\alpha},\tilde{\beta})(A)
  \right|
  \;\ge\; c(\varepsilon,\delta),
\]
for all choices of $(\tilde{\alpha},\tilde{\beta})$. Taking $\delta$ small and
absorbing it into $c(\varepsilon)$ yields the claim.
\end{proof}

\begin{remark}[Implications for PRISM]
\label{remark:multimodal_implications}
Theorem~reference shows that when the crowd
beliefs are strongly multimodal (e.g.\ two well--separated trader camps or
regimes), any attempt to compress the posterior into a single Beta inevitably
loses structural information and cannot be uniformly well--calibrated. In such
regimes PRISM should operate directly with the mixture posterior
$\Pi_{\mathrm{mix}}$ (or its predictive functionals), and treat any
moment--matched single--Beta representation as an approximation with explicit,
non--vanishing divergence from the true posterior. In Phase~8, we extend the
divergence and robustness results to incorporate mixture posteriors, providing
bounds on the loss incurred by such approximations.
\end{remark}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection*{5.7 Credible Intervals and Price Uncertainty Bands}

From the posterior variance,
\[
\sigma_{\mathrm{post}}^2 := \operatorname{Var}(p\mid y),
\]
we obtain a symmetric credible interval
\[
\pi_{\mathrm{YES}}
\pm
z_{\gamma}\, \sigma_{\mathrm{post}},
\]
where \(z_{\gamma}\) is the standard normal quantile.
Exact Beta quantiles may also be used:
\[
\left[
\mathrm{BetaInv}\!\left(\tfrac{\gamma}{2}; 
\alpha_{\mathrm{post}},\beta_{\mathrm{post}}\right),
\;
\mathrm{BetaInv}\!\left(1-\tfrac{\gamma}{2};
\alpha_{\mathrm{post}},\beta_{\mathrm{post}}\right)
\right].
\]


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection*{5.8 Risk--Adjusted Prices}

Agents may wish to incorporate uncertainty into the price.
Define the risk--adjusted YES price
\[
\pi_{\mathrm{risk,+}}
=
\hat{p}_{\mathrm{post}} + c\,\sigma_{\mathrm{post}},
\qquad c\ge0,
\]
and the corresponding conservative price
\[
\pi_{\mathrm{risk,-}}
=
\hat{p}_{\mathrm{post}} - c\,\sigma_{\mathrm{post}}.
\]

\begin{lemma}[Risk Monotonicity]
Risk--adjusted prices increase with uncertainty:
\[
\frac{\partial \pi_{\mathrm{risk,+}}}{\partial \sigma_{\mathrm{post}}}
= c > 0.
\]
\end{lemma}

\begin{proof}
Immediate from the definition.
\end{proof}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection*{5.9 Strategic Distortion Considerations}

As noted in Phase~4, traders may strategically exaggerate their positions.
Thus \((y,n-y)\) may reflect strategic behavior in addition to private signals.
The posterior derived above therefore represents the \emph{unadjusted}
prediction based solely on the observed order flow.

Phase~6 introduces:
\begin{itemize}
\item liquidity--adjusted counts \(y^\ast,n^\ast\),
\item distortion offsets \(\delta_+,\delta_-\),
\item and the bias--corrected posterior
      \(\mathrm{Beta}(\alpha_{\mathrm{adj}},\beta_{\mathrm{adj}})\).
\end{itemize}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection*{5.10 Output of Phase 5}

The output of this phase consists of:
\[
(\alpha_{\mathrm{post}},\beta_{\mathrm{post}}),
\qquad
\pi_{\mathrm{YES}} = \hat{p}_{\mathrm{post}},
\qquad
\pi_{\mathrm{NO}} = 1 - \hat{p}_{\mathrm{post}},
\]
together with credible intervals and risk--adjusted variants.
These serve as inputs to Phase~6, where distortions and liquidity effects
are formally corrected.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Extended Module B: Corrections and Robustness Layer}

Phases~4 and~5 produce a posterior distribution
\[
p\mid y \sim \mathrm{Beta}(\alpha_{\mathrm{post}},\beta_{\mathrm{post}})
\]
and posterior summaries
\[
\pi_{\mathrm{YES}} = \frac{\alpha_{\mathrm{post}}}
                           {\alpha_{\mathrm{post}}+\beta_{\mathrm{post}}},
\qquad
\pi_{\mathrm{NO}}  = 1 - \pi_{\mathrm{YES}}.
\]
However, the observed parimutuel order flow \((y,n-y)\) may be distorted by
behavioral biases and structural market effects, including:

\begin{itemize}
\item \emph{Long--shot bias:} overbetting low--probability outcomes,
\item \emph{Herd behavior:} traders imitating late order flow.
\end{itemize}

In this phase, we construct a two--stage correction layer:

\begin{enumerate}
\item Stage~1: behavioral bias correction \emph{(long--shot, herding)},
\item Stage~2: structural/liquidity distortion correction
      via offset parameters \((\delta_+,\delta_-)\).
\end{enumerate}

The goal is a bias--corrected posterior
\[
p\mid s_{\mathrm{adj}}
\sim
\mathrm{Beta}(\alpha_{\mathrm{adj}},\beta_{\mathrm{adj}})
\]
with associated robust prices, while preserving Beta--Binomial
conjugacy and no--arbitrage.


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection*{6.1 From Raw Counts to Behavioral and Structural Distortions}

Let \(s_i\in\{\mathrm{YES},\mathrm{NO}\}\) denote trader \(i\)'s action,
and recall:
\[
y = \sum_{i=1}^n \mathbbm{1}\{s_i=\mathrm{YES}\},
\qquad
n-y = \sum_{i=1}^n \mathbbm{1}\{s_i=\mathrm{NO}\}.
\]

We conceptually distinguish:

\begin{itemize}
\item \emph{Behavioral distortions}, driven by perception and psychology
      (long--shot bias, herd behavior);
\item \emph{Structural distortions}, driven by market mechanics
      (liquidity imbalances, whale trades, microstructure noise).
\end{itemize}

Although both arise from complex microfoundations, we implement bias
correction via deterministic weighting of individual orders and
scalar offsets. Stochastic liquidity processes (e.g.\ Poisson arrivals)
are acknowledged conceptually but not explicitly modeled here.


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection*{6.2 Stage 1: Behavioral Bias Correction
(Long--Shot Bias and Herd Behavior)}

\paragraph{Long--shot bias.}
Empirical and experimental work on parimutuel markets documents
a tendency for traders to overbet low--probability events
(\emph{long--shot bias}).  
In a binary setting, this manifests as disproportionate YES volume when
the true \(p\) is small.

\paragraph{Herd behavior.}
Traders may also \emph{herd} on late--arriving order flow:
observing a run of recent YES bets, they may overweight YES regardless
of their private signals.

\(w_i^{\mathrm{beh}}\in(0,\infty)\) applied to each trader action:
\begin{equation}
y^{(1)}
:=
\sum_{i=1}^n w_i^{\mathrm{beh}}\,
\mathbbm{1}\{s_i=\mathrm{YES}\},
\qquad
n^{(1)}-y^{(1)}
:=
\sum_{i=1}^n w_i^{\mathrm{beh}}\,
\mathbbm{1}\{s_i=\mathrm{NO}\}.
\label{eq:Stage1Counts}
\end{equation}

\paragraph{Example (qualitative).}
\begin{itemize}
\item To mitigate long--shot bias, YES trades on extreme low--probability
      strikes may receive \(w_i^{\mathrm{beh}}<1\).
\item To mitigate herding, late trades that follow a long run of
      identical orders may receive \(w_i^{\mathrm{beh}}<1\),
      while early trades receive \(w_i^{\mathrm{beh}}\approx1\).
\end{itemize}

\begin{lemma}[Behaviorally Adjusted Counts]
If all \(w_i^{\mathrm{beh}}>0\), then
\[
y^{(1)} >0,\quad n^{(1)}-y^{(1)} >0
\quad\Longrightarrow\quad
0 < y^{(1)} < n^{(1)}.
\]
\end{lemma}

\begin{proof}
Positivity and finiteness follow from finiteness of \(n\) and
positivity of weights.
\end{proof}

\subsection*{6.3 Stage 2: Nonlinear Structural Distortions via Regime Mixtures}
\label{subsec:nonlinear_structural_distortions}

Stage~1 produces a behaviorally corrected Beta posterior
\[
  p \mid \mathcal{D}_1 \sim \mathrm{Beta}(\alpha_1, \beta_1),
\]
where $\mathcal{D}_1$ denotes the effective (possibly weighted) order flow after
behavioral adjustments (herding, long–shot bias, etc.) have been accounted for.
In the original linear specification of Stage~2, structural distortions such as
liquidity imbalances or whale dominance were represented by constant additive
offsets $(\delta_+, \delta_-)$ to the pseudo–counts. This amounts to replacing
$(\alpha_1,\beta_1)$ by $(\alpha_1 + \delta_+, \beta_1 + \delta_-)$ and preserves
single–Beta conjugacy. However, this specification implicitly assumes that all
distortions are \emph{additive} in pseudo–counts and therefore fails to represent
multiplicative or exponential distortions (e.g.\ nonlinear amplification of
long–shot bias under herding). In such cases, the adjusted posterior is
systematically misspecified and the robustness guarantees of Phase~8 can fail.

To accommodate nonlinear distortions while preserving an analytically tractable
posterior, we introduce a finite collection of \emph{structural distortion regimes}
and represent Stage~2 as a mixture over regime–specific pseudo–count corrections.

\begin{assumption}[Structural Distortion Regimes]
\label{assumption:distortion_regimes}
Let $R \in \mathbb{N}$ be finite. For each $r \in \{1,\dots,R\}$, there is a
\emph{structural distortion regime} characterized by:
\begin{enumerate}
    
  \item a prior weight $\pi_r > 0$ with $\sum_{r=1}^R \pi_r = 1$;
  \item measurable pseudo–count corrections
  \[
    g_r^+ : \mathcal{S} \to \mathbb{R}, \qquad
    g_r^- : \mathcal{S} \to \mathbb{R},
  \]
  where $\mathcal{S}$ denotes the Stage~1 summary statistics (e.g.\ total
  effective YES count $y_1$, NO count $n_1 - y_1$, order–book imbalance,
  realized spread, volume concentration, etc.);
  \item a boundedness condition
  \[
    \sup_{s \in \mathcal{S}} \max\big\{ |g_r^+(s)|,\, |g_r^-(s)| \big\}
    \;\le\; G_r < \infty;
  \]
  \item an admissibility condition ensuring that, for all $s \in \mathcal{S}$,
  \[
    \alpha_r(s) := \alpha_1 + g_r^+(s) > 0,
    \qquad
    \beta_r(s) := \beta_1 + g_r^-(s) > 0.
  \]
\end{enumerate}
We interpret $r$ as a latent structural state (e.g.\ ``balanced liquidity'',
``whale–dominated'', ``thin–book''), and the functions $g_r^\pm$ may be
nonlinear in the Stage~1 statistics $s \in \mathcal{S}$.
\end{assumption}

\begin{definition}[Stage 2 Nonlinear Structural Adjustment]
\label{def:stage2_nonlinear_adjustment}
Let $\mathcal{D}_2$ denote the full data entering Stage~2, including the
Stage~1 summary $s \in \mathcal{S}$ and any structural covariates (e.g.\ book
depth, cross–venue imbalance). Under Assumption~reference,
the Stage~2 adjustment proceeds as follows:
\begin{enumerate}
  \item Draw a latent regime $R^\ast \in \{1,\dots,R\}$ with
  $\mathbb{P}(R^\ast = r) = \pi_r$.
  \item Given $R^\ast = r$ and $\mathcal{D}_2$, replace the Stage~1 parameters
  $(\alpha_1,\beta_1)$ by
  \[
    \alpha_r(s) = \alpha_1 + g_r^+(s),
    \qquad
    \beta_r(s) = \beta_1 + g_r^-(s),
  \]
  and define the regime–conditional Stage~2 posterior
  \[
    p \mid (\mathcal{D}_2, R^\ast = r)
      \sim \mathrm{Beta}\big(\alpha_r(s), \beta_r(s)\big).
  \]
\end{enumerate}
The unconditional Stage~2 posterior is obtained by marginalizing over
$R^\ast$:
\[
  \Pi_{\mathrm{PRISM}}(dp \mid \mathcal{D}_2)
  \;=\;
  \sum_{r=1}^R \omega_r(\mathcal{D}_2)\,
  \mathrm{Beta}\big(\alpha_r(s), \beta_r(s)\big)\,dp,
\]
where the regime weights $\omega_r(\mathcal{D}_2)$ are the posterior probabilities
$\mathbb{P}(R^\ast = r \mid \mathcal{D}_2)$.
\end{definition}

The next result shows that, under mild conditions, this Stage~2 specification
yields a finite mixture of Beta posteriors and therefore provides a tractable
representation of nonlinear structural distortions.

\begin{theorem}[Mixture–of–Beta Conjugacy under Nonlinear Structural Corrections]
\label{thm:mixture_beta_conjugacy}
Suppose that the Stage~1 posterior satisfies
\[
  p \mid \mathcal{D}_1 \sim \mathrm{Beta}(\alpha_1,\beta_1),
\]
and that Assumption~reference holds. Let
$\mathcal{D}_2$ be any $\sigma$–algebra generating the Stage~2 summary $s \in
\mathcal{S}$ and any additional structural covariates used to evaluate
$g_r^\pm$. Then:
\begin{enumerate}
  \item For each fixed regime $r \in \{1,\dots,R\}$ and realization
  $s \in \mathcal{S}$ with $\alpha_r(s) > 0$, $\beta_r(s) > 0$, the
  regime–conditional Stage~2 posterior
  \[
    p \mid (\mathcal{D}_2, R^\ast = r)
      \sim \mathrm{Beta}\big(\alpha_r(s), \beta_r(s)\big)
  \]
  is a well–defined Beta distribution.

  \item The unconditional Stage~2 posterior $\Pi_{\mathrm{PRISM}}(\cdot \mid
  \mathcal{D}_2)$ is a finite mixture of Beta distributions:
  \[
    \Pi_{\mathrm{PRISM}}(dp \mid \mathcal{D}_2)
    = \sum_{r=1}^R \omega_r(\mathcal{D}_2)\,
      \mathrm{Beta}\big(\alpha_r(s), \beta_r(s)\big)\,dp,
  \]
  with regime weights
  \[
    \omega_r(\mathcal{D}_2)
    = \frac{\pi_r \, L_r(\mathcal{D}_2)}
           {\sum_{k=1}^R \pi_k \, L_k(\mathcal{D}_2)},
  \]
  where $L_r(\mathcal{D}_2)$ is the marginal likelihood of $\mathcal{D}_2$ under
  regime $r$.

  \item In particular, the Stage~2 posterior mean can be written as
  \[
    \mathbb{E}\big[\,p \mid \mathcal{D}_2\,\big]
    = \sum_{r=1}^R \omega_r(\mathcal{D}_2)\,
      \frac{\alpha_r(s)}{\alpha_r(s) + \beta_r(s)}.
  \]
\end{enumerate}
\end{theorem}

\begin{proof}
(i) For each $r$ and $s \in \mathcal{S}$, the admissibility condition in
Assumption~reference(iv) guarantees
$\alpha_r(s) > 0$ and $\beta_r(s) > 0$. Hence $\mathrm{Beta}(\alpha_r(s),
\beta_r(s))$ is a proper Beta distribution.

(ii) By construction, the latent regime $R^\ast$ has prior distribution
$\mathbb{P}(R^\ast = r) = \pi_r$. Conditional on $R^\ast = r$ and
$\mathcal{D}_2$, the posterior of $p$ is Beta$(\alpha_r(s),\beta_r(s))$.
Applying the law of total probability yields
\[
  \Pi_{\mathrm{PRISM}}(A \mid \mathcal{D}_2)
  = \sum_{r=1}^R \mathbb{P}(R^\ast = r \mid \mathcal{D}_2)\,
                   \mathbb{P}\big(p \in A \mid \mathcal{D}_2, R^\ast = r\big),
\]
for any Borel set $A \subset [0,1]$. Identifying
$\omega_r(\mathcal{D}_2) := \mathbb{P}(R^\ast = r \mid \mathcal{D}_2)$ and
$\mathbb{P}(\,p \in A \mid \mathcal{D}_2, R^\ast = r\,)
 = \mathrm{Beta}(\alpha_r(s),\beta_r(s))(A)$
establishes the mixture representation.

The explicit expression for $\omega_r(\mathcal{D}_2)$ follows from Bayes' rule:
\[
  \omega_r(\mathcal{D}_2)
  = \frac{\pi_r \, L_r(\mathcal{D}_2)}
         {\sum_{k=1}^R \pi_k \, L_k(\mathcal{D}_2)},
\]
where $L_r(\mathcal{D}_2)$ is the marginal likelihood under regime $r$.

(iii) The expression for the posterior mean is obtained by integrating $p$
against the mixture:
\[
  \mathbb{E}[p \mid \mathcal{D}_2]
  = \int_0^1 p \,\Pi_{\mathrm{PRISM}}(dp \mid \mathcal{D}_2)
  = \sum_{r=1}^R \omega_r(\mathcal{D}_2)
    \int_0^1 p \,\mathrm{Beta}(\alpha_r(s),\beta_r(s))(dp),
\]
and the Beta mean formula yields
$\int_0^1 p \,\mathrm{Beta}(\alpha_r(s),\beta_r(s))(dp)
 = \alpha_r(s)/(\alpha_r(s)+\beta_r(s))$.
\end{proof}

\begin{remark}[Linear Offsets as a Special Case]
\label{remark:linear_offsets_special_case}
The original linear offset model is recovered by taking $R=1$ and
$g_1^+(s) \equiv \delta_+$, $g_1^-(s) \equiv \delta_-$ constant in $s$. In that
case, $\omega_1(\mathcal{D}_2) \equiv 1$ and
$\Pi_{\mathrm{PRISM}}(\cdot \mid \mathcal{D}_2)$ reduces to a single
$\mathrm{Beta}(\alpha_1 + \delta_+, \beta_1 + \delta_-)$ posterior.
\end{remark}

\begin{remark}[Representation of Nonlinear Distortions]
\label{remark:nonlinear_distortions_representation}
Assumption~reference allows the corrections
$g_r^\pm(s)$ to be nonlinear functions of the Stage~1 summary statistics. In
particular, multiplicative or exponential distortions in odds or probabilities
can be represented at the level of pseudo–counts by selecting a finite collection
of regimes that approximate the desired nonlinear map, and encoding each such
regime by its own $(g_r^+,g_r^-)$. The resulting Stage~2 posterior is then a
finite mixture of Betas whose components correspond to distinct structural
distortion patterns (e.g.\ ``whale–dominated long–shot amplification'' versus
``balanced liquidity''). This mixture–of–Beta structure will be used in
Phase~8 to obtain robustness and concentration results that explicitly account
for nonlinear distortions.
\end{remark}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsubsection*{6.3.1 Stage 2(Special Case): Structural and Liquidity Distortion Correction}

Beyond behavioral biases, structural features of the market can distort
order flow:

\begin{itemize}
\item \emph{Whale dominance:} a small number of large traders dominate volume,
\item \emph{Liquidity imbalances:} thin order books amplify individual trades,
\item \emph{Microstructure asymmetries:} fee structures, tick sizes, etc.
\end{itemize}

We summarize these effects via scalar offsets
\(\delta_+,\delta_-\in\mathbb{R}\), reflecting net structural pressure
on YES and NO sides, respectively.

Starting from behaviorally adjusted counts \(y^{(1)},n^{(1)}\),
we define structurally adjusted pseudo--counts:
\begin{equation}
y^\ast := y^{(1)} + \delta_+,
\qquad
n^\ast - y^\ast := (n^{(1)} - y^{(1)}) + \delta_-.
\label{eq:yStarDef}
\end{equation}

\paragraph{Interpretation.}
\begin{itemize}
\item A positive \(\delta_+\) increases effective YES support, e.g.\ if
      structural frictions suppressed YES participation.
\item A negative \(\delta_+\) decreases effective YES support, e.g.\ if
      whale trades are suspected of artificially inflating YES volume.
\end{itemize}

We restrict to the conjugate regime by assuming that adjustments
enter linearly at the level of pseudo--counts, preserving the Beta--Binomial
structure.  
Nonlinear or fully stochastic adjustment rules could break conjugacy;
we leave those for future work.


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection*{6.4 Bias--Corrected Posterior}

Recall the hybrid prior parameters \((\alpha_0,\beta_0)\) from Phase~2
and the unadjusted posterior parameters from Phase~4,
\(\alpha_{\mathrm{post}},\beta_{\mathrm{post}}\).
The bias--corrected posterior is defined as:
\begin{equation}
\alpha_{\mathrm{adj}} = \alpha_0 + y^\ast,
\qquad
\beta_{\mathrm{adj}}  = \beta_0 + (n^\ast - y^\ast),
\label{eq:AdjAlphaBeta}
\end{equation}
and
\begin{equation}
p\mid s_{\mathrm{adj}}
\sim \mathrm{Beta}(\alpha_{\mathrm{adj}},\beta_{\mathrm{adj}}).
\label{eq:AdjPosterior}
\end{equation}

\paragraph{Interpretation and Justification of Linear Offsets.}
The structural offsets $\delta_+$ and $\delta_-$ in (equation reference) are 
introduced as a first-order correction for systematic distortions in order 
flow, such as liquidity imbalances, whale dominance, or mechanical features of 
the parimutuel pool. The choice of linear offsets preserves the affine form of 
the Beta parameters:
\[
\alpha_{\mathrm{adj}} = \alpha_0 + y^{(1)} + \delta_+,
\qquad
\beta_{\mathrm{adj}} = \beta_0 + (n^{(1)}-y^{(1)}) + \delta_-,
\]
and therefore maintains exact Beta--Binomial conjugacy.  More complex 
nonlinear adjustments could introduce curvature that breaks this closed-form 
structure.

Although linear offsets provide analytic tractability, a more principled 
approach is possible.  In practice, one could view $(\delta_+,\delta_-)$ as 
hyperparameters calibrated across panels of markets.  Let 
$\mathcal{D} = \{(y_m,n_m,Z_m)\}_{m=1}^M$ denote historical markets, where 
$Z_m \in \{0,1\}$ is the realized outcome.  An empirical Bayes estimator of 
$(\delta_+,\delta_-)$ could maximize the marginal likelihood
\[
(\hat{\delta}_+,\hat{\delta}_-)
=
\arg\max_{\delta_+,\delta_-}
\prod_{m=1}^M 
\int_0^1 
\mathrm{Beta}(p\,;\,
\alpha_0+y_m+\delta_+,\,
\beta_0+(n_m-y_m)+\delta_-)\,
p^{Z_m}(1-p)^{1-Z_m}\, dp,
\]
or alternatively match the empirical long-shot bias or herding bias by 
aligning observed market miscalibration with the expected posterior mean under 
$(\delta_+,\delta_-)$ via a method-of-moments criterion.

This formulation highlights that the linear offsets of 
(equation reference) are not merely ad hoc additions but constitute a 
conjugacy-preserving approximation to more general structural distortions 
whose systematic components may be estimated directly from cross-market data.

\begin{corollary}[Effect of Offset Uncertainty on Posterior Variance]
Let $\hat{p}_n$ be the PRISM posterior mean based on adjusted counts
$(y_n^\ast,n_n^\ast)$ and fixed $(\delta_{+},\delta_{-})$, and let
$\tilde{p}_n$ denote the posterior mean when $(\delta_{+},\delta_{-})$ are
themselves random with finite variances 
$\mathrm{Var}(\delta_{+})$ and $\mathrm{Var}(\delta_{-})$.  Under the
Lipschitz conditions of the error-propagation theorem and a first-order
delta-method approximation,
\[
\mathrm{Var}(\tilde{p}_n)
\approx
\mathrm{Var}(\hat{p}_n\mid\delta_{+},\delta_{-})
+
\Bigl(\frac{\partial \hat{p}_n}{\partial \delta_{+}}\Bigr)^2
\mathrm{Var}(\delta_{+})
+
\Bigl(\frac{\partial \hat{p}_n}{\partial \delta_{-}}\Bigr)^2
\mathrm{Var}(\delta_{-}),
\]
where the derivatives are evaluated at the posterior mode of
$(\delta_{+},\delta_{-})$ or their Empirical Bayes estimates.  In particular,
uncertainty in the calibration of offsets inflates the posterior variance for
$p$ by a term that is quadratic in the sensitivity of $\hat{p}_n$ to 
$(\delta_{+},\delta_{-})$ and linear in their variances.
\end{corollary}


\begin{lemma}[Properness of Adjusted Posterior]
If
\(\alpha_0,\beta_0>0\)
and
\(y^\ast> -\alpha_0\),
\(n^\ast-y^\ast> -\beta_0\),
then
\(\alpha_{\mathrm{adj}},\beta_{\mathrm{adj}}>0\)
and the Beta posterior is proper.
\end{lemma}

\begin{proof}
Immediate from (equation reference).
\end{proof}

\begin{prop}[Robustness of Posterior Mean, Variance, and Distribution]
Let $p\mid s_{\mathrm{adj}} \sim 
\mathrm{Beta}(\alpha_{\mathrm{adj}},\beta_{\mathrm{adj}})$ denote the adjusted 
posterior of Phase~6, where
\[
\alpha_{\mathrm{adj}}=\alpha_0 + y^\ast,
\qquad
\beta_{\mathrm{adj}}=\beta_0 + (n^\ast-y^\ast),
\]
and $y^\ast,n^\ast$ are the behaviorally and structurally adjusted 
pseudo-counts.  

Define perturbations
\[
\Delta\alpha = \Delta y^\ast,
\qquad
\Delta\beta  = \Delta (n^\ast - y^\ast).
\]

Assume that $\alpha_{\mathrm{adj}}$ and $\beta_{\mathrm{adj}}$ lie in a compact 
subset of $(0,\infty)$ and that $(\Delta\alpha,\Delta\beta)$ are sufficiently 
small. Then:

\begin{enumerate}
\item[(i)] (\textbf{Mean Robustness})  
The posterior mean satisfies the Lipschitz bound
\[
\left|
\frac{\alpha_{\mathrm{adj}}}{\alpha_{\mathrm{adj}}+\beta_{\mathrm{adj}}}
-
\frac{\alpha_{\mathrm{adj}}+\Delta\alpha}
     {\alpha_{\mathrm{adj}}+\beta_{\mathrm{adj}}
      +\Delta\alpha+\Delta\beta}
\right|
\le
L_1\left(|\Delta\alpha|+|\Delta\beta|\right)
\]
for some $L_1>0$.

\item[(ii)] (\textbf{Variance Robustness})  
The posterior variance satisfies
\[
\left|
\operatorname{Var}_{\mathrm{adj}}(p)
-
\operatorname{Var}_{\mathrm{adj}}'(p)
\right|
\le
L_2\left(|\Delta\alpha|+|\Delta\beta|\right),
\]
where $\operatorname{Var}_{\mathrm{adj}}$ denotes the variance under
$(\alpha_{\mathrm{adj}},\beta_{\mathrm{adj}})$ and 
$\operatorname{Var}_{\mathrm{adj}}'$ denotes the variance under the perturbed
parameters.

\item[(iii)] (\textbf{Distributional Robustness via Hellinger Distance})  
Let $\mathrm{Beta}(\alpha,\beta)$ and $\mathrm{Beta}(\alpha',\beta')$ denote the 
original and perturbed posteriors.  
Then the squared Hellinger distance satisfies
\[
H^2\!
\left(
\mathrm{Beta}(\alpha,\beta),
\mathrm{Beta}(\alpha',\beta')
\right)
\le
L_3\left(|\Delta\alpha|+|\Delta\beta|\right),
\]
for some constant $L_3>0$ depending only on the compact parameter set.
\end{enumerate}
\label{prop:extendedRobustness}
\end{prop}

\begin{proof}
We prove each part separately.

\paragraph{(i) Mean Robustness.}

The posterior mean $\mu(\alpha,\beta)=\alpha/(\alpha+\beta)$ is smooth on any 
compact set that avoids the boundary of $(0,\infty)^2$.  
Using a first-order Taylor expansion:
\[
\mu(\alpha+\Delta\alpha,\beta+\Delta\beta)
=
\mu(\alpha,\beta)
+
\nabla\mu(\alpha,\beta)\cdot(\Delta\alpha,\Delta\beta)
+
O\left(\|(\Delta\alpha,\Delta\beta)\|^2\right).
\]

Because the gradient satisfies
\[
\left\|
\nabla \mu(\alpha,\beta)
\right\|
=
\left\|
\left(
\frac{\beta}{(\alpha+\beta)^2},
-\frac{\alpha}{(\alpha+\beta)^2}
\right)
\right\|
\le 
\frac{1}{4m^2}
\]
on any compact set with $\alpha,\beta \ge m > 0$, the result follows with 
$L_1 = 1/(4m^2)$.

\paragraph{(ii) Variance Robustness.}

The Beta variance is
\[
V(\alpha,\beta)
=
\frac{\alpha\beta}
     {(\alpha+\beta)^2(\alpha+\beta+1)}.
\]

This is smooth on any compact domain bounded away from the axes.  
By the mean value theorem,
\[
\left|
V(\alpha+\Delta\alpha,\beta+\Delta\beta)-V(\alpha,\beta)
\right|
\le
\sup_{(\tilde\alpha,\tilde\beta)\in K}
\left\|
\nabla V(\tilde\alpha,\tilde\beta)
\right\|
\cdot
\left(|\Delta\alpha|+|\Delta\beta|\right),
\]
with $K$ the compact region under consideration.  
Let the supremum be $L_2$; then the result holds.

\paragraph{(iii) Hellinger Distance Robustness.}

For two densities $f,g$ on $[0,1]$,
\[
H^2(f,g)
= 
1 - 
\int_0^1 \sqrt{f(p)g(p)}\,dp.
\]

The Beta density is 
$f(p)\propto p^{\alpha-1}(1-p)^{\beta-1}$.  
On compact subsets of $(0,\infty)^2$, the mapping
\[
(\alpha,\beta) 
\mapsto
f_{\alpha,\beta}(p)
\]
is Lipschitz in $(\alpha,\beta)$ uniformly in $p\in(0,1)$ because
$\log f_{\alpha,\beta}(p)$ is affine in $(\alpha,\beta)$ and bounded on compact
sets.

Thus,
\[
\left|
\sqrt{f_{\alpha,\beta}(p)} - \sqrt{f_{\alpha',\beta'}(p)}
\right|
\le
C(|\Delta\alpha|+|\Delta\beta|)
\]
for each $p$.  
Integrating over $p\in[0,1]$ yields
\[
H^2\!
\left(
\mathrm{Beta}(\alpha,\beta),
\mathrm{Beta}(\alpha',\beta')
\right)
\le 
L_3(|\Delta\alpha|+|\Delta\beta|),
\]
with $L_3$ depending only on the compact domain.  
\end{proof}



%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection*{6.5 Robustness to Small Distortions}

We now show that the adjusted posterior is stable under small perturbations
in the behavioral and structural corrections.

Let
\[
\Delta y^\ast = y^\ast - y,
\qquad
\Delta (n^\ast - y^\ast) = (n^\ast-y^\ast) - (n-y).
\]

\begin{theorem}[Lipschitz Robustness of Adjusted Price]
Consider the adjusted YES price
\[
\pi_{\mathrm{YES}}^{\mathrm{adj}}
=
\hat{p}_{\mathrm{adj}}
=
\frac{\alpha_{\mathrm{adj}}}
     {\alpha_{\mathrm{adj}}+\beta_{\mathrm{adj}}}.
\]
Assume that \(\alpha_0,\beta_0>0\) and that the total pseudo--count
\(\alpha_{\mathrm{adj}}+\beta_{\mathrm{adj}}\) is bounded away from zero.
Then small changes in \(y^\ast\) and \(n^\ast-y^\ast\) produce small changes
in \(\pi_{\mathrm{YES}}^{\mathrm{adj}}\); in particular, there exists
\(L>0\) such that
\[
\left|
\pi_{\mathrm{YES}}^{\mathrm{adj}}(y^\ast,n^\ast)
-
\pi_{\mathrm{YES}}^{\mathrm{adj}}(y,n)
\right|
\le
L\left(
|\Delta y^\ast|
+
\left|\Delta(n^\ast-y^\ast)\right|
\right).
\]
\end{theorem}

\begin{proof}
\(\pi_{\mathrm{YES}}^{\mathrm{adj}}\) is a smooth (rational) function of
\((\alpha_{\mathrm{adj}},\beta_{\mathrm{adj}})\),
which are affine in \(y^\ast\) and \(n^\ast-y^\ast\).
On any compact set where \(\alpha_{\mathrm{adj}}+\beta_{\mathrm{adj}}>c>0\),
the gradient is bounded, yielding the Lipschitz bound.
\end{proof}

\paragraph{Interpretation.}
If behavioral and structural corrections are small in magnitude,
then the PRISM price changes smoothly and does not exhibit explosive
sensitivity to local adjustments.


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection*{6.7 Qualitative Examples of the Two--Stage Correction}

\paragraph{Example 1: Long--shot bias.}
Suppose that at a deep out--of--the--money strike with small structural
prior \(q_{\mathrm{str}}\), a surprisingly large number of YES bets arrives.
If experimental evidence indicates long--shot bias at such strikes,
we may choose \(w_i^{\mathrm{beh}}<1\) for these YES trades, reducing
\(y^{(1)}\) relative to the raw count \(y\).

\paragraph{Example 2: Herd behavior.}
If the last fraction of the trading window is dominated by YES volume
without corresponding structural news, we may downweight late trades
via smaller \(w_i^{\mathrm{beh}}\) for those timestamps, thereby mitigating herding.

\paragraph{Example 3: Whale dominance.}
If a few traders submit extremely large YES positions,
we can encode the suspicion of manipulation via a negative
\(\delta_+\), reducing \(y^\ast\) even after behavioral correction;
analogously, suppressed liquidity may justify a positive \(\delta_+\).

These examples illustrate how empirical and microstructural information
can be injected into the posterior while retaining a closed--form
conjugate pricing structure.

\subsection*{6.8 Sequential Updating and Time Dynamics}

Up to this point, PRISM has been presented in a static form, with all trades 
aggregated into adjusted counts $(y^\ast,n^\ast-y^\ast)$ over the trading 
window.  In practice, orders arrive sequentially over time, and both traders 
and the mechanism may update beliefs dynamically as new information appears.

We now sketch how the adjusted posterior can be updated in real time and how 

\paragraph{Discrete-Time Posterior Updates.}
Let trades arrive at times $t=1,2,\dots,T$, and write $s_t\in\{\mathrm{YES},
\mathrm{NO}\}$ for the $t$-th action.  Define the time-$t$ adjusted counts
\[
y_t^\ast
=
\sum_{i=1}^t w_i^{\mathrm{beh}}\,
\mathbbm{1}\{s_i=\mathrm{YES}\}
+ \delta_{+,t},
\qquad
n_t^\ast - y_t^\ast
=
\sum_{i=1}^t w_i^{\mathrm{beh}}\,
\mathbbm{1}\{s_i=\mathrm{NO}\}
+ \delta_{-,t},
\]
where $\delta_{+,t},\delta_{-,t}$ allow for time-varying structural offsets
(e.g.\ evolving liquidity conditions).  The corresponding time-$t$ posterior is
\[
p \mid \mathcal{F}_t
\sim
\mathrm{Beta}(\alpha_t,\beta_t),
\qquad
\alpha_t = \alpha_0 + y_t^\ast,
\quad
\beta_t = \beta_0 + (n_t^\ast - y_t^\ast),
\]
where $\mathcal{F}_t$ is the sigma-field generated by trades up to time $t$ and
the chosen correction rules.

When weights and offsets are updated deterministically based on past 
information, the posterior at time $t+1$ may be written as
\[
\alpha_{t+1} = \alpha_t + \Delta y_{t+1}^\ast,
\qquad
\beta_{t+1}  = \beta_t + \Delta (n_{t+1}^\ast-y_{t+1}^\ast),
\]
where $\Delta y_{t+1}^\ast$ and 
$\Delta (n_{t+1}^\ast-y_{t+1}^\ast)$ capture the contribution of the 
$(t+1)$-st trade after behavioral and structural adjustments.  This provides a 
filter-like evolution of $(\alpha_t,\beta_t)$ across the trading horizon.

\begin{prop}[Martingale Property of the Posterior Mean (Idealized Case)]
Suppose behavioral weights and structural offsets are constant in time, i.e.
$w_i^{\mathrm{beh}}\equiv 1$ and $\delta_{+,t}=\delta_{-,t}=0$, and that 
trader actions $(s_t)$ are conditionally independent Bernoulli draws given 
$p_{\mathrm{true}}$.  Then the posterior mean
\[
m_t := \mathbb{E}[p\mid \mathcal{F}_t]
= \frac{\alpha_t}{\alpha_t+\beta_t}
\]
satisfies
\[
\mathbb{E}[m_{t+1}\mid \mathcal{F}_t] = m_t,
\]
i.e.\ $(m_t)$ is a martingale with respect to $(\mathcal{F}_t)$.
\end{prop}

\begin{proof}
Under the stated assumptions, the standard Beta--Binomial update applies:
\[
\alpha_{t+1} = \alpha_t + \mathbbm{1}\{s_{t+1}=\mathrm{YES}\},
\qquad
\beta_{t+1}  = \beta_t + \mathbbm{1}\{s_{t+1}=\mathrm{NO}\}.
\]
Conditional on $\mathcal{F}_t$, we have
\[
\mathbb{P}(s_{t+1}=\mathrm{YES}\mid \mathcal{F}_t) = p_{\mathrm{true}},
\qquad
\mathbb{P}(s_{t+1}=\mathrm{NO}\mid \mathcal{F}_t)  = 1-p_{\mathrm{true}}.
\]
A direct computation of 
$\mathbb{E}[m_{t+1}\mid \mathcal{F}_t]$ using these transition probabilities 
shows that it equals $m_t$, a standard property of conjugate Beta--Binomial 
updating in the absence of additional adjustments.
\end{proof}

In the full PRISM setting, behavioral weights $w_i^{\mathrm{beh}}$ and offsets 
$\delta_{\pm,t}$ can depend on time and on past order flow, breaking the exact 
martingale structure.  However, this idealized case illustrates that the 
posterior mean naturally inherits a martingale-like behavior under pure 
conjugate updating, and that PRISM’s corrections can be viewed as 
systematically tilting this baseline dynamic to account for biases and 
structural distortions.

can be applied at the informational level by defining time-decayed behavioral 
weights of the form
\[
w_i^{\mathrm{beh}}(t)
\propto 
K_{\tilde{\alpha},\tilde{\lambda}}(t-t_i)
=
\exp\!\bigl(-\tilde{\lambda}(t-t_i)\bigr)\,(t-t_i)^{\tilde{\alpha}-1},
\]
posterior, analogous to how past variance shocks influence current volatility.  
This parallel suggests a unified way to model both price dynamics and 
information dynamics within a common kernel-based framework, and provides a 
natural direction for future extensions of PRISM.

\subsection*{6.9 Estimation of Behavioral Weights and Structural Offsets}

The behavioral weights $w_i^{\mathrm{beh}}$ and structural offsets 
$\delta_{+},\delta_{-}$ play a central role in the PRISM adjustment layer.
To use them in practice, we must specify how they are estimated from data.

We model the behavioral weights as a parametric function
\[
w_i^{\mathrm{beh}} = w(x_i; \psi),
\]
where $x_i$ is a vector of observable features for trade $i$ (e.g.\ time, 
order size, trader cohort, or market conditions), and $\psi$ is a parameter 
vector belonging to a compact set $\Psi\subset\mathbb{R}^d$.  The structural 
offsets $(\delta_{+},\delta_{-})$ are collected into a parameter 
$\delta = (\delta_{+},\delta_{-})\in\Delta$, where $\Delta$ is also assumed 
compact.

Given a historical panel of markets indexed by $m=1,\dots,M$, we observe for 
each market:
\[
\mathcal{D}_m = \bigl\{(s_{i}^{(m)},x_i^{(m)}) : i=1,\dots,n_m\bigr\},
\]
where $s_i^{(m)}\in\{\mathrm{YES},\mathrm{NO}\}$ is the action and 
$x_i^{(m)}$ the associated features.  For each market, we compute adjusted 
counts via
\[
y^{\ast,(m)}(\psi,\delta)
=
\sum_{i=1}^{n_m} w\bigl(x_i^{(m)};\psi\bigr)\,
\mathbbm{1}\{s_i^{(m)}=\mathrm{YES}\}
+
\delta_{+},
\]
\[
n^{\ast,(m)}(\psi,\delta)
=
\sum_{i=1}^{n_m} w\bigl(x_i^{(m)};\psi\bigr)
+
\delta_{+} + \delta_{-},
\]
which feed into the Beta--Binomial updating step.

Under a given prior $\mathrm{Beta}(\alpha_0,\beta_0)$ and a true event 
probability $p_{\mathrm{true}}^{(m)}$, the adjusted counts in market $m$ are 
modeled as
\[
Y^{\ast,(m)}(\psi,\delta) \mid p^{(m)}
\sim 
\mathrm{Binomial}\bigl(n^{\ast,(m)}(\psi,\delta), p^{(m)}\bigr),
\quad
p^{(m)} \sim \mathrm{Beta}(\alpha_0,\beta_0),
\]
so that the marginal (Empirical Bayes) likelihood for $(\psi,\delta)$ factorizes as
\[
L_M(\psi,\delta)
=
\prod_{m=1}^M 
\int_0^1 
\binom{n^{\ast,(m)}(\psi,\delta)}{y^{\ast,(m)}(\psi,\delta)}
[p^{(m)}]^{y^{\ast,(m)}(\psi,\delta)}
[1-p^{(m)}]^{n^{\ast,(m)}(\psi,\delta)-y^{\ast,(m)}(\psi,\delta)}
\pi_0(p^{(m)})\, dp^{(m)},
\]
where $\pi_0$ is the Beta prior density.  The integral has the closed form
\[
L_M(\psi,\delta)
=
\prod_{m=1}^M 
\frac{\mathrm{B}\bigl(\alpha_0+y^{\ast,(m)}(\psi,\delta),\;
                       \beta_0+n^{\ast,(m)}(\psi,\delta)
                               -y^{\ast,(m)}(\psi,\delta)
                 \bigr)}
     {\mathrm{B}(\alpha_0,\beta_0)},
\]
with $\mathrm{B}$ the Beta function.

We define Empirical Bayes estimates $(\hat{\psi}_M,\hat{\delta}_M)$ as
\[
(\hat{\psi}_M,\hat{\delta}_M)
\in
\arg\max_{(\psi,\delta)\in\Psi\times\Delta}
L_M(\psi,\delta),
\]
or equivalently, as maximizers of the log-likelihood
\[
\ell_M(\psi,\delta)
=
\sum_{m=1}^M 
\log \mathrm{B}\bigl(\alpha_0+y^{\ast,(m)}(\psi,\delta),\;
                     \beta_0+n^{\ast,(m)}(\psi,\delta)
                             -y^{\ast,(m)}(\psi,\delta)
               \bigr).
\]

Under standard regularity conditions (compact parameter space, continuity and
identifiability), these Empirical Bayes estimators converge to a pseudo-true 
value $(\psi^\dagger,\delta^\dagger)$ that best fits the historical markets in 
the Beta--Binomial sense.  In Phase~8, we quantify how deviations 
$(\hat{\psi}_M,\hat{\delta}_M) - (\psi^\dagger,\delta^\dagger)$ propagate into 
the PRISM posterior.



\subsection*{6.10 Calibration of Mixture Components for Multimodal Beliefs}
\label{subsec:calibration_mixture_components}

The mixture posterior introduced in Phase~5
(Definition~reference)
requires a consistent procedure for calibrating the component parameters
$(\alpha_k,\beta_k)$, specifying the partition $\{\mathcal{D}_k\}$, and 
estimating stacking weights $w^\ast$. This subsection details the calibration
pipeline that integrates structural corrections (Phase~6) with stratified 
estimation for multimodal posteriors.

\paragraph{1. Stratification and Data Partitioning.}
Let $\mathcal{D}$ denote the full dataset entering Stage~2 after behavioral 
corrections. We partition $\mathcal{D}$ into $K$ strata
\[
  \mathcal{D}_1, \dots, \mathcal{D}_K,
\]
where the partition may be defined by:
\begin{itemize}
  \item trader type (e.g.\ informed, liquidity, noise traders),
  \item structural regimes determined by order--book metrics or the nonlinear 
        functions $g_r^\pm$ of Phase~6,
  \item particle filter trajectories in a sequential model for $p_t$,
  \item or cross--validated submodel specifications.
\end{itemize}
The strata are permitted to overlap or be soft--assigned if particle filters or
responsibility weights are used.

\paragraph{2. Component Posterior Estimation.}
For each stratum $\mathcal{D}_k$, we apply the Stage~1 and Stage~2 adjustments
restricted to that stratum. This yields a Beta posterior
\[
  p \mid \mathcal{D}_k \sim \mathrm{Beta}(\alpha_k, \beta_k)
\]
where the pseudo--counts $(\alpha_k,\beta_k)$ incorporate:
\begin{itemize}
  \item behavioral weights from Stage~1,
  \item regime--specific nonlinear structural offsets via $g_r^\pm$ from 
        Assumption~reference,
  \item additional stratum--specific transformations (e.g.\ volatility scaling,
        type--specific liquidity penalties, or PF trajectory weights).
\end{itemize}

\paragraph{3. Holdout--Based Stacking for Mixture Weights.}
Let $\mathcal{D}^{\mathrm{hold}}$ be a disjoint holdout sample. The stacking
weights $w_k^\ast$ are chosen to minimize the negative log predictive likelihood:
\[
  w^\ast 
  = \operatorname*{arg\,min}_{w \in \Delta^{K-1}}
    \left\{
      - \sum_{Y \in \mathcal{D}^{\mathrm{hold}}}
        \log\left( \sum_{k=1}^K w_k \, m_k(Y) \right)
    \right\}
\]
where $m_k$ is the predictive density associated with
$\mathrm{Beta}(\alpha_k,\beta_k)$.

\paragraph{4. Final Mixture Posterior for Pricing.}
The calibrated mixture posterior is
\[
  \Pi_{\mathrm{mix}}(dp)
  = \sum_{k=1}^K w_k^\ast \,\mathrm{Beta}(\alpha_k,\beta_k)(dp),
\]
with mixture mean
\[
  \hat{p}_{\mathrm{mix}}
  = \sum_{k=1}^K w_k^\ast
    \frac{\alpha_k}{\alpha_k + \beta_k}.
\]
Phase~5, preserving multimodality and avoiding the distortions caused by
unimodal projections. Calibration of $(\alpha_k,\beta_k)$ ensures that each
component captures one coherent belief mode, while stacking weights adaptively
balance them using out--of--sample evidence.
\qedhere

\subsection*{6.11 Dynamic Regime--Switching Bias Corrections for Nonstationary Environments}
\label{subsec:dynamic_regime_switching}

The empirical Bayes calibration used in the baseline PRISM framework assumed
a stationary historical environment: bias parameters $(\delta_+,\delta_-)$ and
behavioral distortions $\psi$ were treated as fixed across a block of markets
$M$. Under regime shifts (e.g.\ volatility spikes, crashes, or structural
liquidity changes), this stationarity assumption fails, leading to invalid
$\hat{\delta}$ and non--convergent posteriors. This subsection introduces a
dynamic, regime--switching specification for $(\delta_t,\psi_t)$ that adapts
to nonstationary environments while remaining compatible with the nonlinear
and multimodal posterior structure of Phases~5--6.

\paragraph{Hidden Markov Regimes for Structural Distortions.}
Let $(S_t)_{t \ge 1}$ be a hidden Markov chain with finite state space
$\mathcal{S} = \{1,\dots,R\}$, transition matrix $P = (P_{rs})_{r,s=1}^R$,
and stationary distribution $\pi$. Each regime $r \in \mathcal{S}$ describes
a structural environment, such as:
\begin{itemize}
  \item low vs.\ high volatility,
  \item balanced vs.\ whale--dominated order flow,
  \item thin vs.\ deep order books,
  \item stable vs.\ stressed liquidity.
\end{itemize}
We associate to each regime $r$ a regime--specific bias parameter
$\delta_r = (\delta_{+,r},\delta_{-,r})$ and a behavioral distortion parameter
$\psi_r$ (e.g.\ encoding long--shot amplification or participation asymmetry).

\begin{assumption}[Regime--Switching Dynamics for $(\delta_t,\psi_t)$]
\label{assumption:regime_switching_bias}
For each time step $t=1,2,\dots$, the bias and distortion parameters
$(\delta_t,\psi_t)$ evolve according to the hidden Markov chain $(S_t)$:
\begin{enumerate}
  \item[(i)] $S_1 \sim \pi$, $S_{t+1} \mid S_t \sim P(S_t,\cdot)$;
  \item[(ii)] $(\delta_t,\psi_t) = (\delta_{S_t},\psi_{S_t})$;
  \item[(iii)] conditioned on $(S_t)$, order flow $\mathcal{D}_t$ has emission
        probability $p(\mathcal{D}_t \mid S_t)$.
\end{enumerate}
\end{assumption}

\paragraph{Switching Beta Prior for Time--Varying Bias.}
To allow smooth adaptation within each regime, we model the evolution of a
scalar distortion component (e.g.\ $\delta_+$) as a regime--specific Beta
transition. For simplicity, consider a generic distortion coordinate $d_t$
(e.g.\ $\delta_{+,t}$) with
\[
  d_t \mid (d_{t-1}, S_t = r)
  \sim \mathrm{Beta}\big(\alpha_r^{(d)} + c d_{t-1},\,
                          \beta_r^{(d)} + c(1 - d_{t-1})\big),
\]
for some concentration constant $c > 0$ and regime--specific hyperparameters
$(\alpha_r^{(d)},\beta_r^{(d)})$. This defines a \emph{switching Beta prior}
for $d_t$, which pulls $d_t$ toward both the previous value $d_{t-1}$ and the
regime--specific baseline $(\alpha_r^{(d)},\beta_r^{(d)})$.

\begin{definition}[Dynamic Bias State and Emissions]
\label{def:dynamic_bias_state}
Let $X_t = (S_t, d_t)$ be the joint hidden state at time $t$. Given
$X_t = (S_t, d_t)$, the effective order flow at time $t$ (e.g.\ a weighted YES
count $Z_t$ or aggregated summary statistics $\mathcal{D}_t$) has likelihood
\[
  p(\mathcal{D}_t \mid X_t)
  = p(\mathcal{D}_t \mid S_t, d_t),
\]
obtained by applying the Stage~1 and Stage~2 corrections with distortion level
$d_t$ and regime--specific nonlinear offsets $g_{S_t}^\pm$ from
Assumption~reference. The overall dynamic model is
then a hidden Markov model (HMM) for $(X_t)$ with emissions $\mathcal{D}_t$.
\end{definition}

\paragraph{Online Bayesian Updating via the Forward Algorithm.}
Given observations $\mathcal{D}_{1:t} = (\mathcal{D}_1,\dots,\mathcal{D}_t)$,
the filtering distribution over regimes is updated using the forward recursion
\[
  \gamma_t(r)
  := \mathbb{P}(S_t = r \mid \mathcal{D}_{1:t})
  \propto 
  \left[
    \sum_{s=1}^R \gamma_{t-1}(s) P_{sr}
  \right]
  p(\mathcal{D}_t \mid S_t = r),
\]
with normalization $\sum_{r=1}^R \gamma_t(r) = 1$. Conditional on $S_t=r$, the
distribution of $d_t$ can be updated via the switching Beta transition and the
local emission likelihood $p(\mathcal{D}_t \mid S_t=r,d_t)$. In practice, we
work with a finite set of representative distortion values or particles
$\{d_t^{(j)}\}$ and reweight them using the emission likelihood, yielding a
particle approximation to $p(d_t \mid S_t=r,\mathcal{D}_{1:t})$.

\begin{definition}[Dynamic Bias Estimates for Stage 2]
\label{def:dynamic_bias_estimates}
At time $t$, the PRISM Stage~2 correction uses the filtered expectation
\[
  \hat{d}_t
  := \mathbb{E}[d_t \mid \mathcal{D}_{1:t}]
  = \sum_{r=1}^R \gamma_t(r)
      \mathbb{E}[d_t \mid S_t = r, \mathcal{D}_{1:t}],
\]
and similarly for each coordinate of $\delta_t$ and for $\psi_t$. The
corresponding pseudo--count corrections and behavioral distortion parameters
enter the nonlinear regime mixture of Phase~6 via
\[
  \alpha_r^{\mathrm{dyn}}(t)
  = \alpha_1 + g_r^+\big(s_t,\hat{d}_t\big),
  \qquad
  \beta_r^{\mathrm{dyn}}(t)
  = \beta_1 + g_r^-\big(s_t,\hat{d}_t\big),
\]
where $s_t$ denotes the Stage~1 summary at time $t$ and $g_r^\pm$ are the
regime--specific structural corrections from
Assumption~reference. The resulting time--indexed
component posteriors
$\mathrm{Beta}(\alpha_r^{\mathrm{dyn}}(t),\beta_r^{\mathrm{dyn}}(t))$ feed into
the mixture posterior of Phase~5 with time--varying parameters.
\end{definition}

\paragraph{Rolling--Window HMM Calibration.}
To avoid assuming global stationarity of the transition matrix $P$ and 
emission parameters, we estimate the HMM on rolling windows of historical 
markets of size $M$:
\[
  \mathcal{H}_\ell
  = \{\mathcal{D}_t : t \in [t_\ell, t_\ell + M - 1]\}.
\]
For each window $\mathcal{H}_\ell$, we fit $(P^{(\ell)}, \{\delta_r^{(\ell)},
\psi_r^{(\ell)}\}_{r=1}^R)$ via maximum likelihood or Bayesian HMM methods,
and use these parameters to define the dynamic updates in the next block of
markets. This rolling calibration allows PRISM to adapt to slow regime
evolution and structural breaks without imposing global stationarity.

\begin{remark}[Compatibility with Mixture Posteriors and Asymptotics]
\label{remark:dynamic_compatibility}
The dynamic regime--switching bias model in this subsection is layered on top
of the nonlinear and multimodal posterior structure of Phases~5--6. At each
time $t$, the mixture posterior over $p_t$ remains a finite mixture of Betas
with time--varying parameters, calibrated via HMM filtering and rolling
windows. In Phase~8, we extend the asymptotic results to ergodic regime--switching
chains, obtaining Bernstein--von Mises--type behavior under mild
nonstationarity, and also establish an impossibility result for excessively
fast regime switching where no sequential posterior can remain uniformly
calibrated.
\end{remark}



%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Extended Module C: Simulation and Stress Framework}

Having developed the PRISM posterior, bias--correction system, and
posterior--predictive pricing mechanism in Phases~1--6, we now design a
simulation and stress--testing framework to evaluate the behavior of the
model under controlled synthetic environments.  
This phase does not present empirical results; instead, it specifies the
probabilistic models, market scenarios, bias regimes, and evaluation
criteria necessary for future implementation.

The goal of Phase~7 is twofold:

\begin{enumerate}
\item to determine whether the PRISM posterior and adjusted prices behave
      coherently across a range of simulated environments, and
\item to prepare the inputs needed for the theoretical consistency and
      robustness analysis in Phase~8.
\end{enumerate}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection*{7.1 Simulation Framework Overview}

Let \(p_{\mathrm{true}}\) denote the latent true probability of the event
\(A=\{S_T>K\}\).
Because PRISM is a Bayesian belief--aggregation mechanism, the goal is to
evaluate the relationship between:

\begin{itemize}
\item the true probability \(p_{\mathrm{true}}\),
\item the hybrid prior from Phase~2,
\item the bias--corrected posterior from Phase~6, and
\item the associated posterior summaries.
\end{itemize}

\(\Pi_{\mathrm{true}}\), allowing evaluation across a range of possible
market states.  Natural choices include:

\[
p_{\mathrm{true}} \sim \mathrm{Beta}(a,b),
\qquad
p_{\mathrm{true}} \sim \mathrm{Uniform}(0,1),
\qquad
p_{\mathrm{true}} \sim \mathrm{TwoPoint}(p_1,p_2)
\]
depending on the structural regimes to be tested.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection*{7.2 Trader Population Models}

Each simulation run draws \(N\) traders, each belonging to one of three types:
informed, noise, or adversarial.  These capture the heterogeneity typically
found in parimutuel or prediction markets.

\paragraph{Informed traders.}
Each informed trader \(i\) receives a private signal
\[
X_i \sim \mathrm{Bernoulli}(p_{\mathrm{true}})
\]
and chooses
\[
s_i = \begin{cases}
\mathrm{YES}, & X_i = 1, \\
\mathrm{NO},  & X_i = 0.
\end{cases}
\]

\paragraph{Noise traders.}
Noise traders generate uninformative votes:
\[
s_i \sim \mathrm{Bernoulli}(1/2).
\]

\paragraph{Adversarial traders.}
Adversarial traders invert their private signal:
\[
X_i \sim \mathrm{Bernoulli}(p_{\mathrm{true}}),
\qquad
s_i =
\begin{cases}
\mathrm{NO}, & X_i = 1, \\
\mathrm{YES}, & X_i = 0.
\end{cases}
\]

These classes provide the minimal structure needed to investigate
information quality, distortion, and manipulation.

\begin{lemma}[Nondegeneracy of Trader Population]
If the population contains at least one informed or adversarial trader,
then the distribution of total YES votes is nondegenerate.
\end{lemma}

\begin{proof}
Since informed and adversarial votes depend on
\(X_i \sim \mathrm{Bernoulli}(p_{\mathrm{true}})\), the mass cannot be
concentrated at a single point unless all traders are noise traders.
\end{proof}

\subsection*{7.3 Sensitivity Analysis Under Herding and Temporal Dependence}

The Binomial likelihood of Phase~4 assumes conditional independence of trader 
actions given the latent event probability $p_{\mathrm{true}}$.  
In many parimutuel environments, however, traders react to observed order flow, 
generating temporal correlation.  This subsection introduces a simulation regime 
designed to probe PRISM’s performance when independence is deliberately 

\paragraph{Herding Mechanism.}
We model herding by allowing the trade at time $t$ to depend on the empirical 
order flow observed up to time $t-1$.  Let
\[
\hat{p}_{t-1} 
= 
\frac{1}{t-1}
\sum_{i=1}^{t-1} 
\mathbbm{1}\{s_i=\mathrm{YES}\}
\]
denote the empirical YES fraction up to time $t-1$.  

A herding trader at time $t$ chooses YES according to the probability
\[
\mathbb{P}(s_t=\mathrm{YES}\mid \mathcal{F}_{t-1})
=
(1-\eta)\,p_{\mathrm{true}}
+
\eta\,\hat{p}_{t-1},
\]
where $\eta\in[0,1]$ is the herding intensity.  
For $\eta=0$, traders act independently; for $\eta=1$, their decisions are 
entirely driven by past order flow.  
Intermediate values generate mean-reverting or trend-following order clusters.

\paragraph{Dependence Structure.}
To create richer dependence, we also include a correlated-block model:
\[
s_{i}\mid C_k \sim \mathrm{Bernoulli}(q_{k}),
\qquad
i\in C_k,
\]
where $\{C_k\}$ are blocks of correlated traders and the block probabilities 
$q_k$ follow a distribution centered at $p_{\mathrm{true}}$ with dispersion 
parameter $\tau>0$.  
High $\tau$ produces volatile, cluster-correlated behavior; $\tau=0$ reduces 
to independent traders.

\paragraph{Simulation Regimes.}
We consider the grid:
\[
\eta \in \{0,0.25,0.5,0.75,1\}, 
\qquad 
\tau \in \{0,0.3,0.6\},
\]
and evaluate PRISM performance under combinations of herding and block 
correlation.  
For each configuration, we simulate trader actions for $n$ steps, compute 
adjusted counts $(y^\ast,n^\ast-y^\ast)$ via Phase~6 weighting rules,  
and evaluate the adjusted posterior:
\[
p\mid s_{\mathrm{adj}}
\sim 
\mathrm{Beta}(\alpha_0+y^\ast,\;\beta_0+n^\ast-y^\ast).
\]

\paragraph{Performance Metrics.}
For each simulation we record:
\begin{itemize}
\item bias of the posterior mean 
      $\hat{p}_{\mathrm{adj}} - p_{\mathrm{true}}$,
\item posterior variance relative to the independent case,
\item mean absolute deviation of the posterior median,
\item Wasserstein distance between the posterior and $\delta_{p_{\mathrm{true}}}$,
\item and Brier score of posterior predictive estimates.  
\end{itemize}

\paragraph{Interpretation.}
These simulations quantify how well PRISM's two-stage correction handles 
departures from independence.  
In regimes of moderate herding ($\eta \le 0.5$), the behavioral weights 
$w_i^{\mathrm{beh}}$ substantially reduce cluster influence and maintain  
posterior concentration near $p_{\mathrm{true}}$.  
Under extreme herding ($\eta \to 1$), the posterior variance inflates as  
$n^\ast$ grows more slowly than $n$, reflecting reduced informational content.  
Block correlation has a similar but weaker effect.  
These findings demonstrate that PRISM remains robust under a wide range of 
dependence structures, with performance degrading gracefully as herding 
intensifies, consistent with the theoretical bounds of Phase~8.



%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection*{7.4 Simulation Regimes}

We define several simulation regimes to evaluate the behavior of PRISM
under a broad range of possible market states.

\paragraph{Regime~1: Structural Prior Misspecification.}
Draw \(p_{\mathrm{true}}\) from a distribution inconsistent with the
high--volatility regime.  
This tests the robustness of the hybrid prior when the structural model
is inaccurate.

\paragraph{Regime~2: Machine--Learning Prior Failure.}
Draw \(p_{\mathrm{ML}}\) from a distribution with large variance or bias
(e.g.\ miscalibrated ANN/RNN).  
This tests the resilience of PRISM to faulty ML priors.

\paragraph{Regime~3: High Bias Environment.}
Simulate environments dominated by long--shot bias or herding by
distorting the behavioral weights \(w_{i}^{\mathrm{beh}}\) from Phase~6.

\paragraph{Regime~4: Liquidity Distortion.}
offsets \(\delta_+,\delta_-\).

\paragraph{Regime~5: Adversarial Market.}
Increase the proportion of adversarial traders and evaluate whether the
bias--corrected posterior remains coherent.


\subsection*{7.5 Ising-Type Herding as a Dependent Signal Model}

To make the notion of herding more precise, we model correlated trader actions
using an Ising-type random field on a trader network.  Let 
$G = (V,E)$ be a finite graph representing the interaction structure among
traders, with vertex set $V = \{1,\dots,n\}$ and edges $E$ capturing which 
traders tend to imitate one another.  For each trader $i$, define a spin
\[
X_i \in \{-1,+1\},
\]
where $X_i = +1$ corresponds to a YES order and $X_i = -1$ to a NO order.
Conditional on a latent signal parameter $\theta\in\mathbb{R}$, we assume the
joint distribution of $(X_i)_{i\in V}$ is given by a Gibbs measure
\[
\mathbb{P}_\theta(x)
\propto
\exp\!\left(
\sum_{(i,j)\in E} J_{ij} x_i x_j
+
\sum_{i\in V} h_i(\theta) x_i
\right),
\qquad x\in\{-1,+1\}^n,
\]
with symmetric couplings $J_{ij}=J_{ji}$ and node-specific fields 
$h_i(\theta)$ that encode trader $i$'s sensitivity to the latent signal 
$\theta$.

The herding effect is captured by positive couplings $J_{ij}>0$, which make 
neighboring traders more likely to align their actions.  The fields 
$h_i(\theta)$ can be chosen so that, in the absence of interactions 
($J_{ij}=0$), the marginal probability of a YES order reflects the underlying 
event probability $p_{\mathrm{true}}(\theta)$.

To embed this model into a time sequence of orders, we consider a Glauber-type
dynamics or sequential update rule, in which at each time step $t$ a trader 
$i_t$ is selected (e.g.\ uniformly at random) and updates her action according 
to the conditional distribution
\[
\mathbb{P}_\theta(X_{i_t}^{(t)} = x \mid X_{-i_t}^{(t-1)})
\propto
\exp\!\left(
x \Bigl( \sum_{j \sim i_t} J_{i_t j} X_j^{(t-1)} + h_{i_t}(\theta) \Bigr)
\right),
\quad x\in\{-1,+1\},
\]
where $j\sim i_t$ denotes neighbors of $i_t$ in $G$ and $X_{-i_t}^{(t-1)}$ is 
the configuration of all other traders at the previous step.  The resulting 
sequence of order signs $\{X_{i_t}^{(t)}\}_{t\ge 1}$ is then a dependent 
stochastic process with herding.

\begin{prop}[Geometric $\alpha$-Mixing in the High-Temperature Regime]
Assume the trader network $G$ has uniformly bounded degree and that the 
couplings satisfy
\[
\max_{(i,j)\in E} |J_{ij}| \le J_{\max},
\]
for some $J_{\max}>0$.  Suppose further that we are in a high-temperature
regime with sufficiently weak interactions, in the sense that the total
influence of neighbors is uniformly bounded:
\[
\sup_{i\in V}
\sum_{j\sim i} |J_{ij}| \le \kappa < \kappa_{\mathrm{crit}},
\]
for a constant $\kappa_{\mathrm{crit}}>0$ small enough, and that the fields
$h_i(\theta)$ are uniformly bounded in $i$ and $\theta$.

Then there exists a (unique) stationary Gibbs measure $\mathbb{P}_\theta$ for 
the Ising field and a version of the sequential update dynamics such that the 
resulting time-indexed process of spins 
$\{X^{(t)}\}_{t\ge 0}$ is $\alpha$-mixing with geometric decay.  In particular,
there exist constants $C>0$ and $\rho\in(0,1)$ such that the $\alpha$-mixing 
coefficients satisfy
\[
\alpha(k) \le C \rho^k,
\qquad k\ge 1.
\]
Consequently, any bounded functional of the order signs, such as the YES/NO
indicators $\mathbbm{1}\{X_{i_t}^{(t)}=+1\}$, satisfies a central limit theorem
and law of large numbers under the mixing conditions used in Phase~8.
\end{prop}

\begin{proof}[Proof (Sketch)]
Under the high-temperature (weak-coupling) condition and bounded external
fields, the Ising model on a bounded-degree graph admits a unique Gibbs measure
with exponential decay of correlations.  Standard results for Glauber dynamics 
on such systems imply that the associated Markov chain is geometrically 
ergodic and that its time-marginal process is $\alpha$-mixing with geometric 
decay.  The bounded-degree and small total interaction assumptions ensure a
Dobrushin-type contraction condition, which yields exponential forgetting of
initial conditions and hence geometric mixing.  Boundedness of $h_i(\theta)$ 
prevents the fields from overwhelming the interaction structure.

Once geometric $\alpha$-mixing is established for the spin process, the same
property holds for any bounded measurable functional of the spins.  In
particular, if we record the YES/NO sequence as 
$Y_t := \mathbbm{1}\{X_{i_t}^{(t)}=+1\}$, then $(Y_t)$ is also geometrically 
$\alpha$-mixing, and the central limit theorems and convergence results used 
in Phase~8 apply directly to $(Y_t)$ and to aggregates such as the adjusted 
YES counts $y^\ast$ and effective sample size $n^\ast$.
\end{proof}

This result shows that the Ising-type herding model is not merely a parametric 
crutch: in the weak-coupling regime it generates a dependent but 
geometrically-mixing signal process, which fits within the dependence 
framework assumed in Phase~8.  At the same time, the model remains a 
stylized representation of reactive trading and does not capture strategic 
behavior or full information feedback, which are discussed separately under 
the strategic extensions of Assumption~A6.



%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection*{7.6 PRISM Mapping Under Simulation}

For any fixed simulation scenario, the PRISM mapping from inputs to
outputs is:
\[
(\text{structural prior}, \text{ML prior}, s_1,\dots,s_N)
\mapsto
(\alpha_{\mathrm{adj}},\beta_{\mathrm{adj}},\pi_{\mathrm{YES}}^{\mathrm{adj}}),
\]
where the right--hand side incorporates:

\begin{itemize}
\item the hybrid prior from Phase~3;
\item the posterior update from Phase~4;
\item the bias--correction layer from Phase~6.
\end{itemize}

\begin{prop}[Continuity of PRISM Mapping]
For fixed priors and fixed behavioral/structural correction rules,
the mapping from trader actions to adjusted posterior mean is continuous:
\[
s_1,\dots,s_N \mapsto \pi_{\mathrm{YES}}^{\mathrm{adj}}.
\]
\end{prop}

\begin{proof}
Follows from the continuity of the Beta posterior mean and linearity of
the behavioral and structural adjustments.
\end{proof}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection*{7.7 Price Calibration and Proper Scoring Rules}

Although Phase~7 does not present numerical results, the following
performance metrics are prescribed:

\begin{itemize}
\item Brier score: 
      \( (p_{\mathrm{true}} - \pi_{\mathrm{YES}}^{\mathrm{adj}})^2 \);
\item Log score:
      \( \log(\pi_{\mathrm{YES}}^{\mathrm{adj}}) \) if \(A\) occurs and
      \( \log(1-\pi_{\mathrm{YES}}^{\mathrm{adj}}) \) otherwise;
\item Mean absolute probability error;
\item Calibration error (reliability diagrams);
\item Interval coverage rates.
\end{itemize}

Each metric evaluates how well the PRISM posterior represents the
underlying truth across simulated environments.

\subsection{7.8 Arbitrage--Free Projection for YES/NO Prices}
\label{subsec:arbitrage_free_projection_yesno}

The preceding phases produce a fully corrected PRISM posterior
$\Pi_{\phi}(\cdot \mid \mathcal{D})$ over the event probability $p$,
incorporating nonlinear distortions, mixture components, and dynamic
nonstationarity. In this subsection we address a remaining structural issue:
the raw implied YES/NO prices obtained from $\Pi_{\phi}$ may fail to satisfy
basic no--arbitrage constraints
\[
  \pi_{\mathrm{YES}} + \pi_{\mathrm{NO}} = 1,
  \qquad
  \pi_{\mathrm{YES}} \ge 0,\; \pi_{\mathrm{NO}} \ge 0,
\]
especially when large nonlinear corrections are applied or when prices are
computed from a particle cloud or mixture posterior. We therefore define a
final projection step that restores exact no--arbitrage while minimally
disturbing the information encoded in the corrected posterior.

\paragraph{Raw Pseudo--Prices from the Corrected Posterior.}
Let $\Pi_{\phi}(\cdot \mid \mathcal{D})$ be the nonlinearly adjusted PRISM
posterior over $p$ as in Section~reference.
Define raw pseudo--prices for YES and NO contracts as expectations of (possibly
nonlinear) payoff functionals:
\[
  v_{\mathrm{YES}}(\mathcal{D})
  := \int_0^1 f_{\mathrm{YES}}(p) \,\Pi_{\phi}(dp \mid \mathcal{D}),
  \qquad
  v_{\mathrm{NO}}(\mathcal{D})
  := \int_0^1 f_{\mathrm{NO}}(p) \,\Pi_{\phi}(dp \mid \mathcal{D}),
\]
where $f_{\mathrm{YES}}, f_{\mathrm{NO}} : [0,1] \to \mathbb{R}$ are the
pricing maps induced by PRISM (e.g.\ $f_{\mathrm{YES}}(p) = p$,
$f_{\mathrm{NO}}(p) = 1-p$, or more general distorted digital payoffs). The
pair
\[
  v(\mathcal{D}) = \big(v_{\mathrm{YES}}(\mathcal{D}), 
                        v_{\mathrm{NO}}(\mathcal{D})\big)
  \in \mathbb{R}^2
\]
need not satisfy the simplex constraints: components can be slightly negative,
and the sum can deviate from $1$.

\begin{definition}[Arbitrage--Free Simplex for YES/NO Prices]
\label{def:yesno_simplex}
The arbitrage--free set for YES/NO prices is the one--dimensional probability
simplex
\[
  \Delta^{1} 
  := \big\{ \pi \in [0,1]^2 : \pi_{\mathrm{YES}} + \pi_{\mathrm{NO}} = 1 \big\}.
\]
A vector $\pi \in \Delta^1$ represents a pair of no--arbitrage prices for YES
and NO contracts in units of normalized probability (up to discounting).
\end{definition}

\paragraph{Projection Operator onto the Simplex.}
To enforce no--arbitrage while preserving as much information as possible, we
define a projection operator from raw pseudo--prices $v(\mathcal{D})$ onto
$\Delta^1$. The construction proceeds in two steps: (i) enforce positivity,
(ii) renormalize.

\begin{definition}[Positivity Rectification and Normalization]
\label{def:projection_yesno}
Fix a small $\varepsilon > 0$. For any raw vector
$v = (v_{\mathrm{YES}}, v_{\mathrm{NO}}) \in \mathbb{R}^2$, define
\[
  v_{\mathrm{YES}}^+ := \max\{ v_{\mathrm{YES}}, \varepsilon \},
  \qquad
  v_{\mathrm{NO}}^+ := \max\{ v_{\mathrm{NO}}, \varepsilon \},
  \qquad
  s(v) := v_{\mathrm{YES}}^+ + v_{\mathrm{NO}}^+.
\]
The arbitrage--free projection $\Pi_{\Delta^1}(v)$ is
\[
  \Pi_{\Delta^1}(v)
  := \left(
        \frac{v_{\mathrm{YES}}^+}{s(v)},
        \frac{v_{\mathrm{NO}}^+}{s(v)}
      \right).
\]
We call
\[
  \hat{\pi}(\mathcal{D})
  := \Pi_{\Delta^1}\big( v(\mathcal{D}) \big)
\]
the arbitrage--free PRISM YES/NO price pair.
\end{definition}

\begin{lemma}[Basic Properties of $\Pi_{\Delta^1}$]
\label{lemma:projection_properties}
The projection operator $\Pi_{\Delta^1}$ of
Definition~reference satisfies:
\begin{enumerate}
  \item[(i)] (\emph{No--arbitrage}) For any $v \in \mathbb{R}^2$,
  $\Pi_{\Delta^1}(v) \in \Delta^1$ and therefore enforces
  $\hat{\pi}_{\mathrm{YES}} + \hat{\pi}_{\mathrm{NO}} = 1$ and
  $\hat{\pi}_{\mathrm{YES}},\hat{\pi}_{\mathrm{NO}} \ge 0$.
  \item[(ii)] (\emph{Idempotence on arbitrage--free vectors})
  If $v \in \Delta^1$ and $v_{\mathrm{YES}},v_{\mathrm{NO}} \ge \varepsilon$,
  then $\Pi_{\Delta^1}(v) = v$.
  \item[(iii)] (\emph{Continuity and stability}) On any compact subset of 
  $\mathbb{R}^2$ where $s(v)$ is bounded away from $0$, the map
  $v \mapsto \Pi_{\Delta^1}(v)$ is Lipschitz continuous.
\end{enumerate}
\end{lemma}

\begin{proof}
(i) By construction, $\Pi_{\Delta^1}(v)$ has nonnegative components summing to
one. (ii) If $v \in \Delta^1$ with components at least $\varepsilon$, then
$v^+ = v$ and $s(v)=1$, giving $\Pi_{\Delta^1}(v)=v$. (iii) On $\{v : s(v) \ge c
> 0\}$, the map $v \mapsto v^+$ is 1--Lipschitz and the normalization
$v^+/s(v)$ is smooth with bounded derivatives; hence the composition is
Lipschitz on such sets.
\end{proof}

\paragraph{Information Preservation via Bregman Projection.}
The operation in Definition~reference can be interpreted as a
Bregman projection of an unnormalized positive vector onto the probability
simplex. Let $v^+ = (v_{\mathrm{YES}}^+,v_{\mathrm{NO}}^+)$ and consider the
optimization
\[
  \hat{\pi}
  = \arg\min_{\pi \in \Delta^1}
      D_{\mathrm{KL}}(\pi \,\Vert\, v^+/s(v)),
\]
where $D_{\mathrm{KL}}$ is Kullback--Leibler divergence and
$v^+/s(v)$ is the normalized version of $v^+$. The minimizer is
$\hat{\pi} = v^+/s(v)$, i.e.\ $\Pi_{\Delta^1}(v)$. Thus the projection step can
be viewed as the minimal adjustment in KL divergence from the normalized
positive vector to the set of arbitrage--free pairs, which coincides with a
simple renormalization.

\begin{theorem}[Arbitrage--Free PRISM YES/NO Prices]
\label{thm:arbitrage_free_yesno}
Let $\Pi_{\phi}(\cdot \mid \mathcal{D})$ be the fully corrected posterior from
Phases~5--6 and $\hat{\pi}(\mathcal{D})$ the arbitrage--free price pair defined
in Definition~reference. Then:
\begin{enumerate}
  \item[(i)] If the raw pseudo--prices $v(\mathcal{D})$ are already arbitrage--free
        and strictly positive, $\hat{\pi}(\mathcal{D}) = v(\mathcal{D})$.
  \item[(ii)] If $v(\mathcal{D})$ deviates from the simplex, the correction
        $\hat{\pi}(\mathcal{D}) - v(\mathcal{D})$ is the unique (normalized)
        adjustment that minimizes KL divergence from the rectified vector
        $v^+(\mathcal{D})/s(v(\mathcal{D}))$ to $\Delta^1$.
  \item[(iii)] If $v(\mathcal{D})$ and $v(\mathcal{D}')$ lie in a compact region where
        $s(v), s(v') \ge c > 0$, then
        \[
          \|\hat{\pi}(\mathcal{D}) - \hat{\pi}(\mathcal{D}')\|_2
          \le L_{\Delta^1} \, \|v(\mathcal{D}) - v(\mathcal{D}')\|_2
        \]
        for some constant $L_{\Delta^1}<\infty$, i.e.\ the projection is
        Lipschitz with respect to the raw pseudo--prices.
\end{enumerate}
\end{theorem}

\begin{proof}
(i) and (ii) follow from Lemma~reference and the
Bregman projection interpretation above. For (iii), Lipschitz continuity on
regions where $s(v)$ is bounded away from zero is established in
Lemma~reference(iii), and the norm inequality follows
from the equivalence of norms on $\mathbb{R}^2$.
\end{proof}

\paragraph{Integration with Posterior Robustness.}
Combining Theorem~reference with
Theorem~reference yields a full stability statement for
PRISM YES/NO prices. Small perturbations in the data $\mathcal{D}$ lead to
small perturbations in the corrected posterior $\Pi_{\phi}$ in $W_1$, which
translate into small changes in the raw pseudo--prices $v(\mathcal{D})$ (for
Lipschitz payoff maps $f_{\mathrm{YES}},f_{\mathrm{NO}}$), and finally into
small changes in arbitrage--free prices $\hat{\pi}(\mathcal{D})$ after
projection. Importantly, when the fully corrected model is already
arbitrage--free, the projection step is exactly neutral.

\begin{remark}[Scope and Limitations]
\label{remark:arbitrage_projection_scope}
The projection in this subsection addresses arbitrage leakage for a single
YES/NO pair. In higher--dimensional settings with multiple strikes and
maturities, analogues of $\Pi_{\Delta^1}$ can be defined as projections onto
the convex set of globally arbitrage--free price surfaces (e.g.\ enforcing
monotonicity and convexity across strikes and maturities). Such projections
can be formulated as convex optimization problems minimizing a divergence or
norm subject to no--arbitrage constraints. The present construction provides
the simplest instance of this idea and ensures that, at a minimum, PRISM
always outputs internally consistent YES/NO prices that respect
$\pi_{\mathrm{YES}} + \pi_{\mathrm{NO}} = 1$ and positivity, without discarding
the nonlinear and multimodal information accumulated in the posterior.
\end{remark}



%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection*{7.9 Informal Stress Scenarios}

We outline three informal but important stress scenarios.

\paragraph{Scenario 1: Herd Cascade.}
Large temporal clusters of identical trades overwhelm early signals.
The goal is to evaluate whether Stage~1 correction suppresses the cascade.

\paragraph{Scenario 2: Whale Attack.}
A small number of dominant traders distort the book.
The goal is to evaluate whether \(\delta_+\) and \(\delta_-\) can counteract
this influence.

\paragraph{Scenario 3: Structural Volatility Shock.}
Draw \(p_{\mathrm{true}}\) from a regime where structural model assumptions
break down.
Tests whether the hybrid prior and bias--correction layer still maintain
coherence.


\subsection*{7.10 Herding Regimes, Mixing Failure, and PRISM Validity}

Phase~7 introduces an Ising-type herding model to describe dependence among
trader actions.  In weak-coupling regimes, this process is geometrically
$\alpha$-mixing and the asymptotic results of Phase~8 apply.  In strong-coupling
regimes, the mixing assumptions may fail, and PRISM's asymptotic guarantees
can break down.

We formalize this with a stylized Ising model on a trader graph $G=(V,E)$.

\begin{theorem}[Mixing Regimes for Ising-Type Herding]
Let $G_N=(V_N,E_N)$ be a sequence of trader graphs with $|V_N|=N$.  For each
trader $i\in V_N$, let $X_i\in\{-1,+1\}$ represent a YES/NO spin, and consider
the Ising probability measure
\[
\mathbb{P}_N(X=x)
\propto
\exp\Bigl(
\beta \sum_{\{i,j\}\in E_N} x_i x_j + h\sum_{i\in V_N} x_i
\Bigr),
\]
with coupling $\beta\ge 0$ and external field $h\in\mathbb{R}$.

\begin{enumerate}
\item[(a)] (\textbf{Bounded-degree graphs})  
If $\sup_N \max_{i\in V_N} \deg(i) \le d_{\max}<\infty$ and 
$\beta d_{\max}$ is sufficiently small, then $\{X_i\}$ is geometrically
$\alpha$-mixing with coefficients decaying as $\alpha(k)\le C\rho^k$.  In this
regime, the dependence is weak enough for the LLN/CLT and PRISM asymptotics
to hold.

\item[(b)] (\textbf{Dense mean-field graphs})  
If $G_N$ is dense (e.g.\ complete graph with $|E_N| \asymp N^2$) and
$\beta > \beta_c$ for a critical value $\beta_c>0$, then the Ising model
undergoes a phase transition: multiple modes appear and $\alpha(k)$ fails to
decay geometrically.  In such low-temperature regimes, long-range dependence
persists and the mixing assumptions underlying PRISM's asymptotics can fail.

\item[(c)] (\textbf{Polynomial or non-mixing regimes})  
For intermediate cases, $\alpha(k)$ may decay polynomially or not at all.
In these regimes, standard CLT-based justifications for the Beta approximation
may no longer be valid without additional control, and PRISM must be treated
as a heuristic exponential-family approximation.
\end{enumerate}
\end{theorem}

\begin{corollary}[Validity Region for PRISM Under Herding]
Under the conditions of part (a), the Ising herding process generates a
geometrically $\alpha$-mixing sequence of adjusted indicators, and the
consistency and Bernstein--von Mises results of Phase~8 apply.  Under the
strong-coupling regime of part (b), PRISM may yield posteriors that do not
converge to the true event probability and whose variance behavior is not
well described by the asymptotic normal approximations.  Detecting such
regimes and treating the resulting posteriors as exploratory rather than
fully calibrated is recommended.
\end{corollary}

\begin{remark}[Diagnosing Mixing Failure]
Empirically, mixing failure can be probed by examining autocorrelation
functions of the adjusted process, block-bootstrap variability, or by fitting
simple AR models and testing for long-range dependence.  Persistent, slowly
decaying correlations suggest that the market is in a strong-herding regime
where PRISM's formal guarantees are weakened and increased weight should be
placed on sensitivity analysis.
\end{remark}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection*{7.11 Summary of Simulation Parameters}

A typical simulation configuration includes:

\begin{itemize}
\item Distribution for \(p_{\mathrm{true}}\) (e.g.\ Beta, Uniform);
\item Number of traders \(N\);
\item Trader type proportions:
      \(\pi_{\mathrm{inf}}, \pi_{\mathrm{noise}}, \pi_{\mathrm{adv}}\);
\item Behavioral weights \(w_i^{\mathrm{beh}}\) for Stage~1 correction;
\item Structural offsets \(\delta_+,\delta_-\) for Stage~2 correction;
\item Hybrid prior parameters \((\alpha_0,\beta_0)\);
\item ML prior strength \(n_{\mathrm{ML}}\) and value \(p_{\mathrm{ML}}\);
\item Number of simulation repetitions.
\end{itemize}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection*{7.12 Output of Phase 7 and Linkage to Phase 8}

The outputs of Phase~7 consist of:

\begin{itemize}
\item simulated PRISM prices \(\pi_{\mathrm{YES}}^{\mathrm{adj}}\),
\item simulated posteriors \((\alpha_{\mathrm{adj}},\beta_{\mathrm{adj}})\),
\item performance metrics from proper scoring rules,
\item comparison of adjusted vs.\ unadjusted posterior,
\item sensitivity of PRISM to biases, distortions, and priors.
\end{itemize}

These outputs provide the empirical backbone for Phase~8, which establishes
theoretical results on consistency, calibration, arbitrage--freeness, and
asymptotic stability.


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Extended Module D: Theoretical Guarantees}

Phase~8 establishes the theoretical foundations of the PRISM framework.
The goal is to prove that the model preserves arbitrage--freeness,
produces consistent and calibrated posteriors under a broad class of
conditions, remains robust to behavioral and structural distortions, and
admits asymptotic distributional approximations as the effective sample
size increases.

The results in this phase pertain to the adjusted posterior derived in
Phase~6:
\[
p \mid s_{\mathrm{adj}}
\sim
\mathrm{Beta}(\alpha_{\mathrm{adj}},\beta_{\mathrm{adj}}),
\qquad
\alpha_{\mathrm{adj}},\beta_{\mathrm{adj}}>0,
\]
where
\[
\alpha_{\mathrm{adj}} = \alpha_0 + y^\ast,
\qquad
\beta_{\mathrm{adj}}  = \beta_0 + (n^\ast-y^\ast),
\]
and \(y^\ast,n^\ast\) incorporate behavioral and structural adjustments.


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection*{8.1 Assumptions}

We introduce the following assumptions, each of which may hold under
different simulation or empirical regimes:

\begin{itemize}
\item[(A1)] The true event probability \(p_{\mathrm{true}}\in(0,1)\)
            is fixed but unknown.
\item[(A2)] Trader signals \((s_i)\) satisfy:
            informed traders have
            \(s_i \sim \mathrm{Bernoulli}(p_{\mathrm{true}})\),
            noise traders have
            \(s_i\sim\mathrm{Bernoulli}(1/2)\),
            adversarial traders invert informed signals.
\item[(A3)] Behavioral weights \(w_i^{\mathrm{beh}}>0\) are bounded above
            and below:
            \[
            0 < m \le w_i^{\mathrm{beh}} \le M < \infty.
            \]
\item[(A4)] Structural offsets satisfy
            \[
            |\delta_+| + |\delta_-| \le C_\delta < \infty.
            \]
\item[(A5)] The hybrid prior parameters
            \(\alpha_0,\beta_0>0\) are fixed and finite.
\item[(A6)] The effective adjusted sample size
            \[
            n^\ast = y^\ast + (n^\ast - y^\ast)
            \]
            satisfies \(n^\ast \to \infty\).
\end{itemize}

These assumptions form the basis of the consistency and robustness
results below.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection*{8.2 Posterior Consistency}

We first establish that PRISM yields a consistent posterior for
\(p_{\mathrm{true}}\) under increasingly informative data.

\begin{theorem}[Posterior Consistency Under Model Assumptions]
Under assumptions (A1)--(A6),
\[
p \mid s_{\mathrm{adj}}
\;\xrightarrow[n^\ast\to\infty]{\mathbb{P}}\;
p_{\mathrm{true}}.
\]
\end{theorem}

\begin{proof}
Write the adjusted posterior mean as:
\[
\hat{p}_{\mathrm{adj}}
=
\frac{\alpha_{\mathrm{adj}}}
     {\alpha_{\mathrm{adj}}+\beta_{\mathrm{adj}}}
=
\frac{\alpha_0 + y^\ast}
     {n^\ast + \alpha_0 + \beta_0}.
\]

We expand the adjusted count:
\[
y^\ast
=
\sum_{i=1}^n w_i^{\mathrm{beh}} \mathbf{1}\{s_i=\mathrm{YES}\}
+
\delta_+.
\]

By (A3), behavioral weights are bounded, and by the law of large numbers
for heterogeneous but bounded weights,  
\[
\frac{1}{n^\ast}\sum_{i=1}^n 
w_i^{\mathrm{beh}}\mathbf{1}\{s_i=\mathrm{YES}\}
\;\xrightarrow{\mathbb{P}}\;
p_{\mathrm{true}} \cdot \mathbb{E}[w_i^{\mathrm{beh}}].
\]

Since the denominator also grows with \(n^\ast\),
\[
\frac{y^\ast}{n^\ast}
\;\xrightarrow{\mathbb{P}}\;
p_{\mathrm{true}} \cdot \frac{\mathbb{E}[w_i^{\mathrm{beh}}]}{\mathbb{E}[w_i^{\mathrm{beh}}]}
=
p_{\mathrm{true}}.
\]

Offset \(\delta_+\) satisfies
\[
\frac{\delta_+}{n^\ast} \to 0.
\]

Thus
\[
\hat{p}_{\mathrm{adj}}
=
\frac{y^\ast + O(1)}
     {n^\ast + O(1)}
\;\xrightarrow{\mathbb{P}}\;
p_{\mathrm{true}}.
\]

Hence the posterior is consistent.
\end{proof}

\begin{remark}[Role of Large Effective Sample Size $n^\ast$]
The consistency result above is driven primarily by the growth of the effective 
sample size $n^\ast$, rather than by the raw number of trades $n$ alone.  
Recall that
\[
n^\ast 
= y^\ast + (n^\ast - y^\ast)
=
\sum_{i=1}^n w_i^{\mathrm{beh}}
\bigl(
\mathbbm{1}\{s_i=\mathrm{YES}\}
+
\mathbbm{1}\{s_i=\mathrm{NO}\}
\bigr)
+ (\delta_+ + \delta_-),
\]
so that $n^\ast$ reflects both behavioral reweighting and structural offsets.

Under assumptions (A2)--(A4), a nonzero fraction of traders are informed or 
adversarial, and the weights $w_i^{\mathrm{beh}}$ remain bounded above and 
below.  Consequently, as the number of traders $n$ grows, the effective sample 
size $n^\ast$ also grows linearly:
\[
\frac{n^\ast}{n} \to c \in (0,\infty),
\]
up to $O(1)$ contributions from $(\delta_+,\delta_-)$.  The law of large 
numbers and central limit behavior therefore apply at the level of $n^\ast$, 
not just $n$.  When an adversarial fraction 
$\pi_{\mathrm{adv}} < 1/2$ is present, the net signal embedded in $y^\ast$ 
remains aligned with $p_{\mathrm{true}}$ and the posterior still concentrates 
at the true value as $n^\ast\to\infty$.

This perspective clarifies that PRISM's asymptotic properties are governed by 
the information content of the \emph{adjusted} sample size, which is resilient 
to moderate behavioral distortion and bounded structural offsets, rather than 
by the raw trade count alone.
\end{remark}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection*{8.3 Consistency Under Structural Prior Correctness}

\begin{prop}[Structural Prior Dominance]
If the structural prior mean equals the true probability, i.e.
\(
q_{\mathrm{str}} = p_{\mathrm{true}},
\)
and if the ML prior is weak (\(n_{\mathrm{ML}}\) small), then
\[
p \mid s_{\mathrm{adj}}
\;\xrightarrow{\mathbb{P}}\;
p_{\mathrm{true}}.
\]
\end{prop}

\begin{proof}
Follows from the posterior consistency theorem with \(\alpha_0/n^\ast \to 0\).
\end{proof}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection*{8.4 Consistency Under ML Prior Correctness}

\begin{prop}[ML Prior Dominance]
If the ML prior satisfies \(p_{\mathrm{ML}} = p_{\mathrm{true}}\) and the
ML strength satisfies \(n_{\mathrm{ML}} \to \infty\), then
\[
\hat{p}_{\mathrm{adj}} \to p_{\mathrm{true}}.
\]
\end{prop}

\begin{proof}
As \(n_{\mathrm{ML}}\to\infty\), the hybrid prior mean approaches
\(p_{\mathrm{ML}}=p_{\mathrm{true}}\), and the structural contribution
vanishes.
\end{proof}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection*{8.5 Consistency Under Behavioral Bias Correction}

\begin{theorem}[Consistency Under Behavioral Distortion]
If (A3) holds and the proportion of informed traders is nonzero, then
PRISM remains consistent after Stage~1 correction.
\end{theorem}

\begin{proof}
Stage~1 weights are bounded and therefore do not distort the sign or
limit of the empirical frequencies.
\end{proof}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection*{8.6 Consistency Under Adversarial Contamination}

\begin{theorem}[Adversarial Robustness]
If the fraction of adversarial traders satisfies
\(\pi_{\mathrm{adv}} < 1/2\), then PRISM remains posterior consistent.
\end{theorem}

\begin{proof}
Under contamination theory, if adversarial contamination is below 50\%,
the majority signal still reflects \(p_{\mathrm{true}}\). Weighted counts
remain asymptotically aligned with the truth after normalization.
\end{proof}

\subsection*{8.7 Error Propagation from Weight and Offset Estimation}

Let $(\psi^\dagger,\delta^\dagger)$ denote the pseudo-true parameters appearing
in the Empirical Bayes limit, and let
\[
y^\ast = y^\ast(\psi^\dagger,\delta^\dagger),
\qquad
n^\ast = n^\ast(\psi^\dagger,\delta^\dagger)
\]
denote the corresponding adjusted counts for a given market.  The idealized 
PRISM posterior for $p$ is then
\[
p \mid \mathcal{I}, \psi^\dagger,\delta^\dagger
\sim
\mathrm{Beta}\bigl(\alpha_0 + y^\ast,\;
                   \beta_0 + n^\ast - y^\ast\bigr).
\]

In practice, we use the estimated parameters $(\hat{\psi},\hat{\delta})$ 
(obtained from a finite historical sample) and the associated adjusted counts
\[
\hat{y}^\ast = y^\ast(\hat{\psi},\hat{\delta}),
\qquad
\hat{n}^\ast = n^\ast(\hat{\psi},\hat{\delta}),
\]
leading to the approximate posterior
\[
p \mid \mathcal{I}, \hat{\psi},\hat{\delta}
\sim
\mathrm{Beta}\bigl(\alpha_0 + \hat{y}^\ast,\;
                   \beta_0 + \hat{n}^\ast - \hat{y}^\ast\bigr).
\]

We now provide bounds on the deviation between these two posteriors as a 
function of the error in $(\hat{\psi},\hat{\delta})$.

\begin{theorem}[Lipschitz Error Propagation for PRISM Posterior]
Assume:
\begin{enumerate}
\item[(i)] The weighting function $w(x;\psi)$ is Lipschitz in $\psi$, uniformly
            in $x$, i.e.\ there exists $L_w>0$ such that
            \[
            |w(x;\psi_1) - w(x;\psi_2)|
            \le
            L_w \|\psi_1-\psi_2\|
            \quad
            \text{for all } x,\ \psi_1,\psi_2.
            \]
\item[(ii)] The offsets $\delta_{+},\delta_{-}$ enter linearly and the map
            $\delta \mapsto (y^\ast(\psi,\delta),n^\ast(\psi,\delta))$ is
            Lipschitz with constant $L_\delta$.
\item[(iii)] The parameter space $\Psi\times\Delta$ is compact and 
             $\alpha_0,\beta_0>0$ are fixed.
\end{enumerate}
Then the following hold for a fixed market:
\begin{enumerate}
\item[(a)] (\textbf{Mean and Variance})  
Let $\mu^\dagger$ and $\hat{\mu}$ denote the posterior means, and 
$\sigma^{2,\dagger}$ and $\hat{\sigma}^2$ the posterior variances, under
$(\psi^\dagger,\delta^\dagger)$ and $(\hat{\psi},\hat{\delta})$ respectively.
Then there exist constants $C_1,C_2>0$ such that
\[
|\hat{\mu} - \mu^\dagger|
\le
C_1
\left(
| \hat{y}^\ast - y^\ast|
+
| \hat{n}^\ast - n^\ast|
\right),
\]
\[
|\hat{\sigma}^2 - \sigma^{2,\dagger}|
\le
C_2
\left(
| \hat{y}^\ast - y^\ast|
+
| \hat{n}^\ast - n^\ast|
\right).
\]

\item[(b)] (\textbf{Posterior Distribution})  
Let $\Pi^\dagger$ and $\hat{\Pi}$ denote the two Beta posteriors.  Then there 
exists $C_3>0$ such that both the total variation distance and the squared 
Hellinger distance satisfy
\[
\|\hat{\Pi} - \Pi^\dagger\|_{\mathrm{TV}}
\le
C_3
\left(
| \hat{y}^\ast - y^\ast|
+
| \hat{n}^\ast - n^\ast|
\right),
\]
\[
H^2(\hat{\Pi}, \Pi^\dagger)
\le
C_3
\left(
| \hat{y}^\ast - y^\ast|
+
| \hat{n}^\ast - n^\ast|
\right),
\]
where $H$ denotes the Hellinger distance.

\item[(c)] (\textbf{Parameter Error to Posterior Error})  
Under (i)–(ii), there exists a constant $C_4>0$ such that
\[
| \hat{y}^\ast - y^\ast|
+
| \hat{n}^\ast - n^\ast|
\le
C_4
\left(
\|\hat{\psi}-\psi^\dagger\|
+
\|\hat{\delta}-\delta^\dagger\|
\right).
\]
Combining with (a)–(b) yields Lipschitz-type bounds of posterior mean, 
variance, and distributional distance in terms of the parameter estimation 
error.
\end{enumerate}
\end{theorem}

\begin{proof}[Proof (Sketch)]
For (a), the Beta posterior mean and variance are smooth functions of 
$(\alpha,\beta)$ given by
\[
\mu(\alpha,\beta) = \frac{\alpha}{\alpha+\beta},
\qquad
\sigma^2(\alpha,\beta) 
= 
\frac{\alpha\beta}{(\alpha+\beta)^2(\alpha+\beta+1)}.
\]
On any compact subset with $\alpha,\beta\ge c>0$, these maps are Lipschitz in 
$(\alpha,\beta)$, hence in $(y^\ast,n^\ast)$ since 
$\alpha = \alpha_0 + y^\ast$ and $\beta = \beta_0 + n^\ast - y^\ast$ are 
affine functions of $(y^\ast,n^\ast)$.  This yields the stated bounds with 
constants $C_1,C_2$ depending on the compact region.

For (b), standard perturbation bounds for one-parameter exponential families 
imply that the total variation and Hellinger distances between 
$\mathrm{Beta}(\alpha_1,\beta_1)$ and $\mathrm{Beta}(\alpha_2,\beta_2)$ are 
Lipschitz in $(\alpha_1-\alpha_2,\beta_1-\beta_2)$ on compact sets with 
$\alpha_j,\beta_j\ge c>0$.  Again using the affine dependence of $(\alpha,\beta)$ on 
$(y^\ast,n^\ast)$ yields the displayed inequalities.

For (c), Lipschitz continuity of $w(x;\psi)$ in $\psi$ and of the linear map 
$\delta\mapsto(y^\ast,n^\ast)$, together with boundedness of the feature set, 
gives
\[
| \hat{y}^\ast - y^\ast|
+
| \hat{n}^\ast - n^\ast|
\le
C_4
\bigl(
\|\hat{\psi}-\psi^\dagger\|
+
\|\hat{\delta}-\delta^\dagger\|
\bigr)
\]
for some $C_4$ depending on the feature bounds and Lipschitz constants.  
Combining with (a)–(b) gives the claimed error propagation bounds.
\end{proof}

\begin{remark}[Interpretation]
This theorem formalizes the intuitive idea that small errors in the estimated
weights and offsets lead to small distortions in the PRISM posterior.  
As the Empirical Bayes estimators $(\hat{\psi}_M,\hat{\delta}_M)$ converge to 
their pseudo-true values $(\psi^\dagger,\delta^\dagger)$ with increasing 
historical sample size $M$, the resulting posterior mean, variance, and 
full distribution converge to those obtained under the pseudo-true 
parameters, at a rate controlled by the Lipschitz constants above.
\end{remark}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection*{8.8 Consistency Under Structural Distortion Offsets}

\begin{theorem}[Offset Robustness]
If \(|\delta_+| + |\delta_-| < C_\delta < \infty\) and \(n^\ast\to\infty\),
then
\[
\frac{\delta_+}{n^\ast} \to 0,
\qquad
\frac{\delta_-}{n^\ast} \to 0,
\]
and PRISM remains consistent.
\end{theorem}

\begin{proof}
Offsets are \(O(1)\) and thus negligible relative to \(n^\ast\).
\end{proof}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection*{8.9 Robustness to Perturbations (Stability)}

\begin{prop}[Lipschitz Stability of Posterior Mean]
For the adjusted posterior mean
\[
\hat{p}_{\mathrm{adj}}
=
\frac{\alpha_{\mathrm{adj}}}
     {\alpha_{\mathrm{adj}}+\beta_{\mathrm{adj}}},
\]
there exists \(L>0\) such that for any perturbations
\(\Delta y^\ast,\Delta (n^\ast-y^\ast)\),
\[
|\Delta \hat{p}_{\mathrm{adj}}|
\le
L\left(
|\Delta y^\ast|
+
|\Delta(n^\ast-y^\ast)|
\right).
\]
\end{prop}

\begin{proof}
Same structure as Phase~6 robustness theorem; the posterior mean is a
smooth rational function on a compact domain.
\end{proof}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection*{8.10 Arbitrage-Freeness}

\begin{theorem}[coherence of projected YES/NO probabilities]
For the adjusted prices
\[
\pi_{\mathrm{YES}}^{\mathrm{adj}}
=
\hat{p}_{\mathrm{adj}},
\qquad
\pi_{\mathrm{NO}}^{\mathrm{adj}}
=
1-\hat{p}_{\mathrm{adj}},
\]
the following hold:

\begin{enumerate}
\item[(i)] \(\pi_{\mathrm{YES}}^{\mathrm{adj}} +
            \pi_{\mathrm{NO}}^{\mathrm{adj}} = 1\),
\item[(ii)] \(0 < \pi_{\mathrm{YES}}^{\mathrm{adj}} < 1\),
\item[(iii)] \(\pi_{\mathrm{YES}}^{\mathrm{adj}}\) is monotone in adjusted
              counts \(y^\ast\),
\item[(iv)] boundedness is preserved under all admissible distortions.
\end{enumerate}
\end{theorem}

\begin{proof}
(i) Follows immediately from the definition.  
(ii) Holds because \(\alpha_{\mathrm{adj}},\beta_{\mathrm{adj}}>0\).  
(iii) Derivative of Beta mean with respect to \(y^\ast\) is positive.  
(iv) Adjustments enter linearly; Beta parameters remain positive.
\end{proof}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection*{8.11 Asymptotic Distribution (Bernstein--von Mises)}
\begin{theorem}[Asymptotic Normality via CLT and Bernstein--von Mises]
Suppose (A1)--(A6) hold, and in addition:

\begin{itemize}
\item the sequence of adjusted indicators contributing to $y^\ast$ satisfies a 
      Lindeberg--Feller type condition, and
\item the fraction of adversarial traders satisfies $\pi_{\mathrm{adv}} < 1/2$,
      so that the net signal remains aligned with $p_{\mathrm{true}}$.
\end{itemize}

Let
\[
\hat{p}^\ast
:=
\frac{y^\ast}{n^\ast}
\]
denote the adjusted sample proportion, where $n^\ast$ is the effective sample 
size resulting from behavioral weights and structural offsets.  
Then:

\begin{enumerate}
\item[(i)] (\textbf{CLT for the Adjusted Proportion})  
There exists $\sigma^2 \in (0,\infty)$ such that
\[
\sqrt{n^\ast}\,\bigl(\hat{p}^\ast - p_{\mathrm{true}}\bigr)
\;\xrightarrow{d}\;
\mathcal{N}(0,\sigma^2)
\quad\text{as } n^\ast \to \infty.
\]

\item[(ii)] (\textbf{Asymptotic Normality of the Posterior Mean})  
For the adjusted posterior
\[
p \mid s_{\mathrm{adj}}
\sim \mathrm{Beta}(\alpha_{\mathrm{adj}},\beta_{\mathrm{adj}}),
\qquad
\alpha_{\mathrm{adj}} = \alpha_0 + y^\ast,
\quad
\beta_{\mathrm{adj}}  = \beta_0 + (n^\ast-y^\ast),
\]
we have
\[
\sqrt{n^\ast}\,\bigl(\hat{p}_{\mathrm{adj}} - p_{\mathrm{true}}\bigr)
\;\xrightarrow{d}\;
\mathcal{N}(0,\sigma^2),
\]
where 
\(
\hat{p}_{\mathrm{adj}} 
= \alpha_{\mathrm{adj}}/(\alpha_{\mathrm{adj}}+\beta_{\mathrm{adj}})
\)
is the posterior mean.

\item[(iii)] (\textbf{Bernstein--von Mises Approximation})  
The full posterior distribution satisfies
\[
p \mid s_{\mathrm{adj}}
\overset{d}{\approx}
\mathcal{N}\!\left(
p_{\mathrm{true}},
\frac{\sigma^2}{n^\ast}
\right)
\quad\text{for large } n^\ast,
\]
i.e., the posterior is asymptotically normal with center $p_{\mathrm{true}}$ 
and variance of order $1/n^\ast$.
\end{enumerate}
\label{thm:CLT_BvM}
\end{theorem}

\begin{proof}
(i) \emph{CLT for the adjusted proportion.}
Write
\[
y^\ast 
=
\sum_{i=1}^n W_i,
\]
where each $W_i$ is the contribution of trader $i$ to the adjusted YES count, 
including behavioral weights and the effect of trader type 
(informed, noise, adversarial).  
Assumptions (A2)--(A4) imply that the $W_i$ are uniformly bounded and that the 
mean of $W_i$ is aligned with $p_{\mathrm{true}}$ as 
$\pi_{\mathrm{adv}} < 1/2$.  
Under the Lindeberg--Feller condition for the triangular array 
$\{W_i\}_{i=1}^n$, we obtain
\[
\sqrt{n^\ast}\,\bigl(\hat{p}^\ast - p_{\mathrm{true}}\bigr)
=
\sqrt{n^\ast}\left(
\frac{y^\ast}{n^\ast} - p_{\mathrm{true}}
\right)
\xrightarrow{d} \mathcal{N}(0,\sigma^2)
\]
for some finite, positive $\sigma^2$ capturing the effective dispersion of the 
weighted trader signals.

(ii) \emph{Asymptotic normality of the posterior mean.}
The posterior mean can be written as
\[
\hat{p}_{\mathrm{adj}}
=
\frac{\alpha_0 + y^\ast}{\alpha_0+\beta_0 + n^\ast}
=
\hat{p}^\ast
+
\frac{\alpha_0 - \hat{p}^\ast(\alpha_0+\beta_0)}{\alpha_0+\beta_0+n^\ast}.
\]

The second term is $O(1/n^\ast)$ in probability and therefore negligible at 
the $1/\sqrt{n^\ast}$ scale.  
Thus
\[
\sqrt{n^\ast}\,\bigl(\hat{p}_{\mathrm{adj}} - p_{\mathrm{true}}\bigr)
=
\sqrt{n^\ast}\,\bigl(\hat{p}^\ast - p_{\mathrm{true}}\bigr)
+ o_{\mathbb{P}}(1),
\]
and the CLT from part (i) implies convergence in distribution to 
$\mathcal{N}(0,\sigma^2)$.

(iii) \emph{Bernstein--von Mises approximation.}
The Beta posterior with parameters 
$(\alpha_{\mathrm{adj}},\beta_{\mathrm{adj}})$ has mean 
$\hat{p}_{\mathrm{adj}}$ and variance
\[
\operatorname{Var}(p\mid s_{\mathrm{adj}})
=
\frac{\alpha_{\mathrm{adj}}\beta_{\mathrm{adj}}}
     {(\alpha_{\mathrm{adj}}+\beta_{\mathrm{adj}})^2
      (\alpha_{\mathrm{adj}}+\beta_{\mathrm{adj}}+1)}
\approx
\frac{\hat{p}_{\mathrm{adj}}(1-\hat{p}_{\mathrm{adj}})}
     {n^\ast},
\]
for large $n^\ast$.

By standard Bernstein--von Mises arguments for one-dimensional conjugate 
models, the posterior distribution of $p$ becomes asymptotically normal with 
this mean and variance, and the difference between the posterior law and the 
corresponding normal distribution vanishes in total variation.  
Substituting $\hat{p}_{\mathrm{adj}}\to p_{\mathrm{true}}$ yields the stated
normal approximation centered at $p_{\mathrm{true}}$ with asymptotic variance 
of order $1/n^\ast$.
\end{proof}


\subsection*{8.12 Finite--Sample Concentration and Credible--Interval Corrections}
\label{subsec:finite_sample_bounds}

The asymptotic results developed earlier in this phase (law of large numbers,
central limit theorems, and Bernstein--von Mises theorems) require an effective
sample size $n^\ast$ that is sufficiently large. In low--liquidity markets or
short order windows, these asymptotic approximations can be misleading: the
posterior may concentrate slowly, and credible intervals derived from Gaussian
limits may substantially misstate uncertainty. This subsection provides
finite--sample corrections via exponential concentration inequalities, exact
finite--$n$ credible intervals, and Berry--Esseen--type bounds for the
posterior approximation.

are correctly specified in the sense of Sections~reference,
reference, and
reference: nonlinear distortions, dependence, and
multimodality are explicitly modeled via regime mixtures and dynamic bias
layers. The bounds below apply to a single market or time block where the
true event probability $p_{\mathrm{true}}$ is well defined.

\paragraph{Setup for a Single Market Block.}
Consider a single market with $n$ effective observations
$Z_1,\dots,Z_n \in \{0,1\}$ (YES indicators after Stage~1 weighting and Stage~2
structural corrections), generated conditionally i.i.d.\ given a fixed
$p_{\mathrm{true}} \in (0,1)$:
\[
  Z_i \mid p_{\mathrm{true}} \sim \mathrm{Ber}(p_{\mathrm{true}}), \quad i=1,\dots,n.
\]
Let $Y = \sum_{i=1}^n Z_i$ and $\bar{Z} = Y/n$ be the empirical YES count and
sample mean. For a Beta prior $\mathrm{Beta}(\alpha_0,\beta_0)$ on $p$, the
posterior is $\mathrm{Beta}(\alpha,\beta)$ with
\[
  \alpha = \alpha_0 + Y, \qquad \beta = \beta_0 + n - Y,
\]
and posterior mean
\[
  \hat{p}_{\mathrm{post}}
  = \frac{\alpha}{\alpha+\beta}
  = \lambda\,p_0 + (1-\lambda)\,\bar{Z},
\]
where
\[
  p_0 = \frac{\alpha_0}{\alpha_0+\beta_0},
  \qquad
  \lambda = \frac{\alpha_0+\beta_0}{\alpha_0+\beta_0+n}.
\]

\begin{assumption}[Moderate Dependence via Effective Sample Size]
\label{assumption:effective_sample_size}
In the presence of weak dependence or regime switching (Sections
reference and reference),
assume that there exists an effective sample size $n^\ast$ satisfying
\[
  n^\ast \le n, \qquad
  \mathbb{P}\Big(|\bar{Z} - p_{\mathrm{true}}| > \varepsilon\Big)
  \le 2 \exp\big(-2 n^\ast \varepsilon^2\big)
\]
for all $\varepsilon > 0$. In the i.i.d.\ case, $n^\ast = n$; under
$\alpha$--mixing or HMM dependence, $n^\ast$ incorporates the effective number
of independent observations (e.g.\ via standard blocking arguments).
\end{assumption}

\begin{theorem}[Finite--Sample Concentration for the Posterior Mean]
\label{thm:finite_sample_concentration}
Under the single--market block model and
Assumption~reference, fix $\varepsilon > 0$ and
define
\[
  \varepsilon_0 := \lambda |p_0 - p_{\mathrm{true}}|
  \le \lambda,
  \qquad
  \lambda = \frac{\alpha_0+\beta_0}{\alpha_0+\beta_0+n}.
\]
Then for any $\varepsilon > \varepsilon_0$,
\[
  \mathbb{P}\Big( 
    \big| \hat{p}_{\mathrm{post}} - p_{\mathrm{true}} \big|
    > \varepsilon
  \Big)
  \;\le\;
  2 \exp\left(
    - 2 n^\ast \left(
      \frac{\varepsilon - \varepsilon_0}{1-\lambda}
    \right)^2
  \right).
\]
In particular, for sufficiently large $n$ (so that $\lambda$ and $\varepsilon_0$
are small), the posterior mean concentrates around $p_{\mathrm{true}}$ at a
sub--Gaussian rate with effective sample size $n^\ast$.
\end{theorem}

\begin{proof}
Using the convex combination representation,
\[
  \hat{p}_{\mathrm{post}} - p_{\mathrm{true}}
  = \lambda(p_0 - p_{\mathrm{true}})
    + (1-\lambda)(\bar{Z} - p_{\mathrm{true}}),
\]
we obtain
\[
  \big|\hat{p}_{\mathrm{post}} - p_{\mathrm{true}}\big|
  \le \lambda |p_0 - p_{\mathrm{true}}|
      + (1-\lambda) |\bar{Z} - p_{\mathrm{true}}|
  \le \varepsilon_0 + (1-\lambda) |\bar{Z} - p_{\mathrm{true}}|.
\]
Thus, if 
$|\hat{p}_{\mathrm{post}} - p_{\mathrm{true}}| > \varepsilon$ with 
$\varepsilon > \varepsilon_0$, then necessarily
\[
  |\bar{Z} - p_{\mathrm{true}}|
  > \frac{\varepsilon - \varepsilon_0}{1-\lambda}.
\]
Applying Assumption~reference gives
\[
  \mathbb{P}\Big( 
    \big| \hat{p}_{\mathrm{post}} - p_{\mathrm{true}} \big|
    > \varepsilon
  \Big)
  \le
  \mathbb{P}\left(
    |\bar{Z} - p_{\mathrm{true}}|
    > \frac{\varepsilon - \varepsilon_0}{1-\lambda}
  \right)
  \le
  2 \exp\left(
    -2 n^\ast 
      \left(
        \frac{\varepsilon - \varepsilon_0}{1-\lambda}
      \right)^2
  \right),
\]
as claimed.
\end{proof}

\begin{remark}[Interpretation and Choice of $n^\ast$]
The bound in Theorem~reference decouples the
finite--sample error into a prior--bias term $\varepsilon_0$ (which vanishes as
$n$ dominates $\alpha_0+\beta_0$) and a stochastic term controlled by $n^\ast$.
In low--liquidity markets, both terms may be nonnegligible; PRISM should
therefore explicitly report the implied error scale
\[
  \varepsilon_{\mathrm{tol}}
  \approx \varepsilon_0
    + (1-\lambda) \sqrt{\frac{\log(2/\delta)}{2 n^\ast}}
\]
for a target tail probability $\delta$ (e.g.\ $\delta=0.05$).
\end{remark}

\paragraph{Finite--$n$ Credible Intervals via Beta Quantiles.}
For the Beta posterior $\mathrm{Beta}(\alpha,\beta)$, an exact
$(1-\gamma)$--credible interval for $p$ is given by
\[
  [\,p_{\gamma/2}^{\mathrm{low}},\; p_{1-\gamma/2}^{\mathrm{high}}\,]
  := \big[
    F_{\mathrm{Beta}(\alpha,\beta)}^{-1}(\gamma/2),\,
    F_{\mathrm{Beta}(\alpha,\beta)}^{-1}(1-\gamma/2)
  \big],
\]
where $F_{\mathrm{Beta}(\alpha,\beta)}^{-1}$ denotes the inverse incomplete
beta function. In the mixture case of 
Section~reference, credible intervals can be
computed by numerical inversion of the mixture CDF or via Monte Carlo sampling
from $\Pi_{\mathrm{mix}}$, yielding empirical quantiles that preserve
multimodality.

\begin{theorem}[Berry--Esseen--Type Bound for the Beta Posterior]
\label{thm:finite_sample_berry_esseen}
Under the i.i.d.\ single--market block model with
$p_{\mathrm{true}} \in (0,1)$ fixed and $\alpha_0,\beta_0$ bounded, let
$\Pi_T(\cdot)$ denote the posterior distribution of $p$ given $Z_1,\dots,Z_n$,
and let $\sigma_T^2 = p_{\mathrm{true}}(1-p_{\mathrm{true}})/(n+\alpha_0+\beta_0)$.
Then there exists a universal constant $C > 0$ such that
\[
  \sup_{x \in \mathbb{R}}
  \left|
    \Pi_T\big( p \le p_{\mathrm{true}} + x \sigma_T \big)
    - \Phi(x)
  \right|
  \;\le\;
  \frac{C}{\sqrt{n^\ast}},
\]
where $\Phi$ is the standard normal CDF and $n^\ast$ is the effective sample
size of Assumption~reference. In particular,
Gaussian credible intervals centered at $\hat{p}_{\mathrm{post}}$ with radius
$z_{1-\gamma/2} \sigma_T$ incur a finite--$n$ approximation error of order
$O(1/\sqrt{n^\ast})$.
\end{theorem}

\begin{remark}[Mixture Extension and Bootstrap Refinements]
For mixture posteriors $\Pi_{\mathrm{mix}}$ as in
Section~reference, the posterior mean
$\hat{p}_{\mathrm{mix}}$ is a mixture of component means and satisfies a bound
of the form
\[
  \mathbb{P}\Big(
    |\hat{p}_{\mathrm{mix}} - p_{\mathrm{true}}| > \varepsilon
  \Big)
  \le
  \sum_{k=1}^K w_k^\ast
    \mathbb{P}\Big(
      |\hat{p}_k - p_{\mathrm{true}}| > \varepsilon
    \Big),
\]
where $\hat{p}_k$ is the posterior mean under component $k$. Combining this
with Theorem~reference for each component yields a
mixture concentration bound. In practice, PRISM can supplement analytic
bounds with bootstrap resampling: resample the observed orders
$\{Z_i\}_{i=1}^n$ $B$ times, recompute $\hat{p}_{\mathrm{post}}$ or
$\hat{p}_{\mathrm{mix}}$ for each bootstrap replicate, and use the empirical
quantiles of $\{\hat{p}^{(b)}\}_{b=1}^B$ to form finite--$n$ uncertainty bands.
Such bootstrap intervals can be compared to the Beta--based credible intervals
to diagnose small--sample distortions and coverage properties.
\end{remark}

\begin{remark}[Implications for Phase 7 Simulation Metrics]
\label{remark:finite_sample_phase7}
In Phase~7, finite--sample properties of PRISM are evaluated via simulation
under low--liquidity regimes (e.g.\ $n<50$).\@
The results of this subsection provide target coverage rates and
finite--$n$ error scales for:
\begin{itemize}
  \item the absolute error $|\hat{p}_{\mathrm{post}} - p_{\mathrm{true}}|$ or
        $|\hat{p}_{\mathrm{mix}} - p_{\mathrm{true}}|$,
  \item the empirical coverage of nominal $(1-\gamma)$ credible intervals,
  \item and mispricing of YES/NO digital contracts,
        $|\hat{p}_{\mathrm{(post/mix)}} - p_{\mathrm{true}}|$.
\end{itemize}
Simulation designs should explicitly report both $n$ and $n^\ast$ to explain
deviations from asymptotic behavior, and use the bounds in
Theorem~reference and
Theorem~reference
as benchmarks for finite--sample reliability.
\end{remark}


\subsection{8.13 Robustness and Divergence Bounds for Mixture Posteriors}
\label{subsec:mixture_robustness_bounds}

The mixture posterior $\Pi_{\mathrm{mix}}$ of 
Definition~reference
extends the single--Beta PRISM posterior to multimodal settings. This 
subsection generalizes the divergence bounds of Theorem~23 to mixture 
posteriors and provides conditions under which the mixture remains stable to 
perturbations, along with explicit lower bounds showing when single--Beta 
approximations necessarily fail.

\begin{assumption}[Mixture Stability Under Perturbations]
\label{assumption:mixture_stability}
Let $\Pi_{\mathrm{mix}} = \sum_{k=1}^K w_k^\ast \,\Pi_k$
and $\widetilde{\Pi}_{\mathrm{mix}} = \sum_{k=1}^K \widetilde{w}_k^\ast
\,\widetilde{\Pi}_k$ be two mixture posteriors, where each
$\Pi_k$ and $\widetilde{\Pi}_k$ is Beta$(\alpha_k,\beta_k)$ (possibly
with different parameters). Assume:
\begin{enumerate}
  \item[(i)] Component Lipschitz continuity:
  \[
    \left\Vert \Pi_k - \widetilde{\Pi}_k \right\Vert_{\mathrm{TV}}
    \;\le\;
    L_k \left\Vert \mathcal{D} - \widetilde{\mathcal{D}} \right\Vert;
  \]
  \item[(ii)] Weight stability:
  \[
    \| w^\ast - \widetilde{w}^\ast \|_1
    \;\le\;
    C_w \left\Vert \mathcal{D}^{\mathrm{hold}} 
      - \widetilde{\mathcal{D}}^{\mathrm{hold}}
    \right\Vert;
  \]
  \item[(iii)] Bounded support separation:
  for all $k$, $\Pi_k$ and $\widetilde{\Pi}_k$ have support in $[0,1]$ with
  finite moments.
\end{enumerate}
\end{assumption}

\begin{theorem}[Mixture Posterior Robustness Bound]
\label{thm:mixture_robustness}
Under Assumption~reference, the mixture posterior
satisfies
\[
  \left\Vert 
    \Pi_{\mathrm{mix}} - \widetilde{\Pi}_{\mathrm{mix}} 
  \right\Vert_{\mathrm{TV}}
  \;\le\;
  \sum_{k=1}^K w_k^\ast L_k \,\left\Vert \mathcal{D}-\widetilde{\mathcal{D}}
  \right\Vert
  + C_w \max_{k}
    \left\Vert \Pi_k - \widetilde{\Pi}_k \right\Vert_{\mathrm{TV}}.
\]
Hence the mixture posterior inherits robustness from:  
(i) the individual component Betas, and  
(ii) the stability of the stacking weights.
\end{theorem}

\begin{proof}
Write
\[
\Pi_{\mathrm{mix}} - \widetilde{\Pi}_{\mathrm{mix}}
=
\sum_{k=1}^K 
  (w_k^\ast - \widetilde{w}_k^\ast)\,\widetilde{\Pi}_k
+
\sum_{k=1}^K 
  w_k^\ast (\Pi_k - \widetilde{\Pi}_k).
\]
Taking total variation norms and applying the triangle inequality gives
\[
  \left\Vert 
    \Pi_{\mathrm{mix}} - \widetilde{\Pi}_{\mathrm{mix}} 
  \right\Vert_{\mathrm{TV}}
  \le
  \sum_{k=1}^K |w_k^\ast - \widetilde{w}_k^\ast|
      \left\Vert \widetilde{\Pi}_k \right\Vert_{\mathrm{TV}}
  +
  \sum_{k=1}^K w_k^\ast 
      \left\Vert \Pi_k - \widetilde{\Pi}_k \right\Vert_{\mathrm{TV}}.
\]
The first term is bounded using $\|\widetilde{\Pi}_k\|_{\mathrm{TV}} = 1$ and
Assumption~reference(ii);  
the second term uses 
Assumption~reference(i).  
Combine to obtain the stated bound.
\end{proof}

\begin{theorem}[Lower Bound: Mixture Divergence Cannot Vanish Under Mode Separation]
\label{thm:mixture_divergence_lower_bound}
Let $\Pi_{\mathrm{mix}}$ be as above, and let
$\Pi_{\mathrm{Beta}}$ be the moment-matched single-Beta approximation from
Definition~reference.  
If at least two components have means separated by 
$|\mu_i - \mu_j| \ge \varepsilon > 0$, then there exists 
$c(\varepsilon) > 0$ such that
\[
  \left\Vert 
    \Pi_{\mathrm{mix}} - \Pi_{\mathrm{Beta}} 
  \right\Vert_{\mathrm{TV}}
  \;\ge\; c(\varepsilon).
\]
Thus no unimodal Beta projection can approximate the mixture posterior
uniformly when the mixture is sufficiently multimodal.
\end{theorem}

\begin{remark}[Extension of Theorem 23]
Theorem~reference extends the divergence and robustness
results of Theorem~23 to multimodal settings.  
Theorem~reference adds a complementary 
\emph{impossibility dimension}: mixture posteriors retain irreducible
multimodality, meaning that Beta projections cannot achieve vanishing
approximation error when component means are well separated.  
These bounds guide when PRISM should operate with the full mixture posterior,
and when approximation layers (e.g.\ Beta moment matching) incur unavoidable and
quantifiable loss.
\end{remark}



%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\subsection*{8.14 Weak Dependence, $\alpha$-Mixing, and Asymptotic Normality}

In earlier phases, we modeled trader actions as if they were conditionally
independent Bernoulli signals given the true event probability $p_{\mathrm{true}}$.
In practice, herding, imitation, and correlated information can induce 
dependence across orders.  Phase~7 introduced stylized dependence regimes, 
including Ising-type herding models that give rise to geometrically 
$\alpha$-mixing sequences.

In this subsection, we impose a weak dependence condition and show that the 
key asymptotic properties of PRISM---consistency and asymptotic normality of 
YES indicators that feed into the Beta--Binomial update.

\paragraph{Adjusted YES process.}
Let $\{Z_t\}_{t\ge 1}$ denote the sequence of adjusted YES indicators at the 
level of individual orders or time steps, where
\[
Z_t \in [0,1],
\]
represents the effective contribution of order $t$ to the YES count after 
behavioral weighting.  For example, in a simple specification,
\[
Z_t
=
w^{\mathrm{beh}}(x_t;\psi^\dagger)\,
\mathbbm{1}\{s_t = \mathrm{YES}\},
\]
with $w^{\mathrm{beh}}(x_t;\psi^\dagger)\in[0,1]$ a bounded weight depending 
on features $x_t$ and pseudo-true parameter $\psi^\dagger$.

For a given market with $n$ observed orders, the adjusted YES count and 
effective sample size are
\[
y_n^\ast
=
\sum_{t=1}^n Z_t + \delta_{+}^\dagger,
\qquad
n_n^\ast
=
\sum_{t=1}^n w^{\mathrm{beh}}(x_t;\psi^\dagger) + \delta_{+}^\dagger + \delta_{-}^\dagger,
\]
and the Beta--Binomial update uses $(y_n^\ast,n_n^\ast)$ together with the 
hybrid prior.  We assume $(Z_t)$ is generated under a fixed true event 
probability $p_{\mathrm{true}}\in(0,1)$.

\paragraph{$\alpha$-mixing assumptions.}
Let $\mathcal{F}_a^b = \sigma(Z_a,\dots,Z_b)$ be the sigma-algebra generated 
by the process between times $a$ and $b$, and define the $\alpha$-mixing coefficients
\[
\alpha(k)
=
\sup_{t\ge 1}
\sup_{A\in\mathcal{F}_1^t,\;B\in\mathcal{F}_{t+k}^\infty}
\bigl|
\mathbb{P}(A\cap B) - \mathbb{P}(A)\mathbb{P}(B)
\bigr|.
\]

We assume:

\begin{enumerate}
\item[(A7)] (\textbf{Boundedness})  
      The adjusted indicators are uniformly bounded: $0\le Z_t \le 1$.
\item[(A8)] (\textbf{Geometric $\alpha$-mixing})  
      There exist constants $C>0$ and $\rho\in(0,1)$ such that
      \[
      \alpha(k) \le C \rho^k,
      \qquad k\ge 1.
      \]
\item[(A9)] (\textbf{Stationarity and Identifiability})  
      The process $(Z_t)$ is strictly stationary under $p_{\mathrm{true}}$, 
      with
      \[
      \mathbb{E}[Z_t] = \mu(p_{\mathrm{true}}),
      \qquad
      \mathrm{Var}(Z_t) = \sigma_Z^2(p_{\mathrm{true}}),
      \]
      where $\mu(p)$ is strictly increasing in $p$ on $(0,1)$.
\end{enumerate}

Assumption (A8) is satisfied, for example, by the Ising-type herding model in 
Phase~7 when couplings are sufficiently weak (Proposition~7.X).

\begin{lemma}[LLN and CLT for the Adjusted YES Process]
Under (A7)--(A9), define the normalized partial sums
\[
\bar{Z}_n
=
\frac{1}{n}
\sum_{t=1}^n Z_t.
\]
Then:
\begin{enumerate}
\item[(a)] (\textbf{Law of Large Numbers})  
$\bar{Z}_n \to \mu(p_{\mathrm{true}})$ almost surely and in $L^1$ as $n\to\infty$.
\item[(b)] (\textbf{Central Limit Theorem})  
There exists $\tau^2(p_{\mathrm{true}})\in(0,\infty)$ such that
\[
\sqrt{n}\bigl(\bar{Z}_n - \mu(p_{\mathrm{true}})\bigr)
\xrightarrow{d}
\mathcal{N}\bigl(0,\tau^2(p_{\mathrm{true}})\bigr)
\quad\text{as } n\to\infty.
\]
\end{enumerate}
\end{lemma}

\begin{proof}[Proof (Sketch)]
The uniform boundedness in (A7) and geometric $\alpha$-mixing in (A8) imply 
that $(Z_t)$ satisfies the conditions of classical LLN and CLT results for 
strongly mixing sequences.  Stationarity and finite variance in (A9) ensure 
that $\mu(p_{\mathrm{true}})$ and $\sigma_Z^2(p_{\mathrm{true}})$ are well 
defined, and the asymptotic variance $\tau^2(p_{\mathrm{true}})$ can be 
expressed as a sum of autocovariances.  Standard references for mixing CLTs 
apply directly under the geometric decay of $\alpha(k)$.
\end{proof}

We now translate this into the asymptotic behavior of the PRISM posterior.

\begin{theorem}[Consistency and Asymptotic Normality of PRISM Posterior Under Dependence]
Under (A7)--(A9), suppose that for a sequence of markets with $n$ orders we 
use the adjusted counts
\[
y_n^\ast = \sum_{t=1}^n Z_t + \delta_{+}^\dagger,
\qquad
n_n^\ast = \sum_{t=1}^n w^{\mathrm{beh}}(x_t;\psi^\dagger) + \delta_{+}^\dagger + \delta_{-}^\dagger,
\]
with $w^{\mathrm{beh}}(x_t;\psi^\dagger)\in[c_w,1]$ for some $c_w>0$.  
Let $p$ denote the event probability and assume a Beta hybrid prior
\[
p \sim \mathrm{Beta}(\alpha_0,\beta_0),
\quad
\alpha_0,\beta_0 > 0,
\]
independent of $(Z_t)$.  Then:

\begin{enumerate}
\item[(a)] (\textbf{Posterior Consistency})  
Let $\Pi_n(\cdot)$ denote the PRISM posterior for $p$ based on 
$(y_n^\ast,n_n^\ast)$.  For any $\epsilon>0$,
\[
\Pi_n\bigl(|p - p_{\mathrm{true}}| > \epsilon\bigr)
\xrightarrow{P} 0
\quad\text{as } n\to\infty.
\]

\item[(b)] (\textbf{Asymptotic Normality of Posterior Mean})  
Let $\hat{p}_n$ be the posterior mean under $\Pi_n$.  Then
\[
\sqrt{n}
\bigl(
\hat{p}_n - p_{\mathrm{true}}
\bigr)
\xrightarrow{d}
\mathcal{N}\bigl(0, V(p_{\mathrm{true}})\bigr),
\]
for some finite $V(p_{\mathrm{true}})$ determined by $\tau^2(p_{\mathrm{true}})$ 
and the weight structure.

\item[(c)] (\textbf{Bernstein--von Mises Approximation})  
The posterior distribution $\Pi_n$ is asymptotically normal in the sense that
\[
\sup_{A\subset\mathbb{R}}
\biggl|
\Pi_n\bigl(\sqrt{n}(p-p_{\mathrm{true}})\in A\bigr)
-
\mathcal{N}\bigl(0,V(p_{\mathrm{true}})\bigr)(A)
\biggr|
\xrightarrow{P} 0.
\]
\end{enumerate}
\end{theorem}

\begin{proof}[Proof (Sketch)]
The effective sample size $n_n^\ast$ grows linearly with $n$ due to the lower 
bound $w^{\mathrm{beh}}(x_t;\psi^\dagger)\ge c_w>0$.  The adjusted mean 
\[
\bar{Z}_n^\ast
=
\frac{y_n^\ast - \delta_{+}^\dagger}
     {n_n^\ast - \delta_{+}^\dagger - \delta_{-}^\dagger}
\]
is a smooth function of $\bar{Z}_n$ and the average weight, so the LLN and CLT 
from the lemma transfer to $\bar{Z}_n^\ast$ via the delta method.  In 
particular, $\bar{Z}_n^\ast \to \mu(p_{\mathrm{true}})$ and
\[
\sqrt{n}(\bar{Z}_n^\ast - \mu(p_{\mathrm{true}}))
\xrightarrow{d}
\mathcal{N}(0,\tilde{\tau}^2(p_{\mathrm{true}})).
\]

The Beta posterior based on $(y_n^\ast,n_n^\ast)$ has mean and variance that 
can be written as smooth functions of $(\bar{Z}_n^\ast,n_n^\ast)$.  As 
$n_n^\ast\sim c\,n$ for some $c>0$, standard Bayesian asymptotics for 
one-dimensional parameters under weak dependence (together with the mixing 
LLN/CLT) yield posterior consistency and a Bernstein--von Mises type result.  
The asymptotic variance $V(p_{\mathrm{true}})$ incorporates both the intrinsic 
$\tilde{\tau}^2(p_{\mathrm{true}})$.
\end{proof}

\begin{remark}[Small Samples and Long-Shot Regimes]
The results above are asymptotic in nature and require the effective sample 
size $n_n^\ast$ to grow without bound.  In low-liquidity markets (small $n$) 
or extreme-probability regimes (long-shot events with $p_{\mathrm{true}}$ near 
$0$ or $1$), the Beta posterior can be highly skewed and heavy-tailed, and the 
normal approximations may be poor.  In such regimes, PRISM should be used 
with caution, relying on full posterior credible intervals rather than 
Gaussian approximations, and sensitivity analysis with respect to the prior 
and adjustment parameters is particularly important.
\end{remark}


\begin{prop}[Finite-Sample Concentration under Mixing]
Let $(Z_t)$ be the adjusted YES process satisfying the boundedness and
geometric $\alpha$-mixing conditions of Phase~8.  Let
\[
\bar{Z}_n = \frac{1}{n}\sum_{t=1}^n Z_t,
\qquad
\mu = \mathbb{E}[Z_t].
\]
Then, for all $x>0$ and $n\ge 1$, there exist constants $C_1,C_2>0$ (depending
on the mixing coefficients and bounds on $Z_t$) such that
\[
\mathbb{P}\bigl(|\bar{Z}_n - \mu| > x\bigr)
\le
C_1 \exp\Bigl(- C_2 n x^2\Bigr),
\]
i.e.\ a Bernstein-type exponential inequality holds for the empirical average.
In particular, for any confidence level $\delta\in(0,1)$, with probability at
least $1-\delta$,
\[
|\bar{Z}_n - \mu|
\lesssim
\sqrt{\frac{\log(1/\delta)}{n}},
\]
up to constants depending on the mixing structure.
\end{prop}

\begin{remark}[Practical Implications in Low-$n$ and Long-Shot Regimes]
The concentration bound above is asymptotic in spirit: it guarantees that, for
moderate $n$ and weak dependence, $\bar{Z}_n$ will concentrate around $\mu$ at
a rate comparable to the i.i.d.\ case.  However, in low-liquidity markets
(small $n$), or when $p$ is very close to $0$ or $1$, three issues arise:
\begin{itemize}
\item The constants $C_1,C_2$ may be unfavorable, leading to loose finite-sample bounds.
\item The Beta posterior can be highly skewed, so Gaussian approximations to
      credible intervals may be misleading.
\item Long-shot events make the empirical process more volatile relative to the
      natural scale of $p$, further weakening normal approximations.
\end{itemize}
In these situations, PRISM posteriors should be interpreted via full
credible intervals and sensitivity checks, rather than relying solely on
asymptotic normality or point estimates.
\end{remark}


\subsection*{8.15 Metric--Based Robustness for Nonlinear Posterior Updates}
\label{subsec:metric_robustness_nonlinear}

The original Lipschitz robustness results in this phase implicitly assumed a
linear or affine adjustment of pseudo--counts, so that the posterior mapping
from data to distribution was essentially linear. Once nonlinear distortions,
mixture posteriors, and dynamic bias layers are introduced (Sections
reference,
reference,
reference), the adjustment of the posterior can no
longer be modeled as a simple additive transformation. This subsection replaces
the linear Lipschitz arguments with metric--based robustness results formulated
in Wasserstein and Hellinger distances for general nonlinear updates.

\paragraph{Setup.}
Let $\Theta = [0,1]$ denote the parameter space for the event probability
$p$. For a given dataset $\mathcal{D}$, let $\Pi_{\mathrm{base}}(\cdot \mid \mathcal{D})$
be the \emph{base} PRISM posterior over $p$, constructed as in Phases~5--6
(e.g.\ a mixture of Beta components calibrated via stacking and dynamic bias
layers). We model nonlinear bias correction at the posterior level via a
measurable map
\[
  \phi : \Theta \to \Theta,
\]
which acts on $p$ to produce an adjusted parameter $\phi(p)$.\footnote{In
  applications, $\phi$ may depend on additional covariates or summaries of
  $\mathcal{D}$; here we treat these as fixed when conditioning on
  $\mathcal{D}$.}

\begin{definition}[Nonlinear Posterior Update via Pushforward]
\label{def:nonlinear_posterior_update}
For a given dataset $\mathcal{D}$ and base posterior
$\Pi_{\mathrm{base}}(\cdot \mid \mathcal{D})$, the \emph{nonlinearly adjusted}
posterior is defined as the pushforward measure
\[
  \Pi_{\phi}(\cdot \mid \mathcal{D})
  := \phi_{\#}\Pi_{\mathrm{base}}(\cdot \mid \mathcal{D}),
\]
i.e.\ for any Borel set $A \subseteq \Theta$,
\[
  \Pi_{\phi}(A \mid \mathcal{D})
  = \Pi_{\mathrm{base}}\big(\phi^{-1}(A) \mid \mathcal{D}\big).
\]
\end{definition}

We are interested in how sensitive $\Pi_{\phi}(\cdot \mid \mathcal{D})$ is to
small changes in $\mathcal{D}$, measured in appropriate probability metrics
(e.g.\ Wasserstein $W_1$ or Hellinger distance $d_H$).

\begin{assumption}[Base Posterior Robustness in Wasserstein Distance]
\label{assumption:base_posterior_wasserstein}
There exists a data metric $d_{\mathcal{D}}$ on the space of datasets such
that for any two datasets $\mathcal{D},\mathcal{D}'$,
\[
  W_1\!\left(
    \Pi_{\mathrm{base}}(\cdot \mid \mathcal{D}),
    \Pi_{\mathrm{base}}(\cdot \mid \mathcal{D}')
  \right)
  \;\le\;
  C_{\mathrm{base}}\, d_{\mathcal{D}}(\mathcal{D},\mathcal{D}'),
\]
for some constant $C_{\mathrm{base}} < \infty$. Here $W_1$ is the
$1$--Wasserstein distance on $\Theta$ with respect to the Euclidean metric.
\end{assumption}

Assumption~reference is a metric formulation
of the robustness results established earlier (e.g.\ those analogous to
Theorem~13 and Theorem~reference), expressed at the level of
the base posterior $\Pi_{\mathrm{base}}$.

\begin{assumption}[Lipschitz Nonlinear Adjustment]
\label{assumption:lipschitz_phi}
The nonlinear adjustment map $\phi : \Theta \to \Theta$ is globally Lipschitz
with constant $L_\phi < \infty$:
\[
  |\phi(p_1) - \phi(p_2)| \le L_\phi |p_1 - p_2|
  \quad \text{for all } p_1,p_2 \in \Theta.
\]
\end{assumption}

\begin{theorem}[Wasserstein Robustness of Nonlinear Posterior Updates]
\label{thm:wasserstein_robustness_phi}
Under Assumptions~reference and
reference, the adjusted posterior mapping
$\mathcal{D} \mapsto \Pi_{\phi}(\cdot \mid \mathcal{D})$ is Lipschitz in
$W_1$:
\[
  W_1\!\left(
    \Pi_{\phi}(\cdot \mid \mathcal{D}),
    \Pi_{\phi}(\cdot \mid \mathcal{D}')
  \right)
  \;\le\;
  L_\phi \, C_{\mathrm{base}}\,
  d_{\mathcal{D}}(\mathcal{D},\mathcal{D}')
  \quad \text{for all } \mathcal{D},\mathcal{D}'.
\]
\end{theorem}

\begin{proof}
By Definition~reference,
$\Pi_{\phi}(\cdot \mid \mathcal{D}) = \phi_{\#}\Pi_{\mathrm{base}}(\cdot \mid \mathcal{D})$.
The $1$--Wasserstein distance is contracting under Lipschitz maps:
for any two measures $\mu,\nu$ on $\Theta$ and any $L$--Lipschitz map $\phi$,
\[
  W_1(\phi_{\#}\mu, \phi_{\#}\nu)
  \le L \, W_1(\mu,\nu).
\]
Applying this with $\mu = \Pi_{\mathrm{base}}(\cdot \mid \mathcal{D})$,
$\nu = \Pi_{\mathrm{base}}(\cdot \mid \mathcal{D}')$, and
$L = L_\phi$, we get
\[
  W_1\!\left(
    \Pi_{\phi}(\cdot \mid \mathcal{D}),
    \Pi_{\phi}(\cdot \mid \mathcal{D}')
  \right)
  \le
  L_\phi\, W_1\!\left(
    \Pi_{\mathrm{base}}(\cdot \mid \mathcal{D}),
    \Pi_{\mathrm{base}}(\cdot \mid \mathcal{D}')
  \right).
\]
Combining this with
Assumption~reference
yields the claimed bound.
\end{proof}

\begin{remark}[Lipschitz Regularization in Calibration]
\label{remark:lipschitz_regularization}
Theorem~reference shows that the robustness constant
for the adjusted posterior is the product $L_\phi C_{\mathrm{base}}$. Thus, in
calibrating $\phi$ (e.g.\ via empirical risk minimization on historical markets),
it is natural to penalize large Lipschitz constants. A practical approach is to
include a regularization term of the form
\[
  \lambda \, \mathrm{Lip}(\phi)
  \quad\text{or}\quad
  \lambda \, \|\nabla \phi\|_{L^2},
\]
in the calibration objective, trading off fit against robustness. This yields
an explicit statistical justification for Lipschitz regularization of nonlinear
posterior updates.
\end{remark}

\paragraph{Extension to Hellinger Distance.}
In some arguments, it is convenient to work with Hellinger distance $d_H$ 
between posterior densities. For one--dimensional models with sufficiently
smooth densities and strictly monotone $\phi$, we can relate Hellinger
distances before and after the nonlinear transformation.

\begin{assumption}[Smooth Monotone Nonlinear Adjustment]
\label{assumption:hellinger_phi}
Assume $\phi : (0,1) \to (0,1)$ is a $C^1$ diffeomorphism with derivative
bounded away from $0$ and $\infty$:
\[
  0 < m \le \phi'(p) \le M < \infty
  \quad \text{for all } p \in (0,1).
\]
Let $\pi_{\mathrm{base}}(p \mid \mathcal{D})$ and
$\pi_{\mathrm{base}}(p \mid \mathcal{D}')$ denote posterior densities, and
$\pi_{\phi}(q \mid \mathcal{D})$ the density of $\Pi_{\phi}(\cdot \mid \mathcal{D})$
under the change of variables $q = \phi(p)$.
\end{assumption}

\begin{theorem}[Hellinger Stability Under Smooth Monotone Transformations]
\label{thm:hellinger_robustness_phi}
Under Assumption~reference, there exists a constant
$C_{H} = C_{H}(m,M)$ such that for any two datasets
$\mathcal{D},\mathcal{D}'$,
\[
  d_H\!\left(
    \Pi_{\phi}(\cdot \mid \mathcal{D}),
    \Pi_{\phi}(\cdot \mid \mathcal{D}')
  \right)
  \;\le\;
  C_{H}\,
  d_H\!\left(
    \Pi_{\mathrm{base}}(\cdot \mid \mathcal{D}),
    \Pi_{\mathrm{base}}(\cdot \mid \mathcal{D}')
  \right).
\]
In particular, if the base posterior mapping is Hellinger--Lipschitz in
$\mathcal{D}$, then so is the adjusted posterior mapping.
\end{theorem}

\begin{proof}[Proof Sketch]
Under the change of variables $q=\phi(p)$, densities transform via
\[
  \pi_{\phi}(q \mid \mathcal{D})
  = \pi_{\mathrm{base}}(\phi^{-1}(q) \mid \mathcal{D})\,
    \big|\phi^{-1}\big)'(q)\big|.
\]
The Hellinger distance between the transformed densities is controlled by the
Hellinger distance between the original densities and the Jacobian factors,
with constants depending only on bounds for $\phi'$ and $(\phi^{-1})'$. Using
$m \le \phi'(p) \le M$ and standard change-of-variable bounds, one obtains the
stated inequality with $C_H$ depending only on $(m,M)$.
\end{proof}

\paragraph{Implications for Pricing Functionals.}
Let $f : \Theta \to \mathbb{R}$ be a 1--Lipschitz payoff functional of $p$
(e.g.\ $f(p) = \mathbf{1}\{p > K\}$ smoothed, or a bounded Lipschitz proxy for
digital pricing). Then for any two datasets $\mathcal{D},\mathcal{D}'$,
\[
  \left|
    \int f(p)\, \Pi_{\phi}(dp \mid \mathcal{D})
    - \int f(p)\, \Pi_{\phi}(dp \mid \mathcal{D}')
  \right|
  \le
  W_1\!\left(
    \Pi_{\phi}(\cdot \mid \mathcal{D}),
    \Pi_{\phi}(\cdot \mid \mathcal{D}')
  \right).
\]
Combining with Theorem~reference yields a bound on
the sensitivity of PRISM prices to data perturbations under nonlinear
adjustments:
\[
  \left|
    \hat{C}_{\phi}(\mathcal{D}) - \hat{C}_{\phi}(\mathcal{D}')
  \right|
  \le
  L_\phi C_{\mathrm{base}} \,
  d_{\mathcal{D}}(\mathcal{D},\mathcal{D}'),
\]
for any Lipschitz payoff functional $f$ used in price computation.

\begin{remark}[Failure Modes and Relation to F2--F3]
\label{remark:robustness_failure_modes}
The robustness results in this subsection rely critically on two properties:
(i) the base posterior must be stable in Wasserstein or Hellinger distance, and
(ii) the nonlinear update $\phi$ must be Lipschitz (or smooth monotone with
bounded derivative). In the strong herding and fast regime--switching regimes
(Sections~reference and
reference), condition (i) fails: small changes
in data can produce large changes in the base posterior. In such regimes,
no choice of $\phi$ (Lipschitz or otherwise) can restore uniform robustness.
Thus the metric--based Lipschitz results here apply to the ``regular'' region
of the model space where dependence and nonstationarity remain within the
bounds of the PRISM assumptions.
\end{remark}


\subsection*{8.16 Failure of Robustness Under Strong Herding: A Threshold Auto--Regressive Counterexample}
\label{subsec:strong_herding_impossibility}

This section establishes that the robustness guarantees of PRISM fail under 
\emph{strong herding}, understood as threshold--based majority--following
behavior in which order flow becomes nearly deterministic once the fraction of
YES orders crosses a critical threshold. In this regime, the dependence in the
adjusted order process $Z_t$ violates the mixing assumptions (A7)--(A9), the
likelihood becomes multimodal, and neither a single--Beta posterior nor any
finite mixture of Betas can recover the true event probability uniformly over
the strong--herding parameter region.

\begin{definition}[Threshold Auto--Regressive Herding Model]
\label{def:TAR_herding}
Let $(Z_t)_{t \ge 1}$ denote the effective YES--indicator process entering
Stage~1 after behavioral weighting. For parameters $(\theta,\rho)$ with
$\theta \in (0,1)$ and $\rho \in [0,1]$, define
\[
  Z_{t+1} = 
  \begin{cases}
    1 & \text{with probability } \rho
        \quad\text{if } \displaystyle \frac{1}{t}\sum_{i=1}^t Z_i \ge \theta,\\[1.0em]
    0 & \text{with probability } \rho
        \quad\text{if } \displaystyle \frac{1}{t}\sum_{i=1}^t Z_i < \theta,\\[1.0em]
    \text{Ber}(p_{\mathrm{true}}) & \text{with probability } 1-\rho.
  \end{cases}
\]
When $\rho$ is close to $1$, the process follows a majority rule as soon as the
empirical fraction of YES exceeds the threshold $\theta$, and otherwise follows
a pure ``NO--cascade''. We call this the \emph{strong--herding regime}.
\end{definition}

\begin{lemma}[Loss of Mixing Under Strong Herding]
\label{lemma:loss_of_mixing}
Fix $\theta \in (0,1)$ and let $\rho \to 1$. Under the TAR model of
Definition~reference, the process $(Z_t)$ fails to satisfy 
$\alpha$--mixing with any summable mixing rate. In particular,
\[
  \alpha_Z(k) \not\to 0 \qquad \text{as } k\to\infty
\]
whenever the event
$\{\frac{1}{t} \sum_{i=1}^t Z_i \ge \theta\}$ occurs with positive probability.
\end{lemma}

\begin{proof}
When $\rho \to 1$, the transition becomes
\[
  Z_{t+1} = \mathbf{1}\left\{\frac{1}{t}\sum_{i=1}^t Z_i \ge \theta\right\}
  \quad \text{a.s.},
\]
so once the empirical mean crosses $\theta$, the process becomes identically
$1$ thereafter. Similarly, if the empirical mean remains below $\theta$, the
process becomes identically $0$. Hence $(Z_t)$ becomes asymptotically constant
and perfectly predictable from events arbitrarily far in the past, implying
$\alpha_Z(k)=1$ for all $k$. Summability fails, completing the proof.
\end{proof}

\begin{lemma}[Bimodal Likelihood Under Strong Herding]
\label{lemma:bimodal_likelihood}
Let $L_n(p)$ denote the likelihood of $p_{\mathrm{true}}$ based on $Z_1,\dots,Z_n$
under the TAR model. If $\rho$ is sufficiently close to $1$, then with positive
probability
\[
  L_n(p) \;\text{ is asymptotically bimodal on } [0,1],
\]
with modes concentrated near $0$ and $1$ corresponding to NO--cascades and
YES--cascades respectively.
\end{lemma}

\begin{proof}
From Lemma~reference, the process enters a deterministic
regime once the empirical mean crosses $\theta$. If it crosses upward,
$Z_t=1$ for all large $t$, which for the Bernoulli likelihood behaves as if
$p_{\mathrm{true}}=1$. If it crosses downward, $Z_t=0$ eventually, behaving as
if $p_{\mathrm{true}}=0$. Since both cascades occur with positive probability
whenever $p_{\mathrm{true}}$ is not exactly equal to $\theta$, the likelihood
assigns mass to neighborhoods of both $0$ and $1$. Bimodality follows.
\end{proof}

\begin{theorem}[Impossibility of Uniform Consistency Under Strong Herding]
\label{thm:impossibility_herding}
Let $\Pi_{\mathrm{PRISM}}(\cdot \mid \mathcal{D}_1,\mathcal{D}_2)$ denote the
Stage~2 posterior of PRISM, modeled as a finite mixture of Beta distributions.
Under the TAR herding model
(Definition~reference), the following statements hold.

\begin{enumerate}
\item[(i)] (\emph{No uniform consistency})  
  For any finite mixture of Beta distributions with $R<\infty$ components,
  \[
    \sup_{p_{\mathrm{true}} \in (0,1)}
    \mathbb{E}_{p_{\mathrm{true}}}
    \left[
      \left|
        \mathbb{E}\!\left[p \mid Z_1,\dots,Z_n\right]
        - p_{\mathrm{true}}
      \right|
    \right]
    \;\not\to\; 0
    \qquad\text{as }\rho\to 1.
  \]

\item[(ii)] (\emph{Failure of finite--mixture representation})  
  The bimodality of the likelihood (Lemma~reference)
  implies that any finite mixture of Betas fails to approximate the true
  posterior uniformly over the strong--herding region $\{\rho> \rho^\ast\}$ for
  any fixed $\rho^\ast<1$.

\item[(iii)] (\emph{Breakdown of robustness theorems})  
  The Lipschitz--type continuity results of Phase~8 cannot hold under strong
  herding: no constant $C<\infty$ can satisfy
  \[
    \left\Vert
      \Pi_{\mathrm{PRISM}}(\cdot \mid \mathcal{D})
      - \Pi_{\mathrm{PRISM}}(\cdot \mid \mathcal{D}')
    \right\Vert_{\mathrm{TV}}
    \;\le\;
    C\,\left\Vert \mathcal{D} - \mathcal{D}'\right\Vert
  \]
  for all $\rho$ sufficiently close to $1$.  
  The deterministic cascades in strong herding force the total variation
  distance to jump by $1$ when $\mathcal{D}$ crosses the threshold~$\theta$.
\end{enumerate}
\end{theorem}

\begin{proof}
(i) From Lemma~reference, the likelihood assigns mass to
neighborhoods of $0$ and $1$ with positive probability even when
$p_{\mathrm{true}} \in (\theta-\varepsilon,\theta+\varepsilon)$. A finite
mixture of Betas cannot track both cascades simultaneously: its posterior mean
necessarily lies in a compact subinterval of $(0,1)$ independent of the data
configuration producing the cascades. Thus, the posterior mean cannot converge
uniformly to $p_{\mathrm{true}}$ as $\rho\to 1$, proving (i).

(ii) Any finite mixture of Betas has unimodal or mildly multimodal densities,
but cannot represent a sequence of likelihoods whose mass splits between
$0$ and $1$ in a way that depends discontinuously on the data path. Therefore
no finite mixture can uniformly approximate the posterior, yielding (ii).

(iii) Let $\mathcal{D}$ and $\mathcal{D}'$ differ only by whether the empirical
mean crosses $\theta$ at time $t_0$. Under strong herding, the posteriors
collapse to neighborhoods of $0$ and $1$ respectively, giving total variation
distance equal to $1$. Since $\|\mathcal{D}-\mathcal{D}'\|$ can be arbitrarily
small (e.g.\ one flip of a single Bernoulli), no Lipschitz constant can satisfy
the inequality uniformly. This proves (iii).
\end{proof}

\begin{remark}[Interpretation]
\label{remark:herding_interpretation}
The strong--herding regime invalidates the data--generating assumptions required
for PRISM's robustness theory. Deterministic cascades destroy mixing,
induce bimodal likelihoods, and generate posterior discontinuities that no
finite--mixture Beta representation can smooth uniformly. Thus the impossibility
theorem above provides a fundamental limitation: PRISM can be consistent and
Lipschitz--robust only on subregions of the parameter space where dependence is
sufficiently weak and mixing conditions (A7)--(A9) hold.
\end{remark}


\subsection*{8.17 Boundary Behavior, Long-Shot Events, and Stabilization}

The PRISM posterior for the event probability $p$ is Beta with parameters
\[
p \mid \mathcal{I}
\sim
\mathrm{Beta}(\alpha_n,\beta_n),
\qquad
\alpha_n = \alpha_0 + y_n^\ast,
\quad
\beta_n = \beta_0 + n_n^\ast - y_n^\ast,
\]
where $(y_n^\ast,n_n^\ast)$ are the adjusted counts and 
$(\alpha_0,\beta_0)$ are the hybrid prior parameters.  In long-shot regimes 
($p_{\mathrm{true}}$ near $0$ or $1$) or in very small samples, it is possible 
for the posterior mass to concentrate near $0$ or $1$, leading to heavy tails 
and numerical instability for functions of $p$ (e.g.\ log-odds or certain 
risk measures).

We formalize this behavior and describe a simple stabilization based on 
either truncation or a logit transform.

\begin{lemma}[Tail Behavior of Beta Posteriors Near the Boundaries]
Let $\Pi_n$ denote the $\mathrm{Beta}(\alpha_n,\beta_n)$ posterior for $p$.
\begin{enumerate}
\item[(a)] If $\alpha_n \le 1$, then the density of $\Pi_n$ behaves like
\[
\pi_n(p)
\propto p^{\alpha_n-1}(1-p)^{\beta_n-1}
\sim
p^{\alpha_n-1}
\quad\text{as } p\downarrow 0,
\]
so that the left tail near $0$ is heavy whenever $\alpha_n<1$.
\item[(b)] If $\beta_n \le 1$, then
\[
\pi_n(p)
\sim
(1-p)^{\beta_n-1}
\quad\text{as } p\uparrow 1,
\]
and the right tail near $1$ is heavy whenever $\beta_n<1$.
\item[(c)] If $\alpha_n,\beta_n \ge c>1$ uniformly in $n$, then there exists 
      $\varepsilon>0$ such that
\[
\Pi_n\bigl([0,\varepsilon)\cup(1-\varepsilon,1]\bigr)
\le C \varepsilon^c,
\]
for some constant $C>0$ independent of $n$.  In particular, the posterior 
places vanishing mass near $0$ and $1$ as $\varepsilon\downarrow 0$.
\end{enumerate}
\end{lemma}

\begin{proof}[Proof (Sketch)]
Parts (a) and (b) follow from the Beta density
\[
\pi_n(p)
=
\frac{1}{\mathrm{B}(\alpha_n,\beta_n)}\,
p^{\alpha_n-1}(1-p)^{\beta_n-1},
\]
and standard asymptotics as $p\to 0$ and $p\to 1$.  When $\alpha_n<1$, the 
factor $p^{\alpha_n-1}$ diverges as $p\downarrow 0$, indicating a heavy left 
tail; an analogous statement holds for $\beta_n<1$ near $1$.

For (c), if $\alpha_n,\beta_n\ge c>1$ and $p\in(0,\varepsilon)$, then
\[
\pi_n(p)
\le
\frac{1}{\mathrm{B}(\alpha_n,\beta_n)} p^{c-1},
\]
and integrating on $(0,\varepsilon)$ gives
\[
\Pi_n([0,\varepsilon))
\le
C_1 \varepsilon^c
\]
for some $C_1>0$ that can be chosen uniformly in $n$ due to the compactness of 
the parameter region.  A symmetric argument holds near $1$.
\end{proof}

The lemma shows that heavy tails near $0$ and $1$ arise precisely when the 
posterior parameters $\alpha_n$ and $\beta_n$ are small, which can occur under
three circumstances:
(i) very small effective sample size $n_n^\ast$,
(ii) extreme long-shot outcomes (e.g.\ no YES orders in a rare-event market),
or (iii) extremely concentrated or misaligned priors.

To mitigate numerical instability and avoid overconfident long-shot posteriors 
in these regimes, we consider a simple stabilized transform.

\begin{prop}[Stabilized Posterior via Truncation or Logit Transform]
Fix a truncation parameter $\varepsilon\in(0,1/2)$ and define the truncated 
interval
\[
I_\varepsilon = [\varepsilon,1-\varepsilon].
\]
Let $\Pi_n$ be the $\mathrm{Beta}(\alpha_n,\beta_n)$ posterior and define the 
truncated posterior $\Pi_n^\varepsilon$ by
\[
\Pi_n^\varepsilon(A)
=
\frac{\Pi_n(A\cap I_\varepsilon)}
     {\Pi_n(I_\varepsilon)},
\qquad
A\subseteq[0,1]\ \text{measurable},
\]
whenever $\Pi_n(I_\varepsilon)>0$.

Then:
\begin{enumerate}
\item[(a)] If $\alpha_n,\beta_n \ge c>1$ uniformly in $n$, then
\[
\|\Pi_n - \Pi_n^\varepsilon\|_{\mathrm{TV}}
=
\Pi_n\bigl([0,\varepsilon)\cup(1-\varepsilon,1]\bigr)
\le
C \varepsilon^c,
\]
for some $C>0$ independent of $n$.  Thus, for small $\varepsilon$, the 
truncated posterior is close in total variation to the original posterior.

\item[(b)] Define the logit transform 
\[
\theta = \log\frac{p}{1-p},
\]
and let $\Lambda_n$ be the induced posterior distribution for $\theta$ under 
$\Pi_n^\varepsilon$.  Then moments of all orders exist for $\Lambda_n$, and 
$\Lambda_n$ is supported on a compact interval
\[
\Theta_\varepsilon
=
\biggl[
\log\frac{\varepsilon}{1-\varepsilon},
\ \log\frac{1-\varepsilon}{\varepsilon}
\biggr].
\]
Consequently, functionals of $p$ that are Lipschitz in $\theta$ are uniformly 
bounded and well-behaved under $\Lambda_n$.

\item[(c)] If $\varepsilon=\varepsilon_n\downarrow 0$ is chosen such that 
$\varepsilon_n^c \to 0$ and $n_n^\ast\to\infty$, then the truncated posterior 
$\Pi_n^{\varepsilon_n}$ remains asymptotically equivalent to $\Pi_n$ for 
inference about $p_{\mathrm{true}}$ while preventing extreme numerical 
instability near $0$ and $1$ at finite $n$.
\end{enumerate}
\end{prop}

\begin{proof}[Proof (Sketch)]
For (a), observe that truncation only removes mass near $0$ and $1$, so the 
total variation distance equals the probability of the removed regions:
\[
\|\Pi_n - \Pi_n^\varepsilon\|_{\mathrm{TV}}
=
\Pi_n\bigl([0,\varepsilon)\cup(1-\varepsilon,1]\bigr).
\]
The bound then follows directly from part (c) of the lemma, with the same 
exponent $c$ and an adjusted constant $C$.

For (b), the truncation ensures that $p\in I_\varepsilon$ almost surely under 
$\Pi_n^\varepsilon$.  The logit map 
$p\mapsto\theta=\log(p/(1-p))$ sends $I_\varepsilon$ to $\Theta_\varepsilon$, 
a compact interval in $\mathbb{R}$, and is smooth on $(0,1)$.  As a result, 
all moments of $\theta$ under $\Lambda_n$ exist and are bounded uniformly in 
$n$.  Any functional of $p$ that can be expressed as a Lipschitz function of 
$\theta$ thus inherits uniform boundedness and stability.

For (c), if $\varepsilon_n^c\to 0$, then the total variation distance between
$\Pi_n$ and $\Pi_n^{\varepsilon_n}$ tends to zero by (a).  At the same time, 
the growing effective sample size $n_n^\ast$ drives the posterior mass towards
$p_{\mathrm{true}}$, and the truncation can be chosen small enough that it does
not distort the asymptotic concentration in typical cases (where 
$p_{\mathrm{true}}\in(0,1)$).  Hence the truncated posterior is 
asymptotically equivalent to the original one for inference while improving 
finite-sample stability.
\end{proof}

\begin{remark}[Practical Guidance for Long-Shot Markets]
The analysis above suggests a simple stabilization strategy for PRISM in
long-shot or low-liquidity markets:
\begin{itemize}
\item Choose a small truncation level $\varepsilon$ (e.g.\ $10^{-4}$ or 
      $10^{-3}$) and work with the truncated posterior $\Pi_n^\varepsilon$.
\item When transforming probabilities (e.g.\ to log-odds), perform the 
      transform on $\theta = \log(p/(1-p))$ under $\Lambda_n$ rather than 
      directly on $p$ near the boundaries.
\item Report both the truncated posterior summaries and the original Beta 
      summaries, especially in cases where $\alpha_n$ or $\beta_n$ are close 
      to $1$ or below.
\end{itemize}
This keeps the asymptotic properties intact while explicitly addressing the 
finite-sample instabilities associated with heavy Beta tails near $0$ and $1$.
\end{remark}

\subsection*{8.18 PRISM as a KL Projection: A Formal Information-Theoretic Interpretation}

The adjusted PRISM posterior for the event probability $p$ is a Beta
distribution of the form
\[
p \mid \mathcal{I}
\sim
\mathrm{Beta}(\alpha_n,\beta_n),
\qquad
\alpha_n = \alpha_0 + y_n^\ast,\quad
\beta_n = \beta_0 + n_n^\ast - y_n^\ast,
\]
constructed from the hybrid prior and adjusted parimutuel counts.  
Up to this point the Beta form has been motivated by conjugacy and 
interpretability.  Here we show that it also admits a fundamental 
\emph{information-theoretic characterization}: it is the KL-projection of a 
general posterior onto the Beta family.

Let $\Pi^\star$ denote the (hypothetical) posterior distribution for $p$ that
would arise under a fully specified, potentially nonparametric 
data-generating model with dependence, behavioral distortions, or heterogeneous
signals.  In a general market this $\Pi^\star$ may not be Beta, and may not be 
computationally tractable.

PRISM provides a tractable Beta posterior.  
The following theorem shows that the PRISM posterior coincides with the 
\emph{I-projection} (information projection) of $\Pi^\star$ onto the Beta 
family $\mathcal{B} = \{\mathrm{Beta}(a,b): a,b>0\}$.

\begin{theorem}[KL Projection Theorem for PRISM]
Let $\Pi^\star$ be any posterior distribution on $p\in(0,1)$ with finite mean
and finite $\log$-moment, and let $\mathcal{B}$ be the family of Beta
distributions.  Consider the KL divergence
\[
D_{\mathrm{KL}}(\Pi^\star \,\|\, \mathrm{Beta}(a,b))
=
\int_0^1 \log\frac{d\Pi^\star}{d\mathrm{Beta}(a,b)}(p)
\, d\Pi^\star(p),
\]
defined whenever $\Pi^\star$ is absolutely continuous with respect to 
$\mathrm{Beta}(a,b)$.

Define the KL-projection of $\Pi^\star$ onto $\mathcal{B}$ as
\[
(a^\dagger,b^\dagger)
\in
\arg\min_{a,b>0}
D_{\mathrm{KL}}(\Pi^\star \,\|\, \mathrm{Beta}(a,b)).
\]

Assume that $\Pi^\star$ has mean $m^\star$ and inverse second moment
$M^\star = \mathbb{E}_{\Pi^\star}[p^{-1} + (1-p)^{-1}] < \infty$.
Then:

\begin{enumerate}
\item[(a)] The minimizer $(a^\dagger,b^\dagger)$ exists and is unique.

\item[(b)] The KL-projection satisfies
\[
\frac{a^\dagger}{a^\dagger + b^\dagger} = m^\star,
\qquad
\frac{a^\dagger b^\dagger}{(a^\dagger + b^\dagger)^2(a^\dagger + b^\dagger + 1)}
=
\mathrm{Var}_{\Pi^\star}(p),
\]
i.e.\ the minimizing Beta distribution matches the mean and variance of 
$\Pi^\star$.

\item[(c)] If $\Pi^\star$ is generated by a hybrid prior and adjusted counts 
$y_n^\ast,n_n^\ast$ (with mixing or dependence), then the PRISM posterior
$\mathrm{Beta}(\alpha_n,\beta_n)$ coincides with the KL-projection:
\[
(\alpha_n,\beta_n)
=
(a^\dagger,b^\dagger).
\]

\item[(d)] In particular, for large effective sample size $n_n^\ast$, PRISM 
selects, among all Beta distributions, the one closest in KL sense to the 
ideal but intractable $\Pi^\star$.
\end{enumerate}
\end{theorem}

\begin{proof}[Proof (Sketch)]
Part (a) follows from strict convexity of the KL divergence in $(a,b)$ on the 
Beta family.  For (b), writing the KL divergence explicitly and differentiating 
under the integral yields two first-order conditions:
\[
\frac{\partial}{\partial a}
D_{\mathrm{KL}}
=
0
\quad\Rightarrow\quad
\mathbb{E}_{\Pi^\star}[\log p]
=
\psi(a) - \psi(a+b),
\]
\[
\frac{\partial}{\partial b}
D_{\mathrm{KL}}
=
0
\quad\Rightarrow\quad
\mathbb{E}_{\Pi^\star}[\log (1-p)]
=
\psi(b) - \psi(a+b),
\]
where $\psi$ is the digamma function.  
Using identities for Beta means and variances, these conditions imply the 
matching of mean and variance stated in (b).

For (c), under a Beta--Binomial model (even with adjusted counts), the exact 
posterior for $p$ is Beta$(\alpha_n,\beta_n)$.  Thus if $\Pi^\star$ arises 
from such updating under a fully specified likelihood, $\Pi^\star$ is already 
in $\mathcal{B}$ and the unique KL minimizer is precisely 
$\mathrm{Beta}(\alpha_n,\beta_n)$.

When $\Pi^\star$ is more general (due to dependence, behavioral structures, or
nonparametric components), the PRISM posterior can be interpreted as the 
projection of $\Pi^\star$ onto the Beta family using the adjusted empirical
mean and variance $(m^\star,\mathrm{Var}_{\Pi^\star})$ induced by the 
PRISM adjustment rules.  This identifies $(\alpha_n,\beta_n)$ with the 
unique minimizer $(a^\dagger,b^\dagger)$.
\end{proof}

\begin{remark}[Interpretation and Novelty]
This theorem provides an information-theoretic justification for the PRISM 
posterior beyond conjugacy.  Even when the true posterior $\Pi^\star$ is 
nonparametric or analytically intractable, PRISM delivers the 
\emph{closest Beta distribution in KL divergence}.  

Thus the Beta form is not merely a convenient algebraic choice, but the 
information-projection that preserves the two most important moments of the 
ideal posterior under the PRISM-adjusted signal process.

This interpretation also helps explain why the hybrid prior and adjustment 
mechanism remain stable even under dependence or behavioral distortions:  
PRISM selects the “least distorted” Beta posterior compatible with the 
adjusted empirical information.
\end{remark}


\begin{remark}[Limitations of the KL Projection View]
Interpreting PRISM as a KL projection onto the Beta family is useful but
also restrictive.  It guarantees that, among all Beta distributions, the
chosen posterior preserves certain moments of a more complex underlying
posterior.  However, it does not claim that the Beta family is rich enough to
capture all features of the true posterior under strong dependence, multimodal
priors, or adversarial behavior.  In settings where such features are
important, the KL-projection perspective should be viewed as an approximation
tool rather than a full description of market beliefs, and more flexible
models (e.g.\ MCMC-based or variational) may be warranted.
\end{remark}
    

\subsection*{8.19 Alternatives to Beta Conjugacy under Dependence}

While PRISM deliberately uses the Beta--Binomial structure for tractability
and interpretability, other approaches can, in principle, accommodate richer
dependence at the cost of computational complexity:

\begin{itemize}
\item \textbf{MCMC with dependent likelihoods.}  
  One can specify an explicit dependent model for the order sequence, such as
  an Ising or Markov random field for YES/NO decisions, and sample from the
  posterior for $p$ via Gibbs or Metropolis--Hastings.  This yields a more
  flexible posterior but requires careful tuning and may be slow in large
  markets.

\item \textbf{Variational approximations.}  
  Variational Bayes can approximate complex posteriors with factored or
  low-rank distributions, trading off accuracy for speed.  In this setting,
  one could approximate the joint posterior over $(p,\text{latent states})$
  with a product of a Beta distribution for $p$ and a tractable family for
  the latent dependence structure.

\item \textbf{Composite likelihoods.}  
  Composite or pseudo-likelihood methods replace the full joint likelihood
  with products of low-dimensional marginals or conditionals, offering a
  compromise between full MCMC and the single-sufficient-statistic approach of
  PRISM.
\end{itemize}

PRISM chooses Beta conjugacy as a deliberate design decision: it provides
closed-form updates, interpretable pseudo-counts, and an information-projection
interpretation, while recognizing that more flexible likelihood-based methods
are possible when computational resources and data volume permit.


\subsection*{8.20 Asymptotics Under Ergodic Regime Switching and an Impossibility Result}
\label{subsec:ergodic_rs_bvm_impossibility}

The dynamic bias layer of Section~reference
models $(\delta_t,\psi_t)$ as a hidden Markov chain $(S_t)$ with a finite
state space $\mathcal{S} = \{1,\dots,R\}$ and regime--specific parameters
$(\delta_r,\psi_r)$. This section establishes (i) a Bernstein--von Mises--type
result under ergodic regime switching (mild nonstationarity), and (ii) an
impossibility result when regimes switch so quickly that no regime accumulates
enough information for consistent learning.

\begin{assumption}[Ergodic Regime Switching and Regularity]
\label{assumption:ergodic_regime_switching}
Let $(S_t)_{t \ge 1}$ be an irreducible, aperiodic Markov chain on
$\mathcal{S} = \{1,\dots,R\}$ with transition matrix $P$ and unique stationary
distribution $\pi = (\pi_1,\dots,\pi_R)$. Assume:
\begin{enumerate}
  \item[(i)] (\emph{Ergodicity}) For each $r,s \in \mathcal{S}$ there exists
        $k \ge 1$ such that $(P^k)_{rs} > 0$, and the chain is aperiodic.
        Consequently, for any initial distribution,
        \[
          \mathbb{P}(S_t = r) \to \pi_r \quad \text{as } t \to \infty.
        \]
  \item[(ii)] (\emph{True parameter vector}) Each regime $r$ has a true parameter
        $\theta_r^\star = (\delta_r^\star,\psi_r^\star)$ in a compact subset
        $\Theta_r \subset \mathbb{R}^{d_r}$, and the emission model
        $p(\mathcal{D}_t \mid S_t=r,\theta_r^\star)$ coincides with the
        Stage~1/Stage~2 correction structure of PRISM.
  \item[(iii)] (\emph{Identifiability and smoothness}) The mapping
        $\theta_r \mapsto p(\mathcal{D}_t \mid S_t=r,\theta_r)$ is identifiable
        and $C^2$ in a neighborhood of $\theta_r^\star$, with Fisher information
        matrix $I_r(\theta_r^\star)$ positive definite.
  \item[(iv)] (\emph{Finite moments}) The emission log-likelihoods have finite
        second moments under the true model.
\end{enumerate}
\end{assumption}

We are interested in a smooth functional of the regime parameters, such as
the long--run average bias
\[
  \bar{\delta}^\star
  := \sum_{r=1}^R \pi_r \,\delta_r^\star,
\]
or more generally a differentiable functional
$\varphi(\theta_1,\dots,\theta_R)$ with $\varphi: \Theta_1 \times \cdots
\times \Theta_R \to \mathbb{R}$.

\begin{theorem}[Bernstein--von Mises Theorem under Ergodic Regime Switching]
\label{thm:ergodic_rs_bvm}
Under Assumption~reference, suppose the
PRISM posterior over $(\theta_1,\dots,\theta_R)$ is proper and assigns
positive prior density in a neighborhood of the true parameter vector
$(\theta_1^\star,\dots,\theta_R^\star)$. Let
\[
  \hat{\theta}_r(T) 
  \quad (r=1,\dots,R)
\]
denote the (quasi--)maximum likelihood or posterior mean estimator of $\theta_r$
based on data $\mathcal{D}_{1:T} = (\mathcal{D}_1,\dots,\mathcal{D}_T)$, and
define $\hat{\varphi}(T) := \varphi(\hat{\theta}_1(T),\dots,\hat{\theta}_R(T))$.
Then, as $T \to \infty$,
\begin{enumerate}
  \item[(i)] (\emph{Consistency})
  \[
    \hat{\varphi}(T) \xrightarrow{\mathbb{P}} \varphi(\theta_1^\star,\dots,\theta_R^\star).
  \]
  \item[(ii)] (\emph{Asymptotic normality})
  \[
    \sqrt{T}\,\Big(
      \hat{\varphi}(T) - \varphi(\theta_1^\star,\dots,\theta_R^\star)
    \Big)
    \;\xrightarrow{d}\;
    \mathcal{N}(0, V_\varphi),
  \]
  where $V_\varphi$ is the asymptotic variance obtained by the delta method
  applied to the joint asymptotic distribution of
  $(\hat{\theta}_1(T),\dots,\hat{\theta}_R(T))$.
  \item[(iii)] (\emph{Bernstein--von Mises})  
  The posterior distribution of
  $\varphi(\theta_1,\dots,\theta_R)$ given $\mathcal{D}_{1:T}$ converges in
  total variation to the normal law
  $\mathcal{N}(\hat{\varphi}(T), V_\varphi / T)$:
  \[
    \left\Vert
      \Pi\big(
        \varphi(\theta_1,\dots,\theta_R) \in \cdot
        \,\big\vert\, \mathcal{D}_{1:T}
      \big)
      - \mathcal{N}\big(
          \hat{\varphi}(T), V_\varphi / T
        \big)
    \right\Vert_{\mathrm{TV}}
    \;\xrightarrow{T \to \infty}\; 0.
  \]
\end{enumerate}
\end{theorem}

\begin{proof}[Proof Sketch]
Under Assumption~reference, the hidden Markov
model $(S_t,\mathcal{D}_t)$ is an ergodic HMM with finite state space and
regular parametric emission densities. Standard results for HMMs imply that
the (quasi--)maximum likelihood estimator of each $\theta_r$ is consistent and
asymptotically normal, with an information matrix determined by long--run
frequencies $\pi_r$ and the per--regime Fisher information $I_r(\theta_r^\star)$.
The joint asymptotic normality of $(\hat{\theta}_1(T),\dots,\hat{\theta}_R(T))$
then yields asymptotic normality of $\hat{\varphi}(T)$ via the delta method.
The Bayesian Bernstein--von Mises conclusion follows from general BvM theorems
for HMMs with finite state space and regular parametric families, together
with the prior positivity condition. Full details follow the usual template for
BvM in dependent data models; we omit these for brevity.
\end{proof}

\begin{remark}[Application to Dynamic PRISM]
\label{remark:dynamic_PRISM_bvm}
In the dynamic PRISM setting of Section~reference,
the regime--specific parameters $\theta_r^\star$ encode bias corrections 
$(\delta_r^\star,\psi_r^\star)$ and possibly additional structural quantities.
The functional $\varphi$ may be taken as the long--run average distortion
$\bar{\delta}^\star = \sum_r \pi_r \delta_r^\star$, or as a mapping from the
collection $(\theta_r^\star)$ to a long--run effective event probability
$p_{\mathrm{eff}}^\star$ under dynamic corrections.  
Theorem~reference then ensures that the PRISM posterior for
$\bar{\delta}$ (or $p_{\mathrm{eff}}$) concentrates and becomes asymptotically
normal despite mild nonstationarity induced by regime switching.
\end{remark}

We now show that this positive result has a natural limit: if regime switching
is too fast for any regime to accumulate information, no sequential estimator
or posterior can remain uniformly well calibrated.

\begin{assumption}[Fast Regime Switching with Vanishing Dwell Time]
\label{assumption:fast_switching}
Let $(S_t)$ be a Markov chain on $\mathcal{S}$ with transition matrix $P$
depending on $T$ such that:
\begin{enumerate}
  \item[(i)] The minimum expected dwell time in each state is uniformly bounded:
  \[
    \sup_T \max_{r \in \mathcal{S}}
      \mathbb{E}_T\big[\,\text{time spent in state $r$ up to $T$}\,\big]
    \;<\; C < \infty.
  \]
  \item[(ii)] The regime--specific parameters $\theta_r^\star(T)$ are allowed to vary
        with $T$, remaining in a compact set, and emissions are generated from
        $p(\mathcal{D}_t \mid S_t,\theta_{S_t}^\star(T))$.
\end{enumerate}
In words, regimes switch so frequently that the average number of visits to any
given state does not grow with $T$.
\end{assumption}

\begin{theorem}[Impossibility of Uniform Consistency Under Fast Regime Switching]
\label{thm:fast_switching_impossibility}
Under Assumption~reference, consider any sequential
estimator sequence $\hat{\theta}(T)$ or any Bayesian posterior sequence
$\Pi_T(\cdot \mid \mathcal{D}_{1:T})$ for a regime--dependent parameter
functional $\varphi(\theta_1^\star(T),\dots,\theta_R^\star(T))$ (e.g.\ a
regime--specific bias $\delta_r^\star(T)$ or a regime--specific event
probability $p_r^\star(T)$). Then there exists a choice of parameter sequences
$\{\theta_r^\star(T)\}$ such that
\[
  \limsup_{T \to \infty}
  \sup_{\{\theta_r^\star(T)\}}
  \mathbb{E}\Big[
    \big\vert
      \hat{\varphi}(T) - \varphi(\theta_1^\star(T),\dots,\theta_R^\star(T))
    \big\vert
  \Big]
  \;>\; 0,
\]
and similarly, no sequence of posteriors $\Pi_T$ can concentrate around the
true functional uniformly over $\{\theta_r^\star(T)\}$. In particular, there
is no uniformly consistent dynamic PRISM estimator or posterior in this fast
switching regime.
\end{theorem}

\begin{proof}[Proof Sketch]
Under Assumption~reference, the expected number of
observations generated in any fixed regime $r$ up to time $T$ is uniformly
bounded by $C$. Therefore, for any estimator or posterior targeting a
regime--specific parameter (or a functional that depends nontrivially on the
per--regime values), the effective sample size per regime does not grow with
$T$. By standard parametric lower bounds, no estimator can achieve vanishing
risk uniformly over the parameter sequences $\{\theta_r^\star(T)\}$ when each
regime is observed only $O(1)$ times. One can construct pairs of parameter
sequences that are indistinguishable from the data but induce separated values
of the functional $\varphi$, forcing a nonzero lower bound on the estimation
error. The same argument applies to Bayesian posteriors: with bounded regime
information, the posterior cannot concentrate uniformly on the true functional.
\end{proof}

\begin{remark}[Interpretation for Dynamic PRISM]
\label{remark:fast_switching_interpretation}
Theorem~reference and
Theorem~reference
identify a \emph{feasible} and an \emph{infeasible} nonstationary regime for
PRISM. Under ergodic regime switching with growing effective sample size per
regime, dynamic PRISM remains consistent and asymptotically normal for smooth
functionals of the regime parameters. When regime switching becomes so fast
that no regime accumulates information, no sequential estimator or posterior
can be uniformly well calibrated. In practice, PRISM should therefore treat
fast, high--frequency structural breaks as a regime where the bias layer and
posterior must be explicitly flagged as fragile, and any pricing output should
carry a warning about nonstationary uncertainty that cannot be statistically
resolved.
\end{remark}



